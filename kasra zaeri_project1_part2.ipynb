{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "HKXuvMrC4yGx"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import os, sys \n",
        "import random \n",
        "import matplotlib.pyplot as plt\n",
        "import copy\n",
        "from torchvision.transforms import ToTensor\n",
        "import torch \n",
        "from torch import linalg as LA\n",
        "from torch import Tensor\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import random_split\n",
        "import torchvision.datasets as datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bcw4_A7q66ut",
        "outputId": "7ee807df-dcef-4eea-cd02-6cfe4491fa25"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "VmiISh0b5qTH"
      },
      "outputs": [],
      "source": [
        "mnist_trainset = datasets.MNIST(root='/content/drive/My Drive', train=True, download=True, transform=ToTensor())\n",
        "mnist_testset = datasets.MNIST(root='/content/drive/My Drive', train=False, download=True, transform=ToTensor())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AuhF-1rVzPP-",
        "outputId": "814d9be7-e88a-4798-e202-a5ed153885e7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KNCxYGKydyoJ"
      },
      "source": [
        "# Deep Learning Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "JP0wd8jRd3jG"
      },
      "outputs": [],
      "source": [
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=5)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=2)                             \n",
        "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=5)         \n",
        "        self.fc1 = nn.Linear(1024, 512)\n",
        "        self.fc2 = nn.Linear(512, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.maxpool(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.maxpool(x)\n",
        "        x = x.view(x.size(0), -1)       \n",
        "        x = F.relu(self.fc1(x))\n",
        "        output = self.fc2(x)\n",
        "        return output    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MdKNurySdV9I"
      },
      "source": [
        "# Centralized Optimization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Y5JHuGTaddB4"
      },
      "outputs": [],
      "source": [
        "batch_size = 64\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "                 dataset=mnist_trainset,\n",
        "                 batch_size=batch_size,\n",
        "                 shuffle=True\n",
        "                 )\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "                dataset=mnist_testset,\n",
        "                batch_size=batch_size,\n",
        "                shuffle=False\n",
        "                )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ECVNWyPLddI6"
      },
      "outputs": [],
      "source": [
        "model = CNN().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr = 0.1) \n",
        "num_epochs = 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RcN7lBQ4ddLj",
        "outputId": "b9912cb7-5963-4576-eb3b-b9986117d9e3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Training:  Epoch No:  9 Iteration No:  320 \n",
            " Loss:  0.0017435522750020027\n",
            "Training:  Epoch No:  9 Iteration No:  321 \n",
            " Loss:  0.0001487794506829232\n",
            "Training:  Epoch No:  9 Iteration No:  322 \n",
            " Loss:  0.00110568234231323\n",
            "Training:  Epoch No:  9 Iteration No:  323 \n",
            " Loss:  0.001139216241426766\n",
            "Training:  Epoch No:  9 Iteration No:  324 \n",
            " Loss:  0.0013763989554718137\n",
            "Training:  Epoch No:  9 Iteration No:  325 \n",
            " Loss:  0.0009741192334331572\n",
            "Training:  Epoch No:  9 Iteration No:  326 \n",
            " Loss:  0.00033277482725679874\n",
            "Training:  Epoch No:  9 Iteration No:  327 \n",
            " Loss:  0.001997612416744232\n",
            "Training:  Epoch No:  9 Iteration No:  328 \n",
            " Loss:  0.02146254852414131\n",
            "Training:  Epoch No:  9 Iteration No:  329 \n",
            " Loss:  0.0001703244779491797\n",
            "Training:  Epoch No:  9 Iteration No:  330 \n",
            " Loss:  0.03341437876224518\n",
            "Training:  Epoch No:  9 Iteration No:  331 \n",
            " Loss:  0.0008950140909291804\n",
            "Training:  Epoch No:  9 Iteration No:  332 \n",
            " Loss:  0.00950370542705059\n",
            "Training:  Epoch No:  9 Iteration No:  333 \n",
            " Loss:  0.0007677618996240199\n",
            "Training:  Epoch No:  9 Iteration No:  334 \n",
            " Loss:  0.0018612418789416552\n",
            "Training:  Epoch No:  9 Iteration No:  335 \n",
            " Loss:  0.015339620411396027\n",
            "Training:  Epoch No:  9 Iteration No:  336 \n",
            " Loss:  0.01195704098790884\n",
            "Training:  Epoch No:  9 Iteration No:  337 \n",
            " Loss:  0.00265497132204473\n",
            "Training:  Epoch No:  9 Iteration No:  338 \n",
            " Loss:  0.00030927351326681674\n",
            "Training:  Epoch No:  9 Iteration No:  339 \n",
            " Loss:  0.003345842706039548\n",
            "Training:  Epoch No:  9 Iteration No:  340 \n",
            " Loss:  0.0041420189663767815\n",
            "Training:  Epoch No:  9 Iteration No:  341 \n",
            " Loss:  0.0003730824391823262\n",
            "Training:  Epoch No:  9 Iteration No:  342 \n",
            " Loss:  0.013156933709979057\n",
            "Training:  Epoch No:  9 Iteration No:  343 \n",
            " Loss:  0.04751617833971977\n",
            "Training:  Epoch No:  9 Iteration No:  344 \n",
            " Loss:  0.0002614847326185554\n",
            "Training:  Epoch No:  9 Iteration No:  345 \n",
            " Loss:  0.005043995566666126\n",
            "Training:  Epoch No:  9 Iteration No:  346 \n",
            " Loss:  0.03570739924907684\n",
            "Training:  Epoch No:  9 Iteration No:  347 \n",
            " Loss:  0.056645624339580536\n",
            "Training:  Epoch No:  9 Iteration No:  348 \n",
            " Loss:  0.012205549515783787\n",
            "Training:  Epoch No:  9 Iteration No:  349 \n",
            " Loss:  0.04774021729826927\n",
            "Training:  Epoch No:  9 Iteration No:  350 \n",
            " Loss:  0.0121866874396801\n",
            "Training:  Epoch No:  9 Iteration No:  351 \n",
            " Loss:  0.027896827086806297\n",
            "Training:  Epoch No:  9 Iteration No:  352 \n",
            " Loss:  0.00031230226159095764\n",
            "Training:  Epoch No:  9 Iteration No:  353 \n",
            " Loss:  0.008760426193475723\n",
            "Training:  Epoch No:  9 Iteration No:  354 \n",
            " Loss:  0.0013969943393021822\n",
            "Training:  Epoch No:  9 Iteration No:  355 \n",
            " Loss:  0.0024820182006806135\n",
            "Training:  Epoch No:  9 Iteration No:  356 \n",
            " Loss:  0.009588981047272682\n",
            "Training:  Epoch No:  9 Iteration No:  357 \n",
            " Loss:  0.000644754443783313\n",
            "Training:  Epoch No:  9 Iteration No:  358 \n",
            " Loss:  0.00905328057706356\n",
            "Training:  Epoch No:  9 Iteration No:  359 \n",
            " Loss:  0.018623873591423035\n",
            "Training:  Epoch No:  9 Iteration No:  360 \n",
            " Loss:  0.001145476708188653\n",
            "Training:  Epoch No:  9 Iteration No:  361 \n",
            " Loss:  0.03001983091235161\n",
            "Training:  Epoch No:  9 Iteration No:  362 \n",
            " Loss:  0.0034530472476035357\n",
            "Training:  Epoch No:  9 Iteration No:  363 \n",
            " Loss:  0.0028864385094493628\n",
            "Training:  Epoch No:  9 Iteration No:  364 \n",
            " Loss:  0.003748846473172307\n",
            "Training:  Epoch No:  9 Iteration No:  365 \n",
            " Loss:  0.001300351694226265\n",
            "Training:  Epoch No:  9 Iteration No:  366 \n",
            " Loss:  0.016505474224686623\n",
            "Training:  Epoch No:  9 Iteration No:  367 \n",
            " Loss:  0.00010104988177772611\n",
            "Training:  Epoch No:  9 Iteration No:  368 \n",
            " Loss:  0.005142983049154282\n",
            "Training:  Epoch No:  9 Iteration No:  369 \n",
            " Loss:  0.006281624548137188\n",
            "Training:  Epoch No:  9 Iteration No:  370 \n",
            " Loss:  0.00015939227887429297\n",
            "Training:  Epoch No:  9 Iteration No:  371 \n",
            " Loss:  0.0015067612985149026\n",
            "Training:  Epoch No:  9 Iteration No:  372 \n",
            " Loss:  0.0004108083376195282\n",
            "Training:  Epoch No:  9 Iteration No:  373 \n",
            " Loss:  0.00011555311357369646\n",
            "Training:  Epoch No:  9 Iteration No:  374 \n",
            " Loss:  0.0037265026476234198\n",
            "Training:  Epoch No:  9 Iteration No:  375 \n",
            " Loss:  0.004195980727672577\n",
            "Training:  Epoch No:  9 Iteration No:  376 \n",
            " Loss:  0.0017861549276858568\n",
            "Training:  Epoch No:  9 Iteration No:  377 \n",
            " Loss:  0.0010709133930504322\n",
            "Training:  Epoch No:  9 Iteration No:  378 \n",
            " Loss:  0.004525499418377876\n",
            "Training:  Epoch No:  9 Iteration No:  379 \n",
            " Loss:  0.008576254360377789\n",
            "Training:  Epoch No:  9 Iteration No:  380 \n",
            " Loss:  0.006730805151164532\n",
            "Training:  Epoch No:  9 Iteration No:  381 \n",
            " Loss:  0.003217932302504778\n",
            "Training:  Epoch No:  9 Iteration No:  382 \n",
            " Loss:  0.007831767201423645\n",
            "Training:  Epoch No:  9 Iteration No:  383 \n",
            " Loss:  0.0003377959947101772\n",
            "Training:  Epoch No:  9 Iteration No:  384 \n",
            " Loss:  0.0293833389878273\n",
            "Training:  Epoch No:  9 Iteration No:  385 \n",
            " Loss:  0.018983369693160057\n",
            "Training:  Epoch No:  9 Iteration No:  386 \n",
            " Loss:  0.012613517232239246\n",
            "Training:  Epoch No:  9 Iteration No:  387 \n",
            " Loss:  0.004289745353162289\n",
            "Training:  Epoch No:  9 Iteration No:  388 \n",
            " Loss:  0.004043751861900091\n",
            "Training:  Epoch No:  9 Iteration No:  389 \n",
            " Loss:  0.00589864794164896\n",
            "Training:  Epoch No:  9 Iteration No:  390 \n",
            " Loss:  0.0014246111968532205\n",
            "Training:  Epoch No:  9 Iteration No:  391 \n",
            " Loss:  0.0022466222289949656\n",
            "Training:  Epoch No:  9 Iteration No:  392 \n",
            " Loss:  0.009020600467920303\n",
            "Training:  Epoch No:  9 Iteration No:  393 \n",
            " Loss:  0.0002938563993666321\n",
            "Training:  Epoch No:  9 Iteration No:  394 \n",
            " Loss:  0.003144419752061367\n",
            "Training:  Epoch No:  9 Iteration No:  395 \n",
            " Loss:  0.0002918964601121843\n",
            "Training:  Epoch No:  9 Iteration No:  396 \n",
            " Loss:  0.0024202940985560417\n",
            "Training:  Epoch No:  9 Iteration No:  397 \n",
            " Loss:  0.0011063349666073918\n",
            "Training:  Epoch No:  9 Iteration No:  398 \n",
            " Loss:  0.0031276619993150234\n",
            "Training:  Epoch No:  9 Iteration No:  399 \n",
            " Loss:  0.002738789189606905\n",
            "Training:  Epoch No:  9 Iteration No:  400 \n",
            " Loss:  0.005828274879604578\n",
            "Training:  Epoch No:  9 Iteration No:  401 \n",
            " Loss:  0.003406420350074768\n",
            "Training:  Epoch No:  9 Iteration No:  402 \n",
            " Loss:  0.006050747353583574\n",
            "Training:  Epoch No:  9 Iteration No:  403 \n",
            " Loss:  0.0016681423876434565\n",
            "Training:  Epoch No:  9 Iteration No:  404 \n",
            " Loss:  0.0027765838894993067\n",
            "Training:  Epoch No:  9 Iteration No:  405 \n",
            " Loss:  0.000820742454379797\n",
            "Training:  Epoch No:  9 Iteration No:  406 \n",
            " Loss:  0.001674178522080183\n",
            "Training:  Epoch No:  9 Iteration No:  407 \n",
            " Loss:  0.0003872505621984601\n",
            "Training:  Epoch No:  9 Iteration No:  408 \n",
            " Loss:  0.0013894293224439025\n",
            "Training:  Epoch No:  9 Iteration No:  409 \n",
            " Loss:  0.007485012523829937\n",
            "Training:  Epoch No:  9 Iteration No:  410 \n",
            " Loss:  0.0055291117168962955\n",
            "Training:  Epoch No:  9 Iteration No:  411 \n",
            " Loss:  0.00011628803622443229\n",
            "Training:  Epoch No:  9 Iteration No:  412 \n",
            " Loss:  0.0005988288903608918\n",
            "Training:  Epoch No:  9 Iteration No:  413 \n",
            " Loss:  0.0015781503170728683\n",
            "Training:  Epoch No:  9 Iteration No:  414 \n",
            " Loss:  0.0001821354526327923\n",
            "Training:  Epoch No:  9 Iteration No:  415 \n",
            " Loss:  0.001740672392770648\n",
            "Training:  Epoch No:  9 Iteration No:  416 \n",
            " Loss:  0.004331805277615786\n",
            "Training:  Epoch No:  9 Iteration No:  417 \n",
            " Loss:  0.0013781561283394694\n",
            "Training:  Epoch No:  9 Iteration No:  418 \n",
            " Loss:  0.00416971743106842\n",
            "Training:  Epoch No:  9 Iteration No:  419 \n",
            " Loss:  0.0012982250191271305\n",
            "Training:  Epoch No:  9 Iteration No:  420 \n",
            " Loss:  0.0036482932046055794\n",
            "Training:  Epoch No:  9 Iteration No:  421 \n",
            " Loss:  0.00014831568114459515\n",
            "Training:  Epoch No:  9 Iteration No:  422 \n",
            " Loss:  0.004440320655703545\n",
            "Training:  Epoch No:  9 Iteration No:  423 \n",
            " Loss:  0.0016620054375380278\n",
            "Training:  Epoch No:  9 Iteration No:  424 \n",
            " Loss:  0.002123788697645068\n",
            "Training:  Epoch No:  9 Iteration No:  425 \n",
            " Loss:  0.004205527249723673\n",
            "Training:  Epoch No:  9 Iteration No:  426 \n",
            " Loss:  0.00037509010871872306\n",
            "Training:  Epoch No:  9 Iteration No:  427 \n",
            " Loss:  0.00737569248303771\n",
            "Training:  Epoch No:  9 Iteration No:  428 \n",
            " Loss:  0.0021958143915981054\n",
            "Training:  Epoch No:  9 Iteration No:  429 \n",
            " Loss:  6.81881356285885e-05\n",
            "Training:  Epoch No:  9 Iteration No:  430 \n",
            " Loss:  0.011173711158335209\n",
            "Training:  Epoch No:  9 Iteration No:  431 \n",
            " Loss:  0.0014667785726487637\n",
            "Training:  Epoch No:  9 Iteration No:  432 \n",
            " Loss:  0.0006879556458443403\n",
            "Training:  Epoch No:  9 Iteration No:  433 \n",
            " Loss:  0.007761120796203613\n",
            "Training:  Epoch No:  9 Iteration No:  434 \n",
            " Loss:  0.0027453999500721693\n",
            "Training:  Epoch No:  9 Iteration No:  435 \n",
            " Loss:  0.0012159952893853188\n",
            "Training:  Epoch No:  9 Iteration No:  436 \n",
            " Loss:  0.000443464785348624\n",
            "Training:  Epoch No:  9 Iteration No:  437 \n",
            " Loss:  0.0018572539556771517\n",
            "Training:  Epoch No:  9 Iteration No:  438 \n",
            " Loss:  0.0012200968340039253\n",
            "Training:  Epoch No:  9 Iteration No:  439 \n",
            " Loss:  0.015819497406482697\n",
            "Training:  Epoch No:  9 Iteration No:  440 \n",
            " Loss:  0.0004093183670192957\n",
            "Training:  Epoch No:  9 Iteration No:  441 \n",
            " Loss:  0.0005720987683162093\n",
            "Training:  Epoch No:  9 Iteration No:  442 \n",
            " Loss:  0.0030057509429752827\n",
            "Training:  Epoch No:  9 Iteration No:  443 \n",
            " Loss:  0.0002238215529359877\n",
            "Training:  Epoch No:  9 Iteration No:  444 \n",
            " Loss:  0.019320398569107056\n",
            "Training:  Epoch No:  9 Iteration No:  445 \n",
            " Loss:  0.027057839557528496\n",
            "Training:  Epoch No:  9 Iteration No:  446 \n",
            " Loss:  0.0010258102556690574\n",
            "Training:  Epoch No:  9 Iteration No:  447 \n",
            " Loss:  0.0004810829705093056\n",
            "Training:  Epoch No:  9 Iteration No:  448 \n",
            " Loss:  0.0010879829060286283\n",
            "Training:  Epoch No:  9 Iteration No:  449 \n",
            " Loss:  0.00126854598056525\n",
            "Training:  Epoch No:  9 Iteration No:  450 \n",
            " Loss:  0.00043213192839175463\n",
            "Training:  Epoch No:  9 Iteration No:  451 \n",
            " Loss:  0.009601974859833717\n",
            "Training:  Epoch No:  9 Iteration No:  452 \n",
            " Loss:  0.0016419916646555066\n",
            "Training:  Epoch No:  9 Iteration No:  453 \n",
            " Loss:  0.0012462354497984052\n",
            "Training:  Epoch No:  9 Iteration No:  454 \n",
            " Loss:  0.0017696373397484422\n",
            "Training:  Epoch No:  9 Iteration No:  455 \n",
            " Loss:  0.0032552327029407024\n",
            "Training:  Epoch No:  9 Iteration No:  456 \n",
            " Loss:  0.010388074442744255\n",
            "Training:  Epoch No:  9 Iteration No:  457 \n",
            " Loss:  0.0003461329033598304\n",
            "Training:  Epoch No:  9 Iteration No:  458 \n",
            " Loss:  0.000811257865279913\n",
            "Training:  Epoch No:  9 Iteration No:  459 \n",
            " Loss:  0.03146560862660408\n",
            "Training:  Epoch No:  9 Iteration No:  460 \n",
            " Loss:  0.004119261167943478\n",
            "Training:  Epoch No:  9 Iteration No:  461 \n",
            " Loss:  0.0010571432067081332\n",
            "Training:  Epoch No:  9 Iteration No:  462 \n",
            " Loss:  0.0024259218480437994\n",
            "Training:  Epoch No:  9 Iteration No:  463 \n",
            " Loss:  0.03046240098774433\n",
            "Training:  Epoch No:  9 Iteration No:  464 \n",
            " Loss:  0.0009420652058906853\n",
            "Training:  Epoch No:  9 Iteration No:  465 \n",
            " Loss:  0.0008258784073404968\n",
            "Training:  Epoch No:  9 Iteration No:  466 \n",
            " Loss:  0.0008096714154817164\n",
            "Training:  Epoch No:  9 Iteration No:  467 \n",
            " Loss:  0.0021866129245609045\n",
            "Training:  Epoch No:  9 Iteration No:  468 \n",
            " Loss:  0.0014107553288340569\n",
            "Training:  Epoch No:  9 Iteration No:  469 \n",
            " Loss:  0.002161671407520771\n",
            "Training:  Epoch No:  9 Iteration No:  470 \n",
            " Loss:  0.0002466148289386183\n",
            "Training:  Epoch No:  9 Iteration No:  471 \n",
            " Loss:  0.0008185807964764535\n",
            "Training:  Epoch No:  9 Iteration No:  472 \n",
            " Loss:  0.0006653860327787697\n",
            "Training:  Epoch No:  9 Iteration No:  473 \n",
            " Loss:  0.00042418111115694046\n",
            "Training:  Epoch No:  9 Iteration No:  474 \n",
            " Loss:  0.002779991365969181\n",
            "Training:  Epoch No:  9 Iteration No:  475 \n",
            " Loss:  0.005139739252626896\n",
            "Training:  Epoch No:  9 Iteration No:  476 \n",
            " Loss:  0.0014145566383376718\n",
            "Training:  Epoch No:  9 Iteration No:  477 \n",
            " Loss:  0.003721974790096283\n",
            "Training:  Epoch No:  9 Iteration No:  478 \n",
            " Loss:  0.0030077395495027304\n",
            "Training:  Epoch No:  9 Iteration No:  479 \n",
            " Loss:  0.0001720012369332835\n",
            "Training:  Epoch No:  9 Iteration No:  480 \n",
            " Loss:  0.0037490525282919407\n",
            "Training:  Epoch No:  9 Iteration No:  481 \n",
            " Loss:  0.00020506937289610505\n",
            "Training:  Epoch No:  9 Iteration No:  482 \n",
            " Loss:  0.0027742390520870686\n",
            "Training:  Epoch No:  9 Iteration No:  483 \n",
            " Loss:  0.003024087054654956\n",
            "Training:  Epoch No:  9 Iteration No:  484 \n",
            " Loss:  0.002830806188285351\n",
            "Training:  Epoch No:  9 Iteration No:  485 \n",
            " Loss:  0.0031617488712072372\n",
            "Training:  Epoch No:  9 Iteration No:  486 \n",
            " Loss:  0.0036278576590120792\n",
            "Training:  Epoch No:  9 Iteration No:  487 \n",
            " Loss:  0.00044892606092616916\n",
            "Training:  Epoch No:  9 Iteration No:  488 \n",
            " Loss:  0.015088295564055443\n",
            "Training:  Epoch No:  9 Iteration No:  489 \n",
            " Loss:  0.0030454692896455526\n",
            "Training:  Epoch No:  9 Iteration No:  490 \n",
            " Loss:  0.00018219361663796008\n",
            "Training:  Epoch No:  9 Iteration No:  491 \n",
            " Loss:  0.00028703242423944175\n",
            "Training:  Epoch No:  9 Iteration No:  492 \n",
            " Loss:  0.0011118953116238117\n",
            "Training:  Epoch No:  9 Iteration No:  493 \n",
            " Loss:  0.005613184534013271\n",
            "Training:  Epoch No:  9 Iteration No:  494 \n",
            " Loss:  0.0008659344748593867\n",
            "Training:  Epoch No:  9 Iteration No:  495 \n",
            " Loss:  0.001971418270841241\n",
            "Training:  Epoch No:  9 Iteration No:  496 \n",
            " Loss:  0.001966912066563964\n",
            "Training:  Epoch No:  9 Iteration No:  497 \n",
            " Loss:  0.02025509998202324\n",
            "Training:  Epoch No:  9 Iteration No:  498 \n",
            " Loss:  0.0003558338212314993\n",
            "Training:  Epoch No:  9 Iteration No:  499 \n",
            " Loss:  0.0006723993574269116\n",
            "Training:  Epoch No:  9 Iteration No:  500 \n",
            " Loss:  0.00047843597712926567\n",
            "Training:  Epoch No:  9 Iteration No:  501 \n",
            " Loss:  0.0024554296396672726\n",
            "Training:  Epoch No:  9 Iteration No:  502 \n",
            " Loss:  0.0050745150074362755\n",
            "Training:  Epoch No:  9 Iteration No:  503 \n",
            " Loss:  0.0028157872147858143\n",
            "Training:  Epoch No:  9 Iteration No:  504 \n",
            " Loss:  0.00048228836385533214\n",
            "Training:  Epoch No:  9 Iteration No:  505 \n",
            " Loss:  0.003204058390110731\n",
            "Training:  Epoch No:  9 Iteration No:  506 \n",
            " Loss:  0.0003727441653609276\n",
            "Training:  Epoch No:  9 Iteration No:  507 \n",
            " Loss:  0.0013371446402743459\n",
            "Training:  Epoch No:  9 Iteration No:  508 \n",
            " Loss:  0.01555821392685175\n",
            "Training:  Epoch No:  9 Iteration No:  509 \n",
            " Loss:  0.01923522911965847\n",
            "Training:  Epoch No:  9 Iteration No:  510 \n",
            " Loss:  0.0006264820694923401\n",
            "Training:  Epoch No:  9 Iteration No:  511 \n",
            " Loss:  0.0015051797963678837\n",
            "Training:  Epoch No:  9 Iteration No:  512 \n",
            " Loss:  0.0005684514762833714\n",
            "Training:  Epoch No:  9 Iteration No:  513 \n",
            " Loss:  0.0008842003298923373\n",
            "Training:  Epoch No:  9 Iteration No:  514 \n",
            " Loss:  0.0008600592846050858\n",
            "Training:  Epoch No:  9 Iteration No:  515 \n",
            " Loss:  0.00344196823425591\n",
            "Training:  Epoch No:  9 Iteration No:  516 \n",
            " Loss:  0.00015945227642077953\n",
            "Training:  Epoch No:  9 Iteration No:  517 \n",
            " Loss:  0.0007998876390047371\n",
            "Training:  Epoch No:  9 Iteration No:  518 \n",
            " Loss:  0.001755160978063941\n",
            "Training:  Epoch No:  9 Iteration No:  519 \n",
            " Loss:  0.002245836891233921\n",
            "Training:  Epoch No:  9 Iteration No:  520 \n",
            " Loss:  0.0005214549019001424\n",
            "Training:  Epoch No:  9 Iteration No:  521 \n",
            " Loss:  0.015099647454917431\n",
            "Training:  Epoch No:  9 Iteration No:  522 \n",
            " Loss:  0.0003173492441419512\n",
            "Training:  Epoch No:  9 Iteration No:  523 \n",
            " Loss:  0.006530784070491791\n",
            "Training:  Epoch No:  9 Iteration No:  524 \n",
            " Loss:  0.00030309430439956486\n",
            "Training:  Epoch No:  9 Iteration No:  525 \n",
            " Loss:  0.0045634969137609005\n",
            "Training:  Epoch No:  9 Iteration No:  526 \n",
            " Loss:  0.0008297636522911489\n",
            "Training:  Epoch No:  9 Iteration No:  527 \n",
            " Loss:  5.694401261280291e-05\n",
            "Training:  Epoch No:  9 Iteration No:  528 \n",
            " Loss:  0.008287226781249046\n",
            "Training:  Epoch No:  9 Iteration No:  529 \n",
            " Loss:  0.0014986938331276178\n",
            "Training:  Epoch No:  9 Iteration No:  530 \n",
            " Loss:  0.0024583376944065094\n",
            "Training:  Epoch No:  9 Iteration No:  531 \n",
            " Loss:  0.0037880842573940754\n",
            "Training:  Epoch No:  9 Iteration No:  532 \n",
            " Loss:  0.014150596223771572\n",
            "Training:  Epoch No:  9 Iteration No:  533 \n",
            " Loss:  0.0014449127484112978\n",
            "Training:  Epoch No:  9 Iteration No:  534 \n",
            " Loss:  0.00023378357582259923\n",
            "Training:  Epoch No:  9 Iteration No:  535 \n",
            " Loss:  0.0013565905392169952\n",
            "Training:  Epoch No:  9 Iteration No:  536 \n",
            " Loss:  0.0013558006612583995\n",
            "Training:  Epoch No:  9 Iteration No:  537 \n",
            " Loss:  0.0026322912890464067\n",
            "Training:  Epoch No:  9 Iteration No:  538 \n",
            " Loss:  0.0029744734056293964\n",
            "Training:  Epoch No:  9 Iteration No:  539 \n",
            " Loss:  0.003323429496958852\n",
            "Training:  Epoch No:  9 Iteration No:  540 \n",
            " Loss:  0.005055057816207409\n",
            "Training:  Epoch No:  9 Iteration No:  541 \n",
            " Loss:  0.0021385587751865387\n",
            "Training:  Epoch No:  9 Iteration No:  542 \n",
            " Loss:  0.0016354991821572185\n",
            "Training:  Epoch No:  9 Iteration No:  543 \n",
            " Loss:  0.0021882825531065464\n",
            "Training:  Epoch No:  9 Iteration No:  544 \n",
            " Loss:  0.0009550678660161793\n",
            "Training:  Epoch No:  9 Iteration No:  545 \n",
            " Loss:  0.0036066179163753986\n",
            "Training:  Epoch No:  9 Iteration No:  546 \n",
            " Loss:  0.0032843570224940777\n",
            "Training:  Epoch No:  9 Iteration No:  547 \n",
            " Loss:  0.0013838136801496148\n",
            "Training:  Epoch No:  9 Iteration No:  548 \n",
            " Loss:  0.003779557067900896\n",
            "Training:  Epoch No:  9 Iteration No:  549 \n",
            " Loss:  0.009589288383722305\n",
            "Training:  Epoch No:  9 Iteration No:  550 \n",
            " Loss:  0.003267955966293812\n",
            "Training:  Epoch No:  9 Iteration No:  551 \n",
            " Loss:  0.0010759870056062937\n",
            "Training:  Epoch No:  9 Iteration No:  552 \n",
            " Loss:  0.00125079695135355\n",
            "Training:  Epoch No:  9 Iteration No:  553 \n",
            " Loss:  0.0023382902145385742\n",
            "Training:  Epoch No:  9 Iteration No:  554 \n",
            " Loss:  0.0029588635079562664\n",
            "Training:  Epoch No:  9 Iteration No:  555 \n",
            " Loss:  0.0035378707107156515\n",
            "Training:  Epoch No:  9 Iteration No:  556 \n",
            " Loss:  0.007923898287117481\n",
            "Training:  Epoch No:  9 Iteration No:  557 \n",
            " Loss:  0.003337584435939789\n",
            "Training:  Epoch No:  9 Iteration No:  558 \n",
            " Loss:  0.007085866294801235\n",
            "Training:  Epoch No:  9 Iteration No:  559 \n",
            " Loss:  0.0020923083648085594\n",
            "Training:  Epoch No:  9 Iteration No:  560 \n",
            " Loss:  0.0022950421553105116\n",
            "Training:  Epoch No:  9 Iteration No:  561 \n",
            " Loss:  0.0005773333250544965\n",
            "Training:  Epoch No:  9 Iteration No:  562 \n",
            " Loss:  0.003811679547652602\n",
            "Training:  Epoch No:  9 Iteration No:  563 \n",
            " Loss:  0.0010287014301866293\n",
            "Training:  Epoch No:  9 Iteration No:  564 \n",
            " Loss:  0.0009704081458039582\n",
            "Training:  Epoch No:  9 Iteration No:  565 \n",
            " Loss:  0.001850426779128611\n",
            "Training:  Epoch No:  9 Iteration No:  566 \n",
            " Loss:  0.013025478459894657\n",
            "Training:  Epoch No:  9 Iteration No:  567 \n",
            " Loss:  0.004594251047819853\n",
            "Training:  Epoch No:  9 Iteration No:  568 \n",
            " Loss:  0.0007781157037243247\n",
            "Training:  Epoch No:  9 Iteration No:  569 \n",
            " Loss:  0.00034432110260240734\n",
            "Training:  Epoch No:  9 Iteration No:  570 \n",
            " Loss:  0.0006497590220533311\n",
            "Training:  Epoch No:  9 Iteration No:  571 \n",
            " Loss:  0.0011085347505286336\n",
            "Training:  Epoch No:  9 Iteration No:  572 \n",
            " Loss:  0.0006003474118188024\n",
            "Training:  Epoch No:  9 Iteration No:  573 \n",
            " Loss:  0.000445734040113166\n",
            "Training:  Epoch No:  9 Iteration No:  574 \n",
            " Loss:  0.0009287578868679702\n",
            "Training:  Epoch No:  9 Iteration No:  575 \n",
            " Loss:  0.00045530626084655523\n",
            "Training:  Epoch No:  9 Iteration No:  576 \n",
            " Loss:  0.0006610006676055491\n",
            "Training:  Epoch No:  9 Iteration No:  577 \n",
            " Loss:  0.009536524303257465\n",
            "Training:  Epoch No:  9 Iteration No:  578 \n",
            " Loss:  0.048763208091259\n",
            "Training:  Epoch No:  9 Iteration No:  579 \n",
            " Loss:  0.0016544170212000608\n",
            "Training:  Epoch No:  9 Iteration No:  580 \n",
            " Loss:  0.009910786524415016\n",
            "Training:  Epoch No:  9 Iteration No:  581 \n",
            " Loss:  0.009032400324940681\n",
            "Training:  Epoch No:  9 Iteration No:  582 \n",
            " Loss:  0.022982919588685036\n",
            "Training:  Epoch No:  9 Iteration No:  583 \n",
            " Loss:  0.011203995905816555\n",
            "Training:  Epoch No:  9 Iteration No:  584 \n",
            " Loss:  0.01352914422750473\n",
            "Training:  Epoch No:  9 Iteration No:  585 \n",
            " Loss:  5.615901682176627e-05\n",
            "Training:  Epoch No:  9 Iteration No:  586 \n",
            " Loss:  0.02947515808045864\n",
            "Training:  Epoch No:  9 Iteration No:  587 \n",
            " Loss:  0.0022946202661842108\n",
            "Training:  Epoch No:  9 Iteration No:  588 \n",
            " Loss:  0.007839355617761612\n",
            "Training:  Epoch No:  9 Iteration No:  589 \n",
            " Loss:  0.0014978124527260661\n",
            "Training:  Epoch No:  9 Iteration No:  590 \n",
            " Loss:  0.006425315048545599\n",
            "Training:  Epoch No:  9 Iteration No:  591 \n",
            " Loss:  0.0021223591174930334\n",
            "Training:  Epoch No:  9 Iteration No:  592 \n",
            " Loss:  0.00024253391893580556\n",
            "Training:  Epoch No:  9 Iteration No:  593 \n",
            " Loss:  0.0034505059011280537\n",
            "Training:  Epoch No:  9 Iteration No:  594 \n",
            " Loss:  0.0026102105621248484\n",
            "Training:  Epoch No:  9 Iteration No:  595 \n",
            " Loss:  0.001813703216612339\n",
            "Training:  Epoch No:  9 Iteration No:  596 \n",
            " Loss:  0.0018068341305479407\n",
            "Training:  Epoch No:  9 Iteration No:  597 \n",
            " Loss:  0.000767779943998903\n",
            "Training:  Epoch No:  9 Iteration No:  598 \n",
            " Loss:  0.000778183457441628\n",
            "Training:  Epoch No:  9 Iteration No:  599 \n",
            " Loss:  0.0019562160596251488\n",
            "Training:  Epoch No:  9 Iteration No:  600 \n",
            " Loss:  0.0013569938018918037\n",
            "Training:  Epoch No:  9 Iteration No:  601 \n",
            " Loss:  8.005459676496685e-05\n",
            "Training:  Epoch No:  9 Iteration No:  602 \n",
            " Loss:  0.00011080575495725498\n",
            "Training:  Epoch No:  9 Iteration No:  603 \n",
            " Loss:  0.003571881214156747\n",
            "Training:  Epoch No:  9 Iteration No:  604 \n",
            " Loss:  0.00675152288749814\n",
            "Training:  Epoch No:  9 Iteration No:  605 \n",
            " Loss:  0.0007200047257356346\n",
            "Training:  Epoch No:  9 Iteration No:  606 \n",
            " Loss:  0.0015164116630330682\n",
            "Training:  Epoch No:  9 Iteration No:  607 \n",
            " Loss:  6.611579738091677e-05\n",
            "Training:  Epoch No:  9 Iteration No:  608 \n",
            " Loss:  0.0013045534724369645\n",
            "Training:  Epoch No:  9 Iteration No:  609 \n",
            " Loss:  0.000804589712060988\n",
            "Training:  Epoch No:  9 Iteration No:  610 \n",
            " Loss:  0.0012813771609216928\n",
            "Training:  Epoch No:  9 Iteration No:  611 \n",
            " Loss:  0.0076603963971138\n",
            "Training:  Epoch No:  9 Iteration No:  612 \n",
            " Loss:  0.0012652822770178318\n",
            "Training:  Epoch No:  9 Iteration No:  613 \n",
            " Loss:  0.0016572403255850077\n",
            "Training:  Epoch No:  9 Iteration No:  614 \n",
            " Loss:  0.008040264248847961\n",
            "Training:  Epoch No:  9 Iteration No:  615 \n",
            " Loss:  0.06758596748113632\n",
            "Training:  Epoch No:  9 Iteration No:  616 \n",
            " Loss:  0.0009514921111986041\n",
            "Training:  Epoch No:  9 Iteration No:  617 \n",
            " Loss:  0.0024064439348876476\n",
            "Training:  Epoch No:  9 Iteration No:  618 \n",
            " Loss:  0.026969052851200104\n",
            "Training:  Epoch No:  9 Iteration No:  619 \n",
            " Loss:  0.0007581007084809244\n",
            "Training:  Epoch No:  9 Iteration No:  620 \n",
            " Loss:  0.0013322876766324043\n",
            "Training:  Epoch No:  9 Iteration No:  621 \n",
            " Loss:  0.002889428287744522\n",
            "Training:  Epoch No:  9 Iteration No:  622 \n",
            " Loss:  5.1209663070039824e-05\n",
            "Training:  Epoch No:  9 Iteration No:  623 \n",
            " Loss:  0.0006091604009270668\n",
            "Training:  Epoch No:  9 Iteration No:  624 \n",
            " Loss:  0.04466152936220169\n",
            "Training:  Epoch No:  9 Iteration No:  625 \n",
            " Loss:  0.003641177201643586\n",
            "Training:  Epoch No:  9 Iteration No:  626 \n",
            " Loss:  0.00015053073002491146\n",
            "Training:  Epoch No:  9 Iteration No:  627 \n",
            " Loss:  0.003039853647351265\n",
            "Training:  Epoch No:  9 Iteration No:  628 \n",
            " Loss:  0.0004173967754468322\n",
            "Training:  Epoch No:  9 Iteration No:  629 \n",
            " Loss:  0.0038337998557835817\n",
            "Training:  Epoch No:  9 Iteration No:  630 \n",
            " Loss:  0.006298498250544071\n",
            "Training:  Epoch No:  9 Iteration No:  631 \n",
            " Loss:  0.00407874770462513\n",
            "Training:  Epoch No:  9 Iteration No:  632 \n",
            " Loss:  0.0005364485550671816\n",
            "Training:  Epoch No:  9 Iteration No:  633 \n",
            " Loss:  0.00031052844133228064\n",
            "Training:  Epoch No:  9 Iteration No:  634 \n",
            " Loss:  0.0012652018340304494\n",
            "Training:  Epoch No:  9 Iteration No:  635 \n",
            " Loss:  0.0026069344021379948\n",
            "Training:  Epoch No:  9 Iteration No:  636 \n",
            " Loss:  0.00046869891230016947\n",
            "Training:  Epoch No:  9 Iteration No:  637 \n",
            " Loss:  0.032286837697029114\n",
            "Training:  Epoch No:  9 Iteration No:  638 \n",
            " Loss:  0.0034742355346679688\n",
            "Training:  Epoch No:  9 Iteration No:  639 \n",
            " Loss:  0.005035292822867632\n",
            "Training:  Epoch No:  9 Iteration No:  640 \n",
            " Loss:  0.004701934289187193\n",
            "Training:  Epoch No:  9 Iteration No:  641 \n",
            " Loss:  0.0015540511813014746\n",
            "Training:  Epoch No:  9 Iteration No:  642 \n",
            " Loss:  0.002579448511824012\n",
            "Training:  Epoch No:  9 Iteration No:  643 \n",
            " Loss:  0.06512671709060669\n",
            "Training:  Epoch No:  9 Iteration No:  644 \n",
            " Loss:  0.0006232515443116426\n",
            "Training:  Epoch No:  9 Iteration No:  645 \n",
            " Loss:  0.002107009058818221\n",
            "Training:  Epoch No:  9 Iteration No:  646 \n",
            " Loss:  0.004018459003418684\n",
            "Training:  Epoch No:  9 Iteration No:  647 \n",
            " Loss:  7.951229781610891e-05\n",
            "Training:  Epoch No:  9 Iteration No:  648 \n",
            " Loss:  0.0010161218233406544\n",
            "Training:  Epoch No:  9 Iteration No:  649 \n",
            " Loss:  0.005711150821298361\n",
            "Training:  Epoch No:  9 Iteration No:  650 \n",
            " Loss:  0.01336586195975542\n",
            "Training:  Epoch No:  9 Iteration No:  651 \n",
            " Loss:  0.0007358411094173789\n",
            "Training:  Epoch No:  9 Iteration No:  652 \n",
            " Loss:  0.0009045066544786096\n",
            "Training:  Epoch No:  9 Iteration No:  653 \n",
            " Loss:  0.0029437721241265535\n",
            "Training:  Epoch No:  9 Iteration No:  654 \n",
            " Loss:  0.006582679692655802\n",
            "Training:  Epoch No:  9 Iteration No:  655 \n",
            " Loss:  0.0030797827057540417\n",
            "Training:  Epoch No:  9 Iteration No:  656 \n",
            " Loss:  0.02039329521358013\n",
            "Training:  Epoch No:  9 Iteration No:  657 \n",
            " Loss:  0.0017068589804694057\n",
            "Training:  Epoch No:  9 Iteration No:  658 \n",
            " Loss:  0.007265375927090645\n",
            "Training:  Epoch No:  9 Iteration No:  659 \n",
            " Loss:  0.0063209268264472485\n",
            "Training:  Epoch No:  9 Iteration No:  660 \n",
            " Loss:  0.005529385060071945\n",
            "Training:  Epoch No:  9 Iteration No:  661 \n",
            " Loss:  0.02851976826786995\n",
            "Training:  Epoch No:  9 Iteration No:  662 \n",
            " Loss:  0.001456057303585112\n",
            "Training:  Epoch No:  9 Iteration No:  663 \n",
            " Loss:  0.004135613329708576\n",
            "Training:  Epoch No:  9 Iteration No:  664 \n",
            " Loss:  0.009069950319826603\n",
            "Training:  Epoch No:  9 Iteration No:  665 \n",
            " Loss:  0.002092483453452587\n",
            "Training:  Epoch No:  9 Iteration No:  666 \n",
            " Loss:  0.0013074791058897972\n",
            "Training:  Epoch No:  9 Iteration No:  667 \n",
            " Loss:  0.006760060787200928\n",
            "Training:  Epoch No:  9 Iteration No:  668 \n",
            " Loss:  0.005408532917499542\n",
            "Training:  Epoch No:  9 Iteration No:  669 \n",
            " Loss:  0.010470307432115078\n",
            "Training:  Epoch No:  9 Iteration No:  670 \n",
            " Loss:  0.004264472518116236\n",
            "Training:  Epoch No:  9 Iteration No:  671 \n",
            " Loss:  0.00010338916763430461\n",
            "Training:  Epoch No:  9 Iteration No:  672 \n",
            " Loss:  0.004468228202313185\n",
            "Training:  Epoch No:  9 Iteration No:  673 \n",
            " Loss:  0.0026720149908214808\n",
            "Training:  Epoch No:  9 Iteration No:  674 \n",
            " Loss:  0.006831971928477287\n",
            "Training:  Epoch No:  9 Iteration No:  675 \n",
            " Loss:  0.02709934674203396\n",
            "Training:  Epoch No:  9 Iteration No:  676 \n",
            " Loss:  0.029511375352740288\n",
            "Training:  Epoch No:  9 Iteration No:  677 \n",
            " Loss:  0.014877512119710445\n",
            "Training:  Epoch No:  9 Iteration No:  678 \n",
            " Loss:  0.0176737979054451\n",
            "Training:  Epoch No:  9 Iteration No:  679 \n",
            " Loss:  0.005711167585104704\n",
            "Training:  Epoch No:  9 Iteration No:  680 \n",
            " Loss:  0.01783355511724949\n",
            "Training:  Epoch No:  9 Iteration No:  681 \n",
            " Loss:  0.0022701495327055454\n",
            "Training:  Epoch No:  9 Iteration No:  682 \n",
            " Loss:  0.0006108413217589259\n",
            "Training:  Epoch No:  9 Iteration No:  683 \n",
            " Loss:  0.03807491809129715\n",
            "Training:  Epoch No:  9 Iteration No:  684 \n",
            " Loss:  0.010391623713076115\n",
            "Training:  Epoch No:  9 Iteration No:  685 \n",
            " Loss:  0.051294147968292236\n",
            "Training:  Epoch No:  9 Iteration No:  686 \n",
            " Loss:  0.07239469140768051\n",
            "Training:  Epoch No:  9 Iteration No:  687 \n",
            " Loss:  0.00048528696061111987\n",
            "Training:  Epoch No:  9 Iteration No:  688 \n",
            " Loss:  0.0003108165110461414\n",
            "Training:  Epoch No:  9 Iteration No:  689 \n",
            " Loss:  0.016258440911769867\n",
            "Training:  Epoch No:  9 Iteration No:  690 \n",
            " Loss:  0.025892578065395355\n",
            "Training:  Epoch No:  9 Iteration No:  691 \n",
            " Loss:  0.006452254019677639\n",
            "Training:  Epoch No:  9 Iteration No:  692 \n",
            " Loss:  0.00023725541541352868\n",
            "Training:  Epoch No:  9 Iteration No:  693 \n",
            " Loss:  0.0005068921018391848\n",
            "Training:  Epoch No:  9 Iteration No:  694 \n",
            " Loss:  0.0013848056551069021\n",
            "Training:  Epoch No:  9 Iteration No:  695 \n",
            " Loss:  0.00511389784514904\n",
            "Training:  Epoch No:  9 Iteration No:  696 \n",
            " Loss:  0.000976882758550346\n",
            "Training:  Epoch No:  9 Iteration No:  697 \n",
            " Loss:  0.00011610534420469776\n",
            "Training:  Epoch No:  9 Iteration No:  698 \n",
            " Loss:  0.000371220288798213\n",
            "Training:  Epoch No:  9 Iteration No:  699 \n",
            " Loss:  0.009765785187482834\n",
            "Training:  Epoch No:  9 Iteration No:  700 \n",
            " Loss:  0.0004952340386807919\n",
            "Training:  Epoch No:  9 Iteration No:  701 \n",
            " Loss:  6.7308203142602e-05\n",
            "Training:  Epoch No:  9 Iteration No:  702 \n",
            " Loss:  0.01346905529499054\n",
            "Training:  Epoch No:  9 Iteration No:  703 \n",
            " Loss:  0.0035960646346211433\n",
            "Training:  Epoch No:  9 Iteration No:  704 \n",
            " Loss:  0.0002942003484349698\n",
            "Training:  Epoch No:  9 Iteration No:  705 \n",
            " Loss:  0.018462177366018295\n",
            "Training:  Epoch No:  9 Iteration No:  706 \n",
            " Loss:  0.010179671458899975\n",
            "Training:  Epoch No:  9 Iteration No:  707 \n",
            " Loss:  0.0022012279368937016\n",
            "Training:  Epoch No:  9 Iteration No:  708 \n",
            " Loss:  0.02783847600221634\n",
            "Training:  Epoch No:  9 Iteration No:  709 \n",
            " Loss:  0.00023870500444900244\n",
            "Training:  Epoch No:  9 Iteration No:  710 \n",
            " Loss:  0.0062472024001181126\n",
            "Training:  Epoch No:  9 Iteration No:  711 \n",
            " Loss:  0.0016165324486792088\n",
            "Training:  Epoch No:  9 Iteration No:  712 \n",
            " Loss:  0.0017878985963761806\n",
            "Training:  Epoch No:  9 Iteration No:  713 \n",
            " Loss:  0.001691048382781446\n",
            "Training:  Epoch No:  9 Iteration No:  714 \n",
            " Loss:  0.0006776810041628778\n",
            "Training:  Epoch No:  9 Iteration No:  715 \n",
            " Loss:  0.001395252998918295\n",
            "Training:  Epoch No:  9 Iteration No:  716 \n",
            " Loss:  0.0011636561248451471\n",
            "Training:  Epoch No:  9 Iteration No:  717 \n",
            " Loss:  0.005295348819345236\n",
            "Training:  Epoch No:  9 Iteration No:  718 \n",
            " Loss:  0.0012231767177581787\n",
            "Training:  Epoch No:  9 Iteration No:  719 \n",
            " Loss:  0.0006126788794063032\n",
            "Training:  Epoch No:  9 Iteration No:  720 \n",
            " Loss:  6.430890061892569e-05\n",
            "Training:  Epoch No:  9 Iteration No:  721 \n",
            " Loss:  0.0004198894603177905\n",
            "Training:  Epoch No:  9 Iteration No:  722 \n",
            " Loss:  0.001816108007915318\n",
            "Training:  Epoch No:  9 Iteration No:  723 \n",
            " Loss:  0.05153452232480049\n",
            "Training:  Epoch No:  9 Iteration No:  724 \n",
            " Loss:  0.001106606563553214\n",
            "Training:  Epoch No:  9 Iteration No:  725 \n",
            " Loss:  0.09972403943538666\n",
            "Training:  Epoch No:  9 Iteration No:  726 \n",
            " Loss:  0.07039864361286163\n",
            "Training:  Epoch No:  9 Iteration No:  727 \n",
            " Loss:  0.020291635766625404\n",
            "Training:  Epoch No:  9 Iteration No:  728 \n",
            " Loss:  0.0017726628575474024\n",
            "Training:  Epoch No:  9 Iteration No:  729 \n",
            " Loss:  0.004524983931332827\n",
            "Training:  Epoch No:  9 Iteration No:  730 \n",
            " Loss:  0.00703090988099575\n",
            "Training:  Epoch No:  9 Iteration No:  731 \n",
            " Loss:  0.0017407297855243087\n",
            "Training:  Epoch No:  9 Iteration No:  732 \n",
            " Loss:  0.0014586192555725574\n",
            "Training:  Epoch No:  9 Iteration No:  733 \n",
            " Loss:  0.0004926571855321527\n",
            "Training:  Epoch No:  9 Iteration No:  734 \n",
            " Loss:  0.021668877452611923\n",
            "Training:  Epoch No:  9 Iteration No:  735 \n",
            " Loss:  0.006391369272023439\n",
            "Training:  Epoch No:  9 Iteration No:  736 \n",
            " Loss:  0.0019833294209092855\n",
            "Training:  Epoch No:  9 Iteration No:  737 \n",
            " Loss:  0.00043274936615489423\n",
            "Training:  Epoch No:  9 Iteration No:  738 \n",
            " Loss:  0.0014637275598943233\n",
            "Training:  Epoch No:  9 Iteration No:  739 \n",
            " Loss:  0.0009484156616963446\n",
            "Training:  Epoch No:  9 Iteration No:  740 \n",
            " Loss:  0.03841357305645943\n",
            "Training:  Epoch No:  9 Iteration No:  741 \n",
            " Loss:  0.07287944853305817\n",
            "Training:  Epoch No:  9 Iteration No:  742 \n",
            " Loss:  0.004478853195905685\n",
            "Training:  Epoch No:  9 Iteration No:  743 \n",
            " Loss:  0.0029368740506470203\n",
            "Training:  Epoch No:  9 Iteration No:  744 \n",
            " Loss:  0.06020263582468033\n",
            "Training:  Epoch No:  9 Iteration No:  745 \n",
            " Loss:  0.011491232551634312\n",
            "Training:  Epoch No:  9 Iteration No:  746 \n",
            " Loss:  0.002145640552043915\n",
            "Training:  Epoch No:  9 Iteration No:  747 \n",
            " Loss:  0.0015892802039161325\n",
            "Training:  Epoch No:  9 Iteration No:  748 \n",
            " Loss:  0.009620246477425098\n",
            "Training:  Epoch No:  9 Iteration No:  749 \n",
            " Loss:  0.002374475821852684\n",
            "Training:  Epoch No:  9 Iteration No:  750 \n",
            " Loss:  0.005306019447743893\n",
            "Training:  Epoch No:  9 Iteration No:  751 \n",
            " Loss:  0.002418086864054203\n",
            "Training:  Epoch No:  9 Iteration No:  752 \n",
            " Loss:  0.0034409454092383385\n",
            "Training:  Epoch No:  9 Iteration No:  753 \n",
            " Loss:  0.010439816862344742\n",
            "Training:  Epoch No:  9 Iteration No:  754 \n",
            " Loss:  0.0015077117132022977\n",
            "Training:  Epoch No:  9 Iteration No:  755 \n",
            " Loss:  0.007787806447595358\n",
            "Training:  Epoch No:  9 Iteration No:  756 \n",
            " Loss:  0.0008250061655417085\n",
            "Training:  Epoch No:  9 Iteration No:  757 \n",
            " Loss:  0.00039228482637554407\n",
            "Training:  Epoch No:  9 Iteration No:  758 \n",
            " Loss:  0.0011677250731736422\n",
            "Training:  Epoch No:  9 Iteration No:  759 \n",
            " Loss:  0.006867369171231985\n",
            "Training:  Epoch No:  9 Iteration No:  760 \n",
            " Loss:  0.0009655922185629606\n",
            "Training:  Epoch No:  9 Iteration No:  761 \n",
            " Loss:  0.0033988661598414183\n",
            "Training:  Epoch No:  9 Iteration No:  762 \n",
            " Loss:  0.006407848093658686\n",
            "Training:  Epoch No:  9 Iteration No:  763 \n",
            " Loss:  0.002304650843143463\n",
            "Training:  Epoch No:  9 Iteration No:  764 \n",
            " Loss:  0.0008896403014659882\n",
            "Training:  Epoch No:  9 Iteration No:  765 \n",
            " Loss:  0.001025482313707471\n",
            "Training:  Epoch No:  9 Iteration No:  766 \n",
            " Loss:  0.008983798325061798\n",
            "Training:  Epoch No:  9 Iteration No:  767 \n",
            " Loss:  0.0024465140886604786\n",
            "Training:  Epoch No:  9 Iteration No:  768 \n",
            " Loss:  9.044900070875883e-05\n",
            "Training:  Epoch No:  9 Iteration No:  769 \n",
            " Loss:  0.0013129962608218193\n",
            "Training:  Epoch No:  9 Iteration No:  770 \n",
            " Loss:  0.0016731100622564554\n",
            "Training:  Epoch No:  9 Iteration No:  771 \n",
            " Loss:  0.04142817109823227\n",
            "Training:  Epoch No:  9 Iteration No:  772 \n",
            " Loss:  0.00014270337123889476\n",
            "Training:  Epoch No:  9 Iteration No:  773 \n",
            " Loss:  0.001556634553708136\n",
            "Training:  Epoch No:  9 Iteration No:  774 \n",
            " Loss:  0.0019283803412690759\n",
            "Training:  Epoch No:  9 Iteration No:  775 \n",
            " Loss:  0.004867121111601591\n",
            "Training:  Epoch No:  9 Iteration No:  776 \n",
            " Loss:  0.0002596916747279465\n",
            "Training:  Epoch No:  9 Iteration No:  777 \n",
            " Loss:  0.0008420643862336874\n",
            "Training:  Epoch No:  9 Iteration No:  778 \n",
            " Loss:  0.013103961013257504\n",
            "Training:  Epoch No:  9 Iteration No:  779 \n",
            " Loss:  0.0006308660376816988\n",
            "Training:  Epoch No:  9 Iteration No:  780 \n",
            " Loss:  0.0014868672005832195\n",
            "Training:  Epoch No:  9 Iteration No:  781 \n",
            " Loss:  0.00043531390838325024\n",
            "Training:  Epoch No:  9 Iteration No:  782 \n",
            " Loss:  0.00023509513994213194\n",
            "Training:  Epoch No:  9 Iteration No:  783 \n",
            " Loss:  0.000496113148983568\n",
            "Training:  Epoch No:  9 Iteration No:  784 \n",
            " Loss:  0.0001342451578238979\n",
            "Training:  Epoch No:  9 Iteration No:  785 \n",
            " Loss:  0.0023819447960704565\n",
            "Training:  Epoch No:  9 Iteration No:  786 \n",
            " Loss:  0.004570113029330969\n",
            "Training:  Epoch No:  9 Iteration No:  787 \n",
            " Loss:  0.0037531692069023848\n",
            "Training:  Epoch No:  9 Iteration No:  788 \n",
            " Loss:  0.0029467428103089333\n",
            "Training:  Epoch No:  9 Iteration No:  789 \n",
            " Loss:  0.002321838401257992\n",
            "Training:  Epoch No:  9 Iteration No:  790 \n",
            " Loss:  0.000841091328766197\n",
            "Training:  Epoch No:  9 Iteration No:  791 \n",
            " Loss:  0.004333286080509424\n",
            "Training:  Epoch No:  9 Iteration No:  792 \n",
            " Loss:  0.0020056739449501038\n",
            "Training:  Epoch No:  9 Iteration No:  793 \n",
            " Loss:  0.011491416022181511\n",
            "Training:  Epoch No:  9 Iteration No:  794 \n",
            " Loss:  6.797152309445664e-05\n",
            "Training:  Epoch No:  9 Iteration No:  795 \n",
            " Loss:  0.07306932657957077\n",
            "Training:  Epoch No:  9 Iteration No:  796 \n",
            " Loss:  0.0002850698947440833\n",
            "Training:  Epoch No:  9 Iteration No:  797 \n",
            " Loss:  0.015906699001789093\n",
            "Training:  Epoch No:  9 Iteration No:  798 \n",
            " Loss:  0.0009729568846523762\n",
            "Training:  Epoch No:  9 Iteration No:  799 \n",
            " Loss:  0.00041025353129953146\n",
            "Training:  Epoch No:  9 Iteration No:  800 \n",
            " Loss:  0.020699506625533104\n",
            "Training:  Epoch No:  9 Iteration No:  801 \n",
            " Loss:  0.08199813961982727\n",
            "Training:  Epoch No:  9 Iteration No:  802 \n",
            " Loss:  0.00027020403649657965\n",
            "Training:  Epoch No:  9 Iteration No:  803 \n",
            " Loss:  0.026810482144355774\n",
            "Training:  Epoch No:  9 Iteration No:  804 \n",
            " Loss:  0.006035748403519392\n",
            "Training:  Epoch No:  9 Iteration No:  805 \n",
            " Loss:  0.014699267223477364\n",
            "Training:  Epoch No:  9 Iteration No:  806 \n",
            " Loss:  0.0012427682522684336\n",
            "Training:  Epoch No:  9 Iteration No:  807 \n",
            " Loss:  0.049645353108644485\n",
            "Training:  Epoch No:  9 Iteration No:  808 \n",
            " Loss:  0.002041720785200596\n",
            "Training:  Epoch No:  9 Iteration No:  809 \n",
            " Loss:  0.0284162275493145\n",
            "Training:  Epoch No:  9 Iteration No:  810 \n",
            " Loss:  0.0010234614601358771\n",
            "Training:  Epoch No:  9 Iteration No:  811 \n",
            " Loss:  0.00282823434099555\n",
            "Training:  Epoch No:  9 Iteration No:  812 \n",
            " Loss:  0.010683114640414715\n",
            "Training:  Epoch No:  9 Iteration No:  813 \n",
            " Loss:  0.0016191302565857768\n",
            "Training:  Epoch No:  9 Iteration No:  814 \n",
            " Loss:  0.009727340191602707\n",
            "Training:  Epoch No:  9 Iteration No:  815 \n",
            " Loss:  0.049599334597587585\n",
            "Training:  Epoch No:  9 Iteration No:  816 \n",
            " Loss:  0.0006213096203282475\n",
            "Training:  Epoch No:  9 Iteration No:  817 \n",
            " Loss:  0.0003324124845676124\n",
            "Training:  Epoch No:  9 Iteration No:  818 \n",
            " Loss:  0.04378598928451538\n",
            "Training:  Epoch No:  9 Iteration No:  819 \n",
            " Loss:  0.018393389880657196\n",
            "Training:  Epoch No:  9 Iteration No:  820 \n",
            " Loss:  0.024970807135105133\n",
            "Training:  Epoch No:  9 Iteration No:  821 \n",
            " Loss:  0.003072927938774228\n",
            "Training:  Epoch No:  9 Iteration No:  822 \n",
            " Loss:  0.004763328470289707\n",
            "Training:  Epoch No:  9 Iteration No:  823 \n",
            " Loss:  0.0020000403746962547\n",
            "Training:  Epoch No:  9 Iteration No:  824 \n",
            " Loss:  0.001446746988222003\n",
            "Training:  Epoch No:  9 Iteration No:  825 \n",
            " Loss:  0.009009983390569687\n",
            "Training:  Epoch No:  9 Iteration No:  826 \n",
            " Loss:  0.0007510074065066874\n",
            "Training:  Epoch No:  9 Iteration No:  827 \n",
            " Loss:  0.0003965679497923702\n",
            "Training:  Epoch No:  9 Iteration No:  828 \n",
            " Loss:  0.0011799921048805118\n",
            "Training:  Epoch No:  9 Iteration No:  829 \n",
            " Loss:  0.03567163646221161\n",
            "Training:  Epoch No:  9 Iteration No:  830 \n",
            " Loss:  0.011565311811864376\n",
            "Training:  Epoch No:  9 Iteration No:  831 \n",
            " Loss:  0.005767757073044777\n",
            "Training:  Epoch No:  9 Iteration No:  832 \n",
            " Loss:  0.003905630437657237\n",
            "Training:  Epoch No:  9 Iteration No:  833 \n",
            " Loss:  0.0024049764033406973\n",
            "Training:  Epoch No:  9 Iteration No:  834 \n",
            " Loss:  0.0030091353692114353\n",
            "Training:  Epoch No:  9 Iteration No:  835 \n",
            " Loss:  0.0007559515652246773\n",
            "Training:  Epoch No:  9 Iteration No:  836 \n",
            " Loss:  0.004909386858344078\n",
            "Training:  Epoch No:  9 Iteration No:  837 \n",
            " Loss:  0.000487400742713362\n",
            "Training:  Epoch No:  9 Iteration No:  838 \n",
            " Loss:  0.0010912667494267225\n",
            "Training:  Epoch No:  9 Iteration No:  839 \n",
            " Loss:  0.004369855392724276\n",
            "Training:  Epoch No:  9 Iteration No:  840 \n",
            " Loss:  0.0006514009437523782\n",
            "Training:  Epoch No:  9 Iteration No:  841 \n",
            " Loss:  0.0017415398033335805\n",
            "Training:  Epoch No:  9 Iteration No:  842 \n",
            " Loss:  0.0011896091746166348\n",
            "Training:  Epoch No:  9 Iteration No:  843 \n",
            " Loss:  0.003715889761224389\n",
            "Training:  Epoch No:  9 Iteration No:  844 \n",
            " Loss:  0.003352173836901784\n",
            "Training:  Epoch No:  9 Iteration No:  845 \n",
            " Loss:  0.0006470112130045891\n",
            "Training:  Epoch No:  9 Iteration No:  846 \n",
            " Loss:  0.0005200954619795084\n",
            "Training:  Epoch No:  9 Iteration No:  847 \n",
            " Loss:  0.0006696673226542771\n",
            "Training:  Epoch No:  9 Iteration No:  848 \n",
            " Loss:  0.0024004054721444845\n",
            "Training:  Epoch No:  9 Iteration No:  849 \n",
            " Loss:  0.034764327108860016\n",
            "Training:  Epoch No:  9 Iteration No:  850 \n",
            " Loss:  0.004840446170419455\n",
            "Training:  Epoch No:  9 Iteration No:  851 \n",
            " Loss:  0.0023993023205548525\n",
            "Training:  Epoch No:  9 Iteration No:  852 \n",
            " Loss:  0.02176239900290966\n",
            "Training:  Epoch No:  9 Iteration No:  853 \n",
            " Loss:  0.01256156712770462\n",
            "Training:  Epoch No:  9 Iteration No:  854 \n",
            " Loss:  0.005300106015056372\n",
            "Training:  Epoch No:  9 Iteration No:  855 \n",
            " Loss:  0.007406102027744055\n",
            "Training:  Epoch No:  9 Iteration No:  856 \n",
            " Loss:  0.003695371560752392\n",
            "Training:  Epoch No:  9 Iteration No:  857 \n",
            " Loss:  0.0005807376001030207\n",
            "Training:  Epoch No:  9 Iteration No:  858 \n",
            " Loss:  0.0010126476408913732\n",
            "Training:  Epoch No:  9 Iteration No:  859 \n",
            " Loss:  0.017386576160788536\n",
            "Training:  Epoch No:  9 Iteration No:  860 \n",
            " Loss:  0.006890770513564348\n",
            "Training:  Epoch No:  9 Iteration No:  861 \n",
            " Loss:  0.0009817045647650957\n",
            "Training:  Epoch No:  9 Iteration No:  862 \n",
            " Loss:  0.0008347642724402249\n",
            "Training:  Epoch No:  9 Iteration No:  863 \n",
            " Loss:  0.0015511747915297747\n",
            "Training:  Epoch No:  9 Iteration No:  864 \n",
            " Loss:  0.0009517414728179574\n",
            "Training:  Epoch No:  9 Iteration No:  865 \n",
            " Loss:  0.0018920900765806437\n",
            "Training:  Epoch No:  9 Iteration No:  866 \n",
            " Loss:  0.002619383158162236\n",
            "Training:  Epoch No:  9 Iteration No:  867 \n",
            " Loss:  0.0017974182264879346\n",
            "Training:  Epoch No:  9 Iteration No:  868 \n",
            " Loss:  0.0006130477413535118\n",
            "Training:  Epoch No:  9 Iteration No:  869 \n",
            " Loss:  0.0010175550123676658\n",
            "Training:  Epoch No:  9 Iteration No:  870 \n",
            " Loss:  0.0007805775385349989\n",
            "Training:  Epoch No:  9 Iteration No:  871 \n",
            " Loss:  0.0019285661401227117\n",
            "Training:  Epoch No:  9 Iteration No:  872 \n",
            " Loss:  0.012539364397525787\n",
            "Training:  Epoch No:  9 Iteration No:  873 \n",
            " Loss:  0.0004747033817693591\n",
            "Training:  Epoch No:  9 Iteration No:  874 \n",
            " Loss:  0.0013716531684622169\n",
            "Training:  Epoch No:  9 Iteration No:  875 \n",
            " Loss:  0.000742528063710779\n",
            "Training:  Epoch No:  9 Iteration No:  876 \n",
            " Loss:  0.006704089231789112\n",
            "Training:  Epoch No:  9 Iteration No:  877 \n",
            " Loss:  0.0022896211594343185\n",
            "Training:  Epoch No:  9 Iteration No:  878 \n",
            " Loss:  0.0010135952616110444\n",
            "Training:  Epoch No:  9 Iteration No:  879 \n",
            " Loss:  0.014268247410655022\n",
            "Training:  Epoch No:  9 Iteration No:  880 \n",
            " Loss:  0.0028328364714980125\n",
            "Training:  Epoch No:  9 Iteration No:  881 \n",
            " Loss:  0.0003191577852703631\n",
            "Training:  Epoch No:  9 Iteration No:  882 \n",
            " Loss:  0.00016292378131765872\n",
            "Training:  Epoch No:  9 Iteration No:  883 \n",
            " Loss:  0.0008129241759888828\n",
            "Training:  Epoch No:  9 Iteration No:  884 \n",
            " Loss:  0.0028457946609705687\n",
            "Training:  Epoch No:  9 Iteration No:  885 \n",
            " Loss:  0.001739364699460566\n",
            "Training:  Epoch No:  9 Iteration No:  886 \n",
            " Loss:  0.007057718932628632\n",
            "Training:  Epoch No:  9 Iteration No:  887 \n",
            " Loss:  0.0027549993246793747\n",
            "Training:  Epoch No:  9 Iteration No:  888 \n",
            " Loss:  0.0005866688443347812\n",
            "Training:  Epoch No:  9 Iteration No:  889 \n",
            " Loss:  0.0020354478619992733\n",
            "Training:  Epoch No:  9 Iteration No:  890 \n",
            " Loss:  0.0076803346164524555\n",
            "Training:  Epoch No:  9 Iteration No:  891 \n",
            " Loss:  0.0008851657621562481\n",
            "Training:  Epoch No:  9 Iteration No:  892 \n",
            " Loss:  0.0020007225684821606\n",
            "Training:  Epoch No:  9 Iteration No:  893 \n",
            " Loss:  0.0005764813395217061\n",
            "Training:  Epoch No:  9 Iteration No:  894 \n",
            " Loss:  0.0012669850839301944\n",
            "Training:  Epoch No:  9 Iteration No:  895 \n",
            " Loss:  0.0016508189728483558\n",
            "Training:  Epoch No:  9 Iteration No:  896 \n",
            " Loss:  0.0006648453418165445\n",
            "Training:  Epoch No:  9 Iteration No:  897 \n",
            " Loss:  0.0004659139667637646\n",
            "Training:  Epoch No:  9 Iteration No:  898 \n",
            " Loss:  0.0009841786231845617\n",
            "Training:  Epoch No:  9 Iteration No:  899 \n",
            " Loss:  0.0013722903095185757\n",
            "Training:  Epoch No:  9 Iteration No:  900 \n",
            " Loss:  0.003953681793063879\n",
            "Training:  Epoch No:  9 Iteration No:  901 \n",
            " Loss:  0.0012096207356080413\n",
            "Training:  Epoch No:  9 Iteration No:  902 \n",
            " Loss:  0.0030008507892489433\n",
            "Training:  Epoch No:  9 Iteration No:  903 \n",
            " Loss:  0.018330493941903114\n",
            "Training:  Epoch No:  9 Iteration No:  904 \n",
            " Loss:  0.0009135722648352385\n",
            "Training:  Epoch No:  9 Iteration No:  905 \n",
            " Loss:  0.020151609554886818\n",
            "Training:  Epoch No:  9 Iteration No:  906 \n",
            " Loss:  0.0003491491952445358\n",
            "Training:  Epoch No:  9 Iteration No:  907 \n",
            " Loss:  0.0017920356476679444\n",
            "Training:  Epoch No:  9 Iteration No:  908 \n",
            " Loss:  0.0014454401098191738\n",
            "Training:  Epoch No:  9 Iteration No:  909 \n",
            " Loss:  0.0366986021399498\n",
            "Training:  Epoch No:  9 Iteration No:  910 \n",
            " Loss:  0.0023429442662745714\n",
            "Training:  Epoch No:  9 Iteration No:  911 \n",
            " Loss:  0.0002131061628460884\n",
            "Training:  Epoch No:  9 Iteration No:  912 \n",
            " Loss:  0.005991063546389341\n",
            "Training:  Epoch No:  9 Iteration No:  913 \n",
            " Loss:  0.04912346601486206\n",
            "Training:  Epoch No:  9 Iteration No:  914 \n",
            " Loss:  0.0013283046428114176\n",
            "Training:  Epoch No:  9 Iteration No:  915 \n",
            " Loss:  0.001768202637322247\n",
            "Training:  Epoch No:  9 Iteration No:  916 \n",
            " Loss:  0.002148167695850134\n",
            "Training:  Epoch No:  9 Iteration No:  917 \n",
            " Loss:  0.0044943164102733135\n",
            "Training:  Epoch No:  9 Iteration No:  918 \n",
            " Loss:  0.01622793823480606\n",
            "Training:  Epoch No:  9 Iteration No:  919 \n",
            " Loss:  0.0029844902455806732\n",
            "Training:  Epoch No:  9 Iteration No:  920 \n",
            " Loss:  0.012549865990877151\n",
            "Training:  Epoch No:  9 Iteration No:  921 \n",
            " Loss:  5.21519614267163e-05\n",
            "Training:  Epoch No:  9 Iteration No:  922 \n",
            " Loss:  0.0005102574941702187\n",
            "Training:  Epoch No:  9 Iteration No:  923 \n",
            " Loss:  0.0023975376971066\n",
            "Training:  Epoch No:  9 Iteration No:  924 \n",
            " Loss:  0.018620358780026436\n",
            "Training:  Epoch No:  9 Iteration No:  925 \n",
            " Loss:  0.00039108796045184135\n",
            "Training:  Epoch No:  9 Iteration No:  926 \n",
            " Loss:  0.0023504244163632393\n",
            "Training:  Epoch No:  9 Iteration No:  927 \n",
            " Loss:  0.0008019551169127226\n",
            "Training:  Epoch No:  9 Iteration No:  928 \n",
            " Loss:  0.00014155056851450354\n",
            "Training:  Epoch No:  9 Iteration No:  929 \n",
            " Loss:  0.0022096307948231697\n",
            "Training:  Epoch No:  9 Iteration No:  930 \n",
            " Loss:  0.0022452420089393854\n",
            "Training:  Epoch No:  9 Iteration No:  931 \n",
            " Loss:  0.00781101593747735\n",
            "Training:  Epoch No:  9 Iteration No:  932 \n",
            " Loss:  0.0009699816000647843\n",
            "Training:  Epoch No:  9 Iteration No:  933 \n",
            " Loss:  0.00015661664656363428\n",
            "Training:  Epoch No:  9 Iteration No:  934 \n",
            " Loss:  0.0030388596933335066\n",
            "Training:  Epoch No:  9 Iteration No:  935 \n",
            " Loss:  0.0002349254791624844\n",
            "Training:  Epoch No:  9 Iteration No:  936 \n",
            " Loss:  0.000327038491377607\n",
            "Training:  Epoch No:  9 Iteration No:  937 \n",
            " Loss:  0.00019843209884129465\n",
            "Training:  Epoch No:  9 Iteration No:  938 \n",
            " Loss:  0.0003673494211398065\n",
            "Validation:  Epoch No:  9 \n",
            " Loss:  0.03507568603316532\n",
            "validation accuracy:  98.94\n",
            "Training:  Epoch No:  10 Iteration No:  1 \n",
            " Loss:  0.0020722397603094578\n",
            "Training:  Epoch No:  10 Iteration No:  2 \n",
            " Loss:  0.0066908192820847034\n",
            "Training:  Epoch No:  10 Iteration No:  3 \n",
            " Loss:  0.009612624533474445\n",
            "Training:  Epoch No:  10 Iteration No:  4 \n",
            " Loss:  0.0007797982543706894\n",
            "Training:  Epoch No:  10 Iteration No:  5 \n",
            " Loss:  0.0015687673585489392\n",
            "Training:  Epoch No:  10 Iteration No:  6 \n",
            " Loss:  0.004133198875933886\n",
            "Training:  Epoch No:  10 Iteration No:  7 \n",
            " Loss:  0.002672453410923481\n",
            "Training:  Epoch No:  10 Iteration No:  8 \n",
            " Loss:  0.005698015447705984\n",
            "Training:  Epoch No:  10 Iteration No:  9 \n",
            " Loss:  0.0003243717073928565\n",
            "Training:  Epoch No:  10 Iteration No:  10 \n",
            " Loss:  0.0020175327081233263\n",
            "Training:  Epoch No:  10 Iteration No:  11 \n",
            " Loss:  0.0029921710956841707\n",
            "Training:  Epoch No:  10 Iteration No:  12 \n",
            " Loss:  0.0014799293130636215\n",
            "Training:  Epoch No:  10 Iteration No:  13 \n",
            " Loss:  0.0017960593104362488\n",
            "Training:  Epoch No:  10 Iteration No:  14 \n",
            " Loss:  0.0010151544120162725\n",
            "Training:  Epoch No:  10 Iteration No:  15 \n",
            " Loss:  0.00044800020987167954\n",
            "Training:  Epoch No:  10 Iteration No:  16 \n",
            " Loss:  0.001530937966890633\n",
            "Training:  Epoch No:  10 Iteration No:  17 \n",
            " Loss:  0.0007043338846415281\n",
            "Training:  Epoch No:  10 Iteration No:  18 \n",
            " Loss:  0.0004555720661301166\n",
            "Training:  Epoch No:  10 Iteration No:  19 \n",
            " Loss:  0.00011331698624417186\n",
            "Training:  Epoch No:  10 Iteration No:  20 \n",
            " Loss:  0.009054911322891712\n",
            "Training:  Epoch No:  10 Iteration No:  21 \n",
            " Loss:  0.000369083893019706\n",
            "Training:  Epoch No:  10 Iteration No:  22 \n",
            " Loss:  0.004577732644975185\n",
            "Training:  Epoch No:  10 Iteration No:  23 \n",
            " Loss:  0.002321788342669606\n",
            "Training:  Epoch No:  10 Iteration No:  24 \n",
            " Loss:  0.0009261844679713249\n",
            "Training:  Epoch No:  10 Iteration No:  25 \n",
            " Loss:  0.0006876337574794888\n",
            "Training:  Epoch No:  10 Iteration No:  26 \n",
            " Loss:  0.00025622951216064394\n",
            "Training:  Epoch No:  10 Iteration No:  27 \n",
            " Loss:  0.023176753893494606\n",
            "Training:  Epoch No:  10 Iteration No:  28 \n",
            " Loss:  0.0006245069671422243\n",
            "Training:  Epoch No:  10 Iteration No:  29 \n",
            " Loss:  0.0015667316038161516\n",
            "Training:  Epoch No:  10 Iteration No:  30 \n",
            " Loss:  0.0009510712116025388\n",
            "Training:  Epoch No:  10 Iteration No:  31 \n",
            " Loss:  0.004854266531765461\n",
            "Training:  Epoch No:  10 Iteration No:  32 \n",
            " Loss:  0.0006333303172141314\n",
            "Training:  Epoch No:  10 Iteration No:  33 \n",
            " Loss:  0.005019677337259054\n",
            "Training:  Epoch No:  10 Iteration No:  34 \n",
            " Loss:  0.00015686877304688096\n",
            "Training:  Epoch No:  10 Iteration No:  35 \n",
            " Loss:  0.00033056215033866465\n",
            "Training:  Epoch No:  10 Iteration No:  36 \n",
            " Loss:  0.00031889567617326975\n",
            "Training:  Epoch No:  10 Iteration No:  37 \n",
            " Loss:  0.0020717696752399206\n",
            "Training:  Epoch No:  10 Iteration No:  38 \n",
            " Loss:  0.00048176752170547843\n",
            "Training:  Epoch No:  10 Iteration No:  39 \n",
            " Loss:  0.0018149521201848984\n",
            "Training:  Epoch No:  10 Iteration No:  40 \n",
            " Loss:  0.00038698973366990685\n",
            "Training:  Epoch No:  10 Iteration No:  41 \n",
            " Loss:  0.005200007930397987\n",
            "Training:  Epoch No:  10 Iteration No:  42 \n",
            " Loss:  0.0010705116437748075\n",
            "Training:  Epoch No:  10 Iteration No:  43 \n",
            " Loss:  0.0029503023251891136\n",
            "Training:  Epoch No:  10 Iteration No:  44 \n",
            " Loss:  0.03329867497086525\n",
            "Training:  Epoch No:  10 Iteration No:  45 \n",
            " Loss:  0.0004492904990911484\n",
            "Training:  Epoch No:  10 Iteration No:  46 \n",
            " Loss:  0.0011488896561786532\n",
            "Training:  Epoch No:  10 Iteration No:  47 \n",
            " Loss:  0.0008023499976843596\n",
            "Training:  Epoch No:  10 Iteration No:  48 \n",
            " Loss:  5.834385956404731e-05\n",
            "Training:  Epoch No:  10 Iteration No:  49 \n",
            " Loss:  0.012348995544016361\n",
            "Training:  Epoch No:  10 Iteration No:  50 \n",
            " Loss:  0.0007112936000339687\n",
            "Training:  Epoch No:  10 Iteration No:  51 \n",
            " Loss:  0.0005257045850157738\n",
            "Training:  Epoch No:  10 Iteration No:  52 \n",
            " Loss:  0.000980439712293446\n",
            "Training:  Epoch No:  10 Iteration No:  53 \n",
            " Loss:  0.05629082769155502\n",
            "Training:  Epoch No:  10 Iteration No:  54 \n",
            " Loss:  0.0004187939048279077\n",
            "Training:  Epoch No:  10 Iteration No:  55 \n",
            " Loss:  0.0013723147567361593\n",
            "Training:  Epoch No:  10 Iteration No:  56 \n",
            " Loss:  0.008517432026565075\n",
            "Training:  Epoch No:  10 Iteration No:  57 \n",
            " Loss:  0.004849293269217014\n",
            "Training:  Epoch No:  10 Iteration No:  58 \n",
            " Loss:  0.00121894886251539\n",
            "Training:  Epoch No:  10 Iteration No:  59 \n",
            " Loss:  0.0037660952657461166\n",
            "Training:  Epoch No:  10 Iteration No:  60 \n",
            " Loss:  0.0012331547914072871\n",
            "Training:  Epoch No:  10 Iteration No:  61 \n",
            " Loss:  0.00033955639810301363\n",
            "Training:  Epoch No:  10 Iteration No:  62 \n",
            " Loss:  0.0015349184395745397\n",
            "Training:  Epoch No:  10 Iteration No:  63 \n",
            " Loss:  0.0005888190353289247\n",
            "Training:  Epoch No:  10 Iteration No:  64 \n",
            " Loss:  0.0037410566583275795\n",
            "Training:  Epoch No:  10 Iteration No:  65 \n",
            " Loss:  0.045686859637498856\n",
            "Training:  Epoch No:  10 Iteration No:  66 \n",
            " Loss:  0.0001844997750595212\n",
            "Training:  Epoch No:  10 Iteration No:  67 \n",
            " Loss:  0.0014893963234499097\n",
            "Training:  Epoch No:  10 Iteration No:  68 \n",
            " Loss:  0.0053188069723546505\n",
            "Training:  Epoch No:  10 Iteration No:  69 \n",
            " Loss:  0.0018474800745025277\n",
            "Training:  Epoch No:  10 Iteration No:  70 \n",
            " Loss:  0.003708617528900504\n",
            "Training:  Epoch No:  10 Iteration No:  71 \n",
            " Loss:  0.0011122850701212883\n",
            "Training:  Epoch No:  10 Iteration No:  72 \n",
            " Loss:  0.007004532031714916\n",
            "Training:  Epoch No:  10 Iteration No:  73 \n",
            " Loss:  0.0045363097451627254\n",
            "Training:  Epoch No:  10 Iteration No:  74 \n",
            " Loss:  0.0005612103850580752\n",
            "Training:  Epoch No:  10 Iteration No:  75 \n",
            " Loss:  0.0006758656236343086\n",
            "Training:  Epoch No:  10 Iteration No:  76 \n",
            " Loss:  0.0007001063786447048\n",
            "Training:  Epoch No:  10 Iteration No:  77 \n",
            " Loss:  0.0014005874982103705\n",
            "Training:  Epoch No:  10 Iteration No:  78 \n",
            " Loss:  0.0019895138684660196\n",
            "Training:  Epoch No:  10 Iteration No:  79 \n",
            " Loss:  0.0010985557455569506\n",
            "Training:  Epoch No:  10 Iteration No:  80 \n",
            " Loss:  0.01560708973556757\n",
            "Training:  Epoch No:  10 Iteration No:  81 \n",
            " Loss:  0.0029736002907156944\n",
            "Training:  Epoch No:  10 Iteration No:  82 \n",
            " Loss:  0.001993725309148431\n",
            "Training:  Epoch No:  10 Iteration No:  83 \n",
            " Loss:  0.0155414417386055\n",
            "Training:  Epoch No:  10 Iteration No:  84 \n",
            " Loss:  0.00861454475671053\n",
            "Training:  Epoch No:  10 Iteration No:  85 \n",
            " Loss:  0.045991551131010056\n",
            "Training:  Epoch No:  10 Iteration No:  86 \n",
            " Loss:  0.003114003222435713\n",
            "Training:  Epoch No:  10 Iteration No:  87 \n",
            " Loss:  0.0008706519729457796\n",
            "Training:  Epoch No:  10 Iteration No:  88 \n",
            " Loss:  0.0005057533853687346\n",
            "Training:  Epoch No:  10 Iteration No:  89 \n",
            " Loss:  0.0011107244063168764\n",
            "Training:  Epoch No:  10 Iteration No:  90 \n",
            " Loss:  0.00020022803801111877\n",
            "Training:  Epoch No:  10 Iteration No:  91 \n",
            " Loss:  0.00018290748994331807\n",
            "Training:  Epoch No:  10 Iteration No:  92 \n",
            " Loss:  0.0015313852345570922\n",
            "Training:  Epoch No:  10 Iteration No:  93 \n",
            " Loss:  0.0032866336405277252\n",
            "Training:  Epoch No:  10 Iteration No:  94 \n",
            " Loss:  0.0009055011905729771\n",
            "Training:  Epoch No:  10 Iteration No:  95 \n",
            " Loss:  0.001082965754903853\n",
            "Training:  Epoch No:  10 Iteration No:  96 \n",
            " Loss:  0.00015539306332357228\n",
            "Training:  Epoch No:  10 Iteration No:  97 \n",
            " Loss:  0.0014440513914451003\n",
            "Training:  Epoch No:  10 Iteration No:  98 \n",
            " Loss:  0.000751268642488867\n",
            "Training:  Epoch No:  10 Iteration No:  99 \n",
            " Loss:  0.0023749172687530518\n",
            "Training:  Epoch No:  10 Iteration No:  100 \n",
            " Loss:  0.0018783998675644398\n",
            "Training:  Epoch No:  10 Iteration No:  101 \n",
            " Loss:  0.0026789375115185976\n",
            "Training:  Epoch No:  10 Iteration No:  102 \n",
            " Loss:  0.000345234147971496\n",
            "Training:  Epoch No:  10 Iteration No:  103 \n",
            " Loss:  0.0027574854902923107\n",
            "Training:  Epoch No:  10 Iteration No:  104 \n",
            " Loss:  0.0009212339646182954\n",
            "Training:  Epoch No:  10 Iteration No:  105 \n",
            " Loss:  0.0018087434582412243\n",
            "Training:  Epoch No:  10 Iteration No:  106 \n",
            " Loss:  0.0022192043252289295\n",
            "Training:  Epoch No:  10 Iteration No:  107 \n",
            " Loss:  8.11162608442828e-05\n",
            "Training:  Epoch No:  10 Iteration No:  108 \n",
            " Loss:  0.002754735294729471\n",
            "Training:  Epoch No:  10 Iteration No:  109 \n",
            " Loss:  0.00023096617951523513\n",
            "Training:  Epoch No:  10 Iteration No:  110 \n",
            " Loss:  0.0011629161890596151\n",
            "Training:  Epoch No:  10 Iteration No:  111 \n",
            " Loss:  0.00015350047033280134\n",
            "Training:  Epoch No:  10 Iteration No:  112 \n",
            " Loss:  0.0015805377624928951\n",
            "Training:  Epoch No:  10 Iteration No:  113 \n",
            " Loss:  0.0010820418829098344\n",
            "Training:  Epoch No:  10 Iteration No:  114 \n",
            " Loss:  0.0013179421657696366\n",
            "Training:  Epoch No:  10 Iteration No:  115 \n",
            " Loss:  0.0006417901022359729\n",
            "Training:  Epoch No:  10 Iteration No:  116 \n",
            " Loss:  0.004074269440025091\n",
            "Training:  Epoch No:  10 Iteration No:  117 \n",
            " Loss:  0.0009062305907718837\n",
            "Training:  Epoch No:  10 Iteration No:  118 \n",
            " Loss:  0.001197222271002829\n",
            "Training:  Epoch No:  10 Iteration No:  119 \n",
            " Loss:  0.0013391132233664393\n",
            "Training:  Epoch No:  10 Iteration No:  120 \n",
            " Loss:  0.00129502615891397\n",
            "Training:  Epoch No:  10 Iteration No:  121 \n",
            " Loss:  0.005299054551869631\n",
            "Training:  Epoch No:  10 Iteration No:  122 \n",
            " Loss:  0.002328057074919343\n",
            "Training:  Epoch No:  10 Iteration No:  123 \n",
            " Loss:  0.0013614482013508677\n",
            "Training:  Epoch No:  10 Iteration No:  124 \n",
            " Loss:  0.0003452728851698339\n",
            "Training:  Epoch No:  10 Iteration No:  125 \n",
            " Loss:  0.0013779032742604613\n",
            "Training:  Epoch No:  10 Iteration No:  126 \n",
            " Loss:  0.008383304812014103\n",
            "Training:  Epoch No:  10 Iteration No:  127 \n",
            " Loss:  0.0006674643373116851\n",
            "Training:  Epoch No:  10 Iteration No:  128 \n",
            " Loss:  0.0005468514864332974\n",
            "Training:  Epoch No:  10 Iteration No:  129 \n",
            " Loss:  0.0009668217971920967\n",
            "Training:  Epoch No:  10 Iteration No:  130 \n",
            " Loss:  0.00017517209926154464\n",
            "Training:  Epoch No:  10 Iteration No:  131 \n",
            " Loss:  0.00020049534214194864\n",
            "Training:  Epoch No:  10 Iteration No:  132 \n",
            " Loss:  6.945538916625082e-05\n",
            "Training:  Epoch No:  10 Iteration No:  133 \n",
            " Loss:  0.0006620535277761519\n",
            "Training:  Epoch No:  10 Iteration No:  134 \n",
            " Loss:  0.0333419032394886\n",
            "Training:  Epoch No:  10 Iteration No:  135 \n",
            " Loss:  0.0008689328096807003\n",
            "Training:  Epoch No:  10 Iteration No:  136 \n",
            " Loss:  0.003937024623155594\n",
            "Training:  Epoch No:  10 Iteration No:  137 \n",
            " Loss:  0.0018303991528227925\n",
            "Training:  Epoch No:  10 Iteration No:  138 \n",
            " Loss:  0.00013505565584637225\n",
            "Training:  Epoch No:  10 Iteration No:  139 \n",
            " Loss:  0.0013523704838007689\n",
            "Training:  Epoch No:  10 Iteration No:  140 \n",
            " Loss:  0.006190283223986626\n",
            "Training:  Epoch No:  10 Iteration No:  141 \n",
            " Loss:  0.009804723784327507\n",
            "Training:  Epoch No:  10 Iteration No:  142 \n",
            " Loss:  0.010515417903661728\n",
            "Training:  Epoch No:  10 Iteration No:  143 \n",
            " Loss:  0.0023579054977744818\n",
            "Training:  Epoch No:  10 Iteration No:  144 \n",
            " Loss:  0.0012068809010088444\n",
            "Training:  Epoch No:  10 Iteration No:  145 \n",
            " Loss:  0.0001174101562355645\n",
            "Training:  Epoch No:  10 Iteration No:  146 \n",
            " Loss:  0.0008506777230650187\n",
            "Training:  Epoch No:  10 Iteration No:  147 \n",
            " Loss:  0.012962051667273045\n",
            "Training:  Epoch No:  10 Iteration No:  148 \n",
            " Loss:  0.00217448896728456\n",
            "Training:  Epoch No:  10 Iteration No:  149 \n",
            " Loss:  0.0001986713905353099\n",
            "Training:  Epoch No:  10 Iteration No:  150 \n",
            " Loss:  0.000453104090411216\n",
            "Training:  Epoch No:  10 Iteration No:  151 \n",
            " Loss:  0.025597980245947838\n",
            "Training:  Epoch No:  10 Iteration No:  152 \n",
            " Loss:  3.486905916361138e-05\n",
            "Training:  Epoch No:  10 Iteration No:  153 \n",
            " Loss:  0.000577979430090636\n",
            "Training:  Epoch No:  10 Iteration No:  154 \n",
            " Loss:  0.00036382253165356815\n",
            "Training:  Epoch No:  10 Iteration No:  155 \n",
            " Loss:  0.0025796741247177124\n",
            "Training:  Epoch No:  10 Iteration No:  156 \n",
            " Loss:  0.001442063832655549\n",
            "Training:  Epoch No:  10 Iteration No:  157 \n",
            " Loss:  0.0017220753943547606\n",
            "Training:  Epoch No:  10 Iteration No:  158 \n",
            " Loss:  0.0012026273179799318\n",
            "Training:  Epoch No:  10 Iteration No:  159 \n",
            " Loss:  0.00350067182444036\n",
            "Training:  Epoch No:  10 Iteration No:  160 \n",
            " Loss:  0.005331425927579403\n",
            "Training:  Epoch No:  10 Iteration No:  161 \n",
            " Loss:  9.919554577209055e-05\n",
            "Training:  Epoch No:  10 Iteration No:  162 \n",
            " Loss:  0.0002808349672704935\n",
            "Training:  Epoch No:  10 Iteration No:  163 \n",
            " Loss:  0.005630547180771828\n",
            "Training:  Epoch No:  10 Iteration No:  164 \n",
            " Loss:  0.06541747599840164\n",
            "Training:  Epoch No:  10 Iteration No:  165 \n",
            " Loss:  0.0007139872759580612\n",
            "Training:  Epoch No:  10 Iteration No:  166 \n",
            " Loss:  0.002865647431463003\n",
            "Training:  Epoch No:  10 Iteration No:  167 \n",
            " Loss:  0.008331332355737686\n",
            "Training:  Epoch No:  10 Iteration No:  168 \n",
            " Loss:  0.0003184653469361365\n",
            "Training:  Epoch No:  10 Iteration No:  169 \n",
            " Loss:  0.0015740642556920648\n",
            "Training:  Epoch No:  10 Iteration No:  170 \n",
            " Loss:  0.00027144604246132076\n",
            "Training:  Epoch No:  10 Iteration No:  171 \n",
            " Loss:  9.643315570428967e-05\n",
            "Training:  Epoch No:  10 Iteration No:  172 \n",
            " Loss:  0.0013672320637851954\n",
            "Training:  Epoch No:  10 Iteration No:  173 \n",
            " Loss:  0.0008992953225970268\n",
            "Training:  Epoch No:  10 Iteration No:  174 \n",
            " Loss:  0.0014593057567253709\n",
            "Training:  Epoch No:  10 Iteration No:  175 \n",
            " Loss:  0.002121970523148775\n",
            "Training:  Epoch No:  10 Iteration No:  176 \n",
            " Loss:  0.0004478078044485301\n",
            "Training:  Epoch No:  10 Iteration No:  177 \n",
            " Loss:  0.0341852605342865\n",
            "Training:  Epoch No:  10 Iteration No:  178 \n",
            " Loss:  0.002380114048719406\n",
            "Training:  Epoch No:  10 Iteration No:  179 \n",
            " Loss:  0.0003105417708866298\n",
            "Training:  Epoch No:  10 Iteration No:  180 \n",
            " Loss:  0.005378467496484518\n",
            "Training:  Epoch No:  10 Iteration No:  181 \n",
            " Loss:  0.0002739907067734748\n",
            "Training:  Epoch No:  10 Iteration No:  182 \n",
            " Loss:  0.0005960283451713622\n",
            "Training:  Epoch No:  10 Iteration No:  183 \n",
            " Loss:  0.0011587247718125582\n",
            "Training:  Epoch No:  10 Iteration No:  184 \n",
            " Loss:  0.002283959649503231\n",
            "Training:  Epoch No:  10 Iteration No:  185 \n",
            " Loss:  0.0021560019813477993\n",
            "Training:  Epoch No:  10 Iteration No:  186 \n",
            " Loss:  0.0033031352795660496\n",
            "Training:  Epoch No:  10 Iteration No:  187 \n",
            " Loss:  0.004196506459265947\n",
            "Training:  Epoch No:  10 Iteration No:  188 \n",
            " Loss:  0.004050880670547485\n",
            "Training:  Epoch No:  10 Iteration No:  189 \n",
            " Loss:  0.01687486469745636\n",
            "Training:  Epoch No:  10 Iteration No:  190 \n",
            " Loss:  0.0014392910525202751\n",
            "Training:  Epoch No:  10 Iteration No:  191 \n",
            " Loss:  0.0002865838469006121\n",
            "Training:  Epoch No:  10 Iteration No:  192 \n",
            " Loss:  0.00011151889339089394\n",
            "Training:  Epoch No:  10 Iteration No:  193 \n",
            " Loss:  0.003002262208610773\n",
            "Training:  Epoch No:  10 Iteration No:  194 \n",
            " Loss:  0.006037158891558647\n",
            "Training:  Epoch No:  10 Iteration No:  195 \n",
            " Loss:  0.004103015176951885\n",
            "Training:  Epoch No:  10 Iteration No:  196 \n",
            " Loss:  0.0005803579115308821\n",
            "Training:  Epoch No:  10 Iteration No:  197 \n",
            " Loss:  0.0045629506930708885\n",
            "Training:  Epoch No:  10 Iteration No:  198 \n",
            " Loss:  0.00017943452985491604\n",
            "Training:  Epoch No:  10 Iteration No:  199 \n",
            " Loss:  0.00014647688658442348\n",
            "Training:  Epoch No:  10 Iteration No:  200 \n",
            " Loss:  0.011181884445250034\n",
            "Training:  Epoch No:  10 Iteration No:  201 \n",
            " Loss:  0.0006245382246561348\n",
            "Training:  Epoch No:  10 Iteration No:  202 \n",
            " Loss:  0.01039528101682663\n",
            "Training:  Epoch No:  10 Iteration No:  203 \n",
            " Loss:  0.0033989634830504656\n",
            "Training:  Epoch No:  10 Iteration No:  204 \n",
            " Loss:  0.006324841640889645\n",
            "Training:  Epoch No:  10 Iteration No:  205 \n",
            " Loss:  0.0018687001429498196\n",
            "Training:  Epoch No:  10 Iteration No:  206 \n",
            " Loss:  9.409596714249346e-06\n",
            "Training:  Epoch No:  10 Iteration No:  207 \n",
            " Loss:  0.002577977953478694\n",
            "Training:  Epoch No:  10 Iteration No:  208 \n",
            " Loss:  0.00029122448177076876\n",
            "Training:  Epoch No:  10 Iteration No:  209 \n",
            " Loss:  0.0006038198480382562\n",
            "Training:  Epoch No:  10 Iteration No:  210 \n",
            " Loss:  0.03575550392270088\n",
            "Training:  Epoch No:  10 Iteration No:  211 \n",
            " Loss:  0.0014020336093381047\n",
            "Training:  Epoch No:  10 Iteration No:  212 \n",
            " Loss:  0.0053790295496582985\n",
            "Training:  Epoch No:  10 Iteration No:  213 \n",
            " Loss:  0.02709187939763069\n",
            "Training:  Epoch No:  10 Iteration No:  214 \n",
            " Loss:  0.0005900980322621763\n",
            "Training:  Epoch No:  10 Iteration No:  215 \n",
            " Loss:  0.0016056878957897425\n",
            "Training:  Epoch No:  10 Iteration No:  216 \n",
            " Loss:  0.0009182735811918974\n",
            "Training:  Epoch No:  10 Iteration No:  217 \n",
            " Loss:  0.00023838004563003778\n",
            "Training:  Epoch No:  10 Iteration No:  218 \n",
            " Loss:  0.0004212980857118964\n",
            "Training:  Epoch No:  10 Iteration No:  219 \n",
            " Loss:  0.0005205427878536284\n",
            "Training:  Epoch No:  10 Iteration No:  220 \n",
            " Loss:  0.008186377584934235\n",
            "Training:  Epoch No:  10 Iteration No:  221 \n",
            " Loss:  0.0004298655840102583\n",
            "Training:  Epoch No:  10 Iteration No:  222 \n",
            " Loss:  0.0008802495431154966\n",
            "Training:  Epoch No:  10 Iteration No:  223 \n",
            " Loss:  4.161192919127643e-05\n",
            "Training:  Epoch No:  10 Iteration No:  224 \n",
            " Loss:  0.0005381916416808963\n",
            "Training:  Epoch No:  10 Iteration No:  225 \n",
            " Loss:  0.006836932618170977\n",
            "Training:  Epoch No:  10 Iteration No:  226 \n",
            " Loss:  9.003198647405952e-05\n",
            "Training:  Epoch No:  10 Iteration No:  227 \n",
            " Loss:  0.00030721366056241095\n",
            "Training:  Epoch No:  10 Iteration No:  228 \n",
            " Loss:  0.001990013290196657\n",
            "Training:  Epoch No:  10 Iteration No:  229 \n",
            " Loss:  0.003683570772409439\n",
            "Training:  Epoch No:  10 Iteration No:  230 \n",
            " Loss:  0.01493597961962223\n",
            "Training:  Epoch No:  10 Iteration No:  231 \n",
            " Loss:  0.0001536985655548051\n",
            "Training:  Epoch No:  10 Iteration No:  232 \n",
            " Loss:  0.0017716289730742574\n",
            "Training:  Epoch No:  10 Iteration No:  233 \n",
            " Loss:  0.0024129729717969894\n",
            "Training:  Epoch No:  10 Iteration No:  234 \n",
            " Loss:  6.0036269132979214e-05\n",
            "Training:  Epoch No:  10 Iteration No:  235 \n",
            " Loss:  0.0008704138454049826\n",
            "Training:  Epoch No:  10 Iteration No:  236 \n",
            " Loss:  0.027615349739789963\n",
            "Training:  Epoch No:  10 Iteration No:  237 \n",
            " Loss:  0.0009986493969336152\n",
            "Training:  Epoch No:  10 Iteration No:  238 \n",
            " Loss:  0.0007352540851570666\n",
            "Training:  Epoch No:  10 Iteration No:  239 \n",
            " Loss:  0.00021954483236186206\n",
            "Training:  Epoch No:  10 Iteration No:  240 \n",
            " Loss:  0.00575646897777915\n",
            "Training:  Epoch No:  10 Iteration No:  241 \n",
            " Loss:  0.00032918964279815555\n",
            "Training:  Epoch No:  10 Iteration No:  242 \n",
            " Loss:  0.00020643160678446293\n",
            "Training:  Epoch No:  10 Iteration No:  243 \n",
            " Loss:  0.04470134526491165\n",
            "Training:  Epoch No:  10 Iteration No:  244 \n",
            " Loss:  0.0025585945695638657\n",
            "Training:  Epoch No:  10 Iteration No:  245 \n",
            " Loss:  0.002690514549612999\n",
            "Training:  Epoch No:  10 Iteration No:  246 \n",
            " Loss:  0.0008097761892713606\n",
            "Training:  Epoch No:  10 Iteration No:  247 \n",
            " Loss:  0.005114437080919743\n",
            "Training:  Epoch No:  10 Iteration No:  248 \n",
            " Loss:  0.0009582945494912565\n",
            "Training:  Epoch No:  10 Iteration No:  249 \n",
            " Loss:  0.0032819430343806744\n",
            "Training:  Epoch No:  10 Iteration No:  250 \n",
            " Loss:  0.00010132967872777954\n",
            "Training:  Epoch No:  10 Iteration No:  251 \n",
            " Loss:  0.09832584112882614\n",
            "Training:  Epoch No:  10 Iteration No:  252 \n",
            " Loss:  0.0010072840377688408\n",
            "Training:  Epoch No:  10 Iteration No:  253 \n",
            " Loss:  0.00400507589802146\n",
            "Training:  Epoch No:  10 Iteration No:  254 \n",
            " Loss:  0.001729642041027546\n",
            "Training:  Epoch No:  10 Iteration No:  255 \n",
            " Loss:  0.0009356077644042671\n",
            "Training:  Epoch No:  10 Iteration No:  256 \n",
            " Loss:  0.0027725135441869497\n",
            "Training:  Epoch No:  10 Iteration No:  257 \n",
            " Loss:  0.000557130144443363\n",
            "Training:  Epoch No:  10 Iteration No:  258 \n",
            " Loss:  0.0008696909644640982\n",
            "Training:  Epoch No:  10 Iteration No:  259 \n",
            " Loss:  0.002478476148098707\n",
            "Training:  Epoch No:  10 Iteration No:  260 \n",
            " Loss:  0.0010611125035211444\n",
            "Training:  Epoch No:  10 Iteration No:  261 \n",
            " Loss:  0.0009182265494018793\n",
            "Training:  Epoch No:  10 Iteration No:  262 \n",
            " Loss:  0.0012468229979276657\n",
            "Training:  Epoch No:  10 Iteration No:  263 \n",
            " Loss:  0.0013728413032367826\n",
            "Training:  Epoch No:  10 Iteration No:  264 \n",
            " Loss:  0.0008734616567380726\n",
            "Training:  Epoch No:  10 Iteration No:  265 \n",
            " Loss:  0.002515699714422226\n",
            "Training:  Epoch No:  10 Iteration No:  266 \n",
            " Loss:  0.007023629266768694\n",
            "Training:  Epoch No:  10 Iteration No:  267 \n",
            " Loss:  0.0016448431415483356\n",
            "Training:  Epoch No:  10 Iteration No:  268 \n",
            " Loss:  0.005902870558202267\n",
            "Training:  Epoch No:  10 Iteration No:  269 \n",
            " Loss:  0.001698496867902577\n",
            "Training:  Epoch No:  10 Iteration No:  270 \n",
            " Loss:  3.134533471893519e-05\n",
            "Training:  Epoch No:  10 Iteration No:  271 \n",
            " Loss:  0.0016202086117118597\n",
            "Training:  Epoch No:  10 Iteration No:  272 \n",
            " Loss:  0.0018935565603896976\n",
            "Training:  Epoch No:  10 Iteration No:  273 \n",
            " Loss:  0.0009646318503655493\n",
            "Training:  Epoch No:  10 Iteration No:  274 \n",
            " Loss:  0.0009897458367049694\n",
            "Training:  Epoch No:  10 Iteration No:  275 \n",
            " Loss:  0.000779524038080126\n",
            "Training:  Epoch No:  10 Iteration No:  276 \n",
            " Loss:  0.008567276410758495\n",
            "Training:  Epoch No:  10 Iteration No:  277 \n",
            " Loss:  0.01045896578580141\n",
            "Training:  Epoch No:  10 Iteration No:  278 \n",
            " Loss:  0.0002232127881143242\n",
            "Training:  Epoch No:  10 Iteration No:  279 \n",
            " Loss:  0.005428822711110115\n",
            "Training:  Epoch No:  10 Iteration No:  280 \n",
            " Loss:  0.0002433118352200836\n",
            "Training:  Epoch No:  10 Iteration No:  281 \n",
            " Loss:  0.009287026710808277\n",
            "Training:  Epoch No:  10 Iteration No:  282 \n",
            " Loss:  0.0003292066976428032\n",
            "Training:  Epoch No:  10 Iteration No:  283 \n",
            " Loss:  0.002465833444148302\n",
            "Training:  Epoch No:  10 Iteration No:  284 \n",
            " Loss:  0.007542387116700411\n",
            "Training:  Epoch No:  10 Iteration No:  285 \n",
            " Loss:  2.4163444322766736e-05\n",
            "Training:  Epoch No:  10 Iteration No:  286 \n",
            " Loss:  0.00041178992250934243\n",
            "Training:  Epoch No:  10 Iteration No:  287 \n",
            " Loss:  0.000880207575391978\n",
            "Training:  Epoch No:  10 Iteration No:  288 \n",
            " Loss:  0.002508923877030611\n",
            "Training:  Epoch No:  10 Iteration No:  289 \n",
            " Loss:  0.0004303561872802675\n",
            "Training:  Epoch No:  10 Iteration No:  290 \n",
            " Loss:  0.026582196354866028\n",
            "Training:  Epoch No:  10 Iteration No:  291 \n",
            " Loss:  0.0005461316322907805\n",
            "Training:  Epoch No:  10 Iteration No:  292 \n",
            " Loss:  0.0009673092281445861\n",
            "Training:  Epoch No:  10 Iteration No:  293 \n",
            " Loss:  0.00016465484804939479\n",
            "Training:  Epoch No:  10 Iteration No:  294 \n",
            " Loss:  0.003554749069735408\n",
            "Training:  Epoch No:  10 Iteration No:  295 \n",
            " Loss:  0.0010423889616504312\n",
            "Training:  Epoch No:  10 Iteration No:  296 \n",
            " Loss:  0.0002712038403842598\n",
            "Training:  Epoch No:  10 Iteration No:  297 \n",
            " Loss:  0.006375609897077084\n",
            "Training:  Epoch No:  10 Iteration No:  298 \n",
            " Loss:  0.0016295561799779534\n",
            "Training:  Epoch No:  10 Iteration No:  299 \n",
            " Loss:  0.0015079925069585443\n",
            "Training:  Epoch No:  10 Iteration No:  300 \n",
            " Loss:  0.04762295261025429\n",
            "Training:  Epoch No:  10 Iteration No:  301 \n",
            " Loss:  0.0008831890299916267\n",
            "Training:  Epoch No:  10 Iteration No:  302 \n",
            " Loss:  0.007328816689550877\n",
            "Training:  Epoch No:  10 Iteration No:  303 \n",
            " Loss:  0.00019605229317676276\n",
            "Training:  Epoch No:  10 Iteration No:  304 \n",
            " Loss:  0.0029658558778464794\n",
            "Training:  Epoch No:  10 Iteration No:  305 \n",
            " Loss:  0.0002319874765817076\n",
            "Training:  Epoch No:  10 Iteration No:  306 \n",
            " Loss:  0.00012606289237737656\n",
            "Training:  Epoch No:  10 Iteration No:  307 \n",
            " Loss:  0.00013060556375421584\n",
            "Training:  Epoch No:  10 Iteration No:  308 \n",
            " Loss:  0.0005812883609905839\n",
            "Training:  Epoch No:  10 Iteration No:  309 \n",
            " Loss:  0.00015880382852628827\n",
            "Training:  Epoch No:  10 Iteration No:  310 \n",
            " Loss:  0.002398567507043481\n",
            "Training:  Epoch No:  10 Iteration No:  311 \n",
            " Loss:  0.005948333535343409\n",
            "Training:  Epoch No:  10 Iteration No:  312 \n",
            " Loss:  0.0002493166539352387\n",
            "Training:  Epoch No:  10 Iteration No:  313 \n",
            " Loss:  0.00030422728741541505\n",
            "Training:  Epoch No:  10 Iteration No:  314 \n",
            " Loss:  0.05555640161037445\n",
            "Training:  Epoch No:  10 Iteration No:  315 \n",
            " Loss:  8.75828554853797e-05\n",
            "Training:  Epoch No:  10 Iteration No:  316 \n",
            " Loss:  0.0008413316681981087\n",
            "Training:  Epoch No:  10 Iteration No:  317 \n",
            " Loss:  0.0011588941561058164\n",
            "Training:  Epoch No:  10 Iteration No:  318 \n",
            " Loss:  0.00040963146602734923\n",
            "Training:  Epoch No:  10 Iteration No:  319 \n",
            " Loss:  0.008084642700850964\n",
            "Training:  Epoch No:  10 Iteration No:  320 \n",
            " Loss:  2.2765842004446313e-05\n",
            "Training:  Epoch No:  10 Iteration No:  321 \n",
            " Loss:  0.0031135512981563807\n",
            "Training:  Epoch No:  10 Iteration No:  322 \n",
            " Loss:  0.00046650340664200485\n",
            "Training:  Epoch No:  10 Iteration No:  323 \n",
            " Loss:  0.0007356969872489572\n",
            "Training:  Epoch No:  10 Iteration No:  324 \n",
            " Loss:  0.00015971623361110687\n",
            "Training:  Epoch No:  10 Iteration No:  325 \n",
            " Loss:  0.0002623995824251324\n",
            "Training:  Epoch No:  10 Iteration No:  326 \n",
            " Loss:  0.009456884115934372\n",
            "Training:  Epoch No:  10 Iteration No:  327 \n",
            " Loss:  0.0005890832399018109\n",
            "Training:  Epoch No:  10 Iteration No:  328 \n",
            " Loss:  0.012823463417589664\n",
            "Training:  Epoch No:  10 Iteration No:  329 \n",
            " Loss:  0.002332991221919656\n",
            "Training:  Epoch No:  10 Iteration No:  330 \n",
            " Loss:  0.0001633890060475096\n",
            "Training:  Epoch No:  10 Iteration No:  331 \n",
            " Loss:  0.04193713888525963\n",
            "Training:  Epoch No:  10 Iteration No:  332 \n",
            " Loss:  0.001339924638159573\n",
            "Training:  Epoch No:  10 Iteration No:  333 \n",
            " Loss:  0.00019565118418540806\n",
            "Training:  Epoch No:  10 Iteration No:  334 \n",
            " Loss:  0.006602328270673752\n",
            "Training:  Epoch No:  10 Iteration No:  335 \n",
            " Loss:  0.00294055906124413\n",
            "Training:  Epoch No:  10 Iteration No:  336 \n",
            " Loss:  0.0016881633782759309\n",
            "Training:  Epoch No:  10 Iteration No:  337 \n",
            " Loss:  0.0002991121436934918\n",
            "Training:  Epoch No:  10 Iteration No:  338 \n",
            " Loss:  0.0012729127192869782\n",
            "Training:  Epoch No:  10 Iteration No:  339 \n",
            " Loss:  0.0009948607767000794\n",
            "Training:  Epoch No:  10 Iteration No:  340 \n",
            " Loss:  0.002013479359447956\n",
            "Training:  Epoch No:  10 Iteration No:  341 \n",
            " Loss:  0.006109524983912706\n",
            "Training:  Epoch No:  10 Iteration No:  342 \n",
            " Loss:  0.005993084516376257\n",
            "Training:  Epoch No:  10 Iteration No:  343 \n",
            " Loss:  0.00012880461872555315\n",
            "Training:  Epoch No:  10 Iteration No:  344 \n",
            " Loss:  0.0026194825768470764\n",
            "Training:  Epoch No:  10 Iteration No:  345 \n",
            " Loss:  0.004570482298731804\n",
            "Training:  Epoch No:  10 Iteration No:  346 \n",
            " Loss:  0.0023169897031039\n",
            "Training:  Epoch No:  10 Iteration No:  347 \n",
            " Loss:  0.001562492223456502\n",
            "Training:  Epoch No:  10 Iteration No:  348 \n",
            " Loss:  0.000960995617788285\n",
            "Training:  Epoch No:  10 Iteration No:  349 \n",
            " Loss:  0.004059936851263046\n",
            "Training:  Epoch No:  10 Iteration No:  350 \n",
            " Loss:  0.032567400485277176\n",
            "Training:  Epoch No:  10 Iteration No:  351 \n",
            " Loss:  0.0024217558093369007\n",
            "Training:  Epoch No:  10 Iteration No:  352 \n",
            " Loss:  0.002775137545540929\n",
            "Training:  Epoch No:  10 Iteration No:  353 \n",
            " Loss:  0.00035651493817567825\n",
            "Training:  Epoch No:  10 Iteration No:  354 \n",
            " Loss:  0.003315639216452837\n",
            "Training:  Epoch No:  10 Iteration No:  355 \n",
            " Loss:  0.0009000402060337365\n",
            "Training:  Epoch No:  10 Iteration No:  356 \n",
            " Loss:  0.0006944355554878712\n",
            "Training:  Epoch No:  10 Iteration No:  357 \n",
            " Loss:  0.00016745015454944223\n",
            "Training:  Epoch No:  10 Iteration No:  358 \n",
            " Loss:  0.022262662649154663\n",
            "Training:  Epoch No:  10 Iteration No:  359 \n",
            " Loss:  0.002436794340610504\n",
            "Training:  Epoch No:  10 Iteration No:  360 \n",
            " Loss:  0.0008834890904836357\n",
            "Training:  Epoch No:  10 Iteration No:  361 \n",
            " Loss:  0.0005346516263671219\n",
            "Training:  Epoch No:  10 Iteration No:  362 \n",
            " Loss:  0.006516430526971817\n",
            "Training:  Epoch No:  10 Iteration No:  363 \n",
            " Loss:  0.002858280437067151\n",
            "Training:  Epoch No:  10 Iteration No:  364 \n",
            " Loss:  0.0010429797694087029\n",
            "Training:  Epoch No:  10 Iteration No:  365 \n",
            " Loss:  0.0024837686214596033\n",
            "Training:  Epoch No:  10 Iteration No:  366 \n",
            " Loss:  0.0014879623195156455\n",
            "Training:  Epoch No:  10 Iteration No:  367 \n",
            " Loss:  9.920698357746005e-05\n",
            "Training:  Epoch No:  10 Iteration No:  368 \n",
            " Loss:  0.011591672897338867\n",
            "Training:  Epoch No:  10 Iteration No:  369 \n",
            " Loss:  0.03274141252040863\n",
            "Training:  Epoch No:  10 Iteration No:  370 \n",
            " Loss:  0.026095988228917122\n",
            "Training:  Epoch No:  10 Iteration No:  371 \n",
            " Loss:  0.0024677803739905357\n",
            "Training:  Epoch No:  10 Iteration No:  372 \n",
            " Loss:  0.0008426601998507977\n",
            "Training:  Epoch No:  10 Iteration No:  373 \n",
            " Loss:  0.0005511637427844107\n",
            "Training:  Epoch No:  10 Iteration No:  374 \n",
            " Loss:  0.002172122709453106\n",
            "Training:  Epoch No:  10 Iteration No:  375 \n",
            " Loss:  0.00347270630300045\n",
            "Training:  Epoch No:  10 Iteration No:  376 \n",
            " Loss:  0.0007892054854892194\n",
            "Training:  Epoch No:  10 Iteration No:  377 \n",
            " Loss:  0.0014189170906320214\n",
            "Training:  Epoch No:  10 Iteration No:  378 \n",
            " Loss:  0.000669828848913312\n",
            "Training:  Epoch No:  10 Iteration No:  379 \n",
            " Loss:  0.000841891102027148\n",
            "Training:  Epoch No:  10 Iteration No:  380 \n",
            " Loss:  0.007750006392598152\n",
            "Training:  Epoch No:  10 Iteration No:  381 \n",
            " Loss:  0.008102440275251865\n",
            "Training:  Epoch No:  10 Iteration No:  382 \n",
            " Loss:  0.007539225742220879\n",
            "Training:  Epoch No:  10 Iteration No:  383 \n",
            " Loss:  6.0538692196132615e-05\n",
            "Training:  Epoch No:  10 Iteration No:  384 \n",
            " Loss:  0.000327652640407905\n",
            "Training:  Epoch No:  10 Iteration No:  385 \n",
            " Loss:  0.03907656669616699\n",
            "Training:  Epoch No:  10 Iteration No:  386 \n",
            " Loss:  0.0009504662011750042\n",
            "Training:  Epoch No:  10 Iteration No:  387 \n",
            " Loss:  0.0002649425296112895\n",
            "Training:  Epoch No:  10 Iteration No:  388 \n",
            " Loss:  0.0001425720693077892\n",
            "Training:  Epoch No:  10 Iteration No:  389 \n",
            " Loss:  0.0019039246253669262\n",
            "Training:  Epoch No:  10 Iteration No:  390 \n",
            " Loss:  0.0003635098983068019\n",
            "Training:  Epoch No:  10 Iteration No:  391 \n",
            " Loss:  0.00041975086787715554\n",
            "Training:  Epoch No:  10 Iteration No:  392 \n",
            " Loss:  0.008537443354725838\n",
            "Training:  Epoch No:  10 Iteration No:  393 \n",
            " Loss:  0.00046195529284887016\n",
            "Training:  Epoch No:  10 Iteration No:  394 \n",
            " Loss:  0.0003249259025324136\n",
            "Training:  Epoch No:  10 Iteration No:  395 \n",
            " Loss:  0.00047071295557543635\n",
            "Training:  Epoch No:  10 Iteration No:  396 \n",
            " Loss:  0.002787219127640128\n",
            "Training:  Epoch No:  10 Iteration No:  397 \n",
            " Loss:  0.00035357882734388113\n",
            "Training:  Epoch No:  10 Iteration No:  398 \n",
            " Loss:  0.009216554462909698\n",
            "Training:  Epoch No:  10 Iteration No:  399 \n",
            " Loss:  0.0012902953894808888\n",
            "Training:  Epoch No:  10 Iteration No:  400 \n",
            " Loss:  0.0002979836717713624\n",
            "Training:  Epoch No:  10 Iteration No:  401 \n",
            " Loss:  0.0019166971324011683\n",
            "Training:  Epoch No:  10 Iteration No:  402 \n",
            " Loss:  0.00023425965628121048\n",
            "Training:  Epoch No:  10 Iteration No:  403 \n",
            " Loss:  0.0018019331619143486\n",
            "Training:  Epoch No:  10 Iteration No:  404 \n",
            " Loss:  0.005368105135858059\n",
            "Training:  Epoch No:  10 Iteration No:  405 \n",
            " Loss:  0.004012215882539749\n",
            "Training:  Epoch No:  10 Iteration No:  406 \n",
            " Loss:  0.020038556307554245\n",
            "Training:  Epoch No:  10 Iteration No:  407 \n",
            " Loss:  8.287021773867309e-05\n",
            "Training:  Epoch No:  10 Iteration No:  408 \n",
            " Loss:  0.005123435985296965\n",
            "Training:  Epoch No:  10 Iteration No:  409 \n",
            " Loss:  0.00028942982316948473\n",
            "Training:  Epoch No:  10 Iteration No:  410 \n",
            " Loss:  0.002824879251420498\n",
            "Training:  Epoch No:  10 Iteration No:  411 \n",
            " Loss:  0.001910147606395185\n",
            "Training:  Epoch No:  10 Iteration No:  412 \n",
            " Loss:  0.008506517857313156\n",
            "Training:  Epoch No:  10 Iteration No:  413 \n",
            " Loss:  0.003753434866666794\n",
            "Training:  Epoch No:  10 Iteration No:  414 \n",
            " Loss:  0.007954861037433147\n",
            "Training:  Epoch No:  10 Iteration No:  415 \n",
            " Loss:  0.00016356790729332715\n",
            "Training:  Epoch No:  10 Iteration No:  416 \n",
            " Loss:  0.009132705628871918\n",
            "Training:  Epoch No:  10 Iteration No:  417 \n",
            " Loss:  0.00021859596017748117\n",
            "Training:  Epoch No:  10 Iteration No:  418 \n",
            " Loss:  0.001129442942328751\n",
            "Training:  Epoch No:  10 Iteration No:  419 \n",
            " Loss:  0.00402070814743638\n",
            "Training:  Epoch No:  10 Iteration No:  420 \n",
            " Loss:  0.0008033805643208325\n",
            "Training:  Epoch No:  10 Iteration No:  421 \n",
            " Loss:  0.00011431645543780178\n",
            "Training:  Epoch No:  10 Iteration No:  422 \n",
            " Loss:  0.0003293459885753691\n",
            "Training:  Epoch No:  10 Iteration No:  423 \n",
            " Loss:  0.0096116429194808\n",
            "Training:  Epoch No:  10 Iteration No:  424 \n",
            " Loss:  0.00047935801558196545\n",
            "Training:  Epoch No:  10 Iteration No:  425 \n",
            " Loss:  0.0011430515442043543\n",
            "Training:  Epoch No:  10 Iteration No:  426 \n",
            " Loss:  0.0037225300911813974\n",
            "Training:  Epoch No:  10 Iteration No:  427 \n",
            " Loss:  0.005676744505763054\n",
            "Training:  Epoch No:  10 Iteration No:  428 \n",
            " Loss:  0.00010467662650626153\n",
            "Training:  Epoch No:  10 Iteration No:  429 \n",
            " Loss:  0.0005799892242066562\n",
            "Training:  Epoch No:  10 Iteration No:  430 \n",
            " Loss:  0.013697701506316662\n",
            "Training:  Epoch No:  10 Iteration No:  431 \n",
            " Loss:  0.0006499881274066865\n",
            "Training:  Epoch No:  10 Iteration No:  432 \n",
            " Loss:  0.005932377185672522\n",
            "Training:  Epoch No:  10 Iteration No:  433 \n",
            " Loss:  0.0013835093704983592\n",
            "Training:  Epoch No:  10 Iteration No:  434 \n",
            " Loss:  0.0009105986682698131\n",
            "Training:  Epoch No:  10 Iteration No:  435 \n",
            " Loss:  0.0044065797701478004\n",
            "Training:  Epoch No:  10 Iteration No:  436 \n",
            " Loss:  6.363914144458249e-05\n",
            "Training:  Epoch No:  10 Iteration No:  437 \n",
            " Loss:  0.0014435399789363146\n",
            "Training:  Epoch No:  10 Iteration No:  438 \n",
            " Loss:  5.5502176110167056e-05\n",
            "Training:  Epoch No:  10 Iteration No:  439 \n",
            " Loss:  0.00015215028543025255\n",
            "Training:  Epoch No:  10 Iteration No:  440 \n",
            " Loss:  0.002245403826236725\n",
            "Training:  Epoch No:  10 Iteration No:  441 \n",
            " Loss:  0.007248079404234886\n",
            "Training:  Epoch No:  10 Iteration No:  442 \n",
            " Loss:  0.0003056937421206385\n",
            "Training:  Epoch No:  10 Iteration No:  443 \n",
            " Loss:  0.0035076939966529608\n",
            "Training:  Epoch No:  10 Iteration No:  444 \n",
            " Loss:  0.00026559983962215483\n",
            "Training:  Epoch No:  10 Iteration No:  445 \n",
            " Loss:  0.0036094128154218197\n",
            "Training:  Epoch No:  10 Iteration No:  446 \n",
            " Loss:  0.002094327239319682\n",
            "Training:  Epoch No:  10 Iteration No:  447 \n",
            " Loss:  0.04366203397512436\n",
            "Training:  Epoch No:  10 Iteration No:  448 \n",
            " Loss:  0.004846865311264992\n",
            "Training:  Epoch No:  10 Iteration No:  449 \n",
            " Loss:  0.012946130707859993\n",
            "Training:  Epoch No:  10 Iteration No:  450 \n",
            " Loss:  0.005724022164940834\n",
            "Training:  Epoch No:  10 Iteration No:  451 \n",
            " Loss:  0.014894255436956882\n",
            "Training:  Epoch No:  10 Iteration No:  452 \n",
            " Loss:  0.041739653795957565\n",
            "Training:  Epoch No:  10 Iteration No:  453 \n",
            " Loss:  0.001446933252736926\n",
            "Training:  Epoch No:  10 Iteration No:  454 \n",
            " Loss:  0.011143866926431656\n",
            "Training:  Epoch No:  10 Iteration No:  455 \n",
            " Loss:  0.02809934876859188\n",
            "Training:  Epoch No:  10 Iteration No:  456 \n",
            " Loss:  0.003609617706388235\n",
            "Training:  Epoch No:  10 Iteration No:  457 \n",
            " Loss:  0.0007457644096575677\n",
            "Training:  Epoch No:  10 Iteration No:  458 \n",
            " Loss:  0.00010349373769713566\n",
            "Training:  Epoch No:  10 Iteration No:  459 \n",
            " Loss:  0.004522709641605616\n",
            "Training:  Epoch No:  10 Iteration No:  460 \n",
            " Loss:  0.002164259320124984\n",
            "Training:  Epoch No:  10 Iteration No:  461 \n",
            " Loss:  0.004866291303187609\n",
            "Training:  Epoch No:  10 Iteration No:  462 \n",
            " Loss:  0.02677333541214466\n",
            "Training:  Epoch No:  10 Iteration No:  463 \n",
            " Loss:  0.0024576918222010136\n",
            "Training:  Epoch No:  10 Iteration No:  464 \n",
            " Loss:  0.0001751706877257675\n",
            "Training:  Epoch No:  10 Iteration No:  465 \n",
            " Loss:  0.001549258828163147\n",
            "Training:  Epoch No:  10 Iteration No:  466 \n",
            " Loss:  3.429253047215752e-05\n",
            "Training:  Epoch No:  10 Iteration No:  467 \n",
            " Loss:  0.00042792793828994036\n",
            "Training:  Epoch No:  10 Iteration No:  468 \n",
            " Loss:  0.005298806820064783\n",
            "Training:  Epoch No:  10 Iteration No:  469 \n",
            " Loss:  0.016379166394472122\n",
            "Training:  Epoch No:  10 Iteration No:  470 \n",
            " Loss:  0.001936326501891017\n",
            "Training:  Epoch No:  10 Iteration No:  471 \n",
            " Loss:  0.001514557283371687\n",
            "Training:  Epoch No:  10 Iteration No:  472 \n",
            " Loss:  0.07763306796550751\n",
            "Training:  Epoch No:  10 Iteration No:  473 \n",
            " Loss:  0.009784827008843422\n",
            "Training:  Epoch No:  10 Iteration No:  474 \n",
            " Loss:  0.0010676839156076312\n",
            "Training:  Epoch No:  10 Iteration No:  475 \n",
            " Loss:  0.004353018943220377\n",
            "Training:  Epoch No:  10 Iteration No:  476 \n",
            " Loss:  0.0009903328027576208\n",
            "Training:  Epoch No:  10 Iteration No:  477 \n",
            " Loss:  0.012439676560461521\n",
            "Training:  Epoch No:  10 Iteration No:  478 \n",
            " Loss:  0.0031304750591516495\n",
            "Training:  Epoch No:  10 Iteration No:  479 \n",
            " Loss:  0.0068125296384096146\n",
            "Training:  Epoch No:  10 Iteration No:  480 \n",
            " Loss:  0.005785773508250713\n",
            "Training:  Epoch No:  10 Iteration No:  481 \n",
            " Loss:  0.0048238178715109825\n",
            "Training:  Epoch No:  10 Iteration No:  482 \n",
            " Loss:  0.003053767839446664\n",
            "Training:  Epoch No:  10 Iteration No:  483 \n",
            " Loss:  0.0064034368842840195\n",
            "Training:  Epoch No:  10 Iteration No:  484 \n",
            " Loss:  0.0015520532615482807\n",
            "Training:  Epoch No:  10 Iteration No:  485 \n",
            " Loss:  0.0037283592391759157\n",
            "Training:  Epoch No:  10 Iteration No:  486 \n",
            " Loss:  0.0003783264255616814\n",
            "Training:  Epoch No:  10 Iteration No:  487 \n",
            " Loss:  0.0007988719153217971\n",
            "Training:  Epoch No:  10 Iteration No:  488 \n",
            " Loss:  0.0005894768983125687\n",
            "Training:  Epoch No:  10 Iteration No:  489 \n",
            " Loss:  0.002203191164880991\n",
            "Training:  Epoch No:  10 Iteration No:  490 \n",
            " Loss:  0.0015862741274759173\n",
            "Training:  Epoch No:  10 Iteration No:  491 \n",
            " Loss:  0.00045467709423974156\n",
            "Training:  Epoch No:  10 Iteration No:  492 \n",
            " Loss:  0.02267840877175331\n",
            "Training:  Epoch No:  10 Iteration No:  493 \n",
            " Loss:  0.033276207745075226\n",
            "Training:  Epoch No:  10 Iteration No:  494 \n",
            " Loss:  0.0003272312169428915\n",
            "Training:  Epoch No:  10 Iteration No:  495 \n",
            " Loss:  0.0014581240247935057\n",
            "Training:  Epoch No:  10 Iteration No:  496 \n",
            " Loss:  0.0005675904685631394\n",
            "Training:  Epoch No:  10 Iteration No:  497 \n",
            " Loss:  0.004716911353170872\n",
            "Training:  Epoch No:  10 Iteration No:  498 \n",
            " Loss:  0.001817889977246523\n",
            "Training:  Epoch No:  10 Iteration No:  499 \n",
            " Loss:  0.006420536898076534\n",
            "Training:  Epoch No:  10 Iteration No:  500 \n",
            " Loss:  0.002281514462083578\n",
            "Training:  Epoch No:  10 Iteration No:  501 \n",
            " Loss:  0.0005239478778094053\n",
            "Training:  Epoch No:  10 Iteration No:  502 \n",
            " Loss:  0.0012721215607598424\n",
            "Training:  Epoch No:  10 Iteration No:  503 \n",
            " Loss:  0.0025862210895866156\n",
            "Training:  Epoch No:  10 Iteration No:  504 \n",
            " Loss:  0.0065828245133161545\n",
            "Training:  Epoch No:  10 Iteration No:  505 \n",
            " Loss:  0.001774482661858201\n",
            "Training:  Epoch No:  10 Iteration No:  506 \n",
            " Loss:  8.258238813141361e-05\n",
            "Training:  Epoch No:  10 Iteration No:  507 \n",
            " Loss:  0.002605143003165722\n",
            "Training:  Epoch No:  10 Iteration No:  508 \n",
            " Loss:  0.020867861807346344\n",
            "Training:  Epoch No:  10 Iteration No:  509 \n",
            " Loss:  0.004864020738750696\n",
            "Training:  Epoch No:  10 Iteration No:  510 \n",
            " Loss:  0.0019835347775369883\n",
            "Training:  Epoch No:  10 Iteration No:  511 \n",
            " Loss:  0.0001755822595441714\n",
            "Training:  Epoch No:  10 Iteration No:  512 \n",
            " Loss:  0.01126524992287159\n",
            "Training:  Epoch No:  10 Iteration No:  513 \n",
            " Loss:  0.000343991065165028\n",
            "Training:  Epoch No:  10 Iteration No:  514 \n",
            " Loss:  0.0005173120298422873\n",
            "Training:  Epoch No:  10 Iteration No:  515 \n",
            " Loss:  0.03563844785094261\n",
            "Training:  Epoch No:  10 Iteration No:  516 \n",
            " Loss:  0.0011384489480406046\n",
            "Training:  Epoch No:  10 Iteration No:  517 \n",
            " Loss:  0.0006860133726149797\n",
            "Training:  Epoch No:  10 Iteration No:  518 \n",
            " Loss:  0.00041274039540439844\n",
            "Training:  Epoch No:  10 Iteration No:  519 \n",
            " Loss:  8.253255509771407e-05\n",
            "Training:  Epoch No:  10 Iteration No:  520 \n",
            " Loss:  0.0071984813548624516\n",
            "Training:  Epoch No:  10 Iteration No:  521 \n",
            " Loss:  0.015935426577925682\n",
            "Training:  Epoch No:  10 Iteration No:  522 \n",
            " Loss:  0.0003460518491920084\n",
            "Training:  Epoch No:  10 Iteration No:  523 \n",
            " Loss:  0.0003300917742308229\n",
            "Training:  Epoch No:  10 Iteration No:  524 \n",
            " Loss:  0.005011125933378935\n",
            "Training:  Epoch No:  10 Iteration No:  525 \n",
            " Loss:  0.0018417875980958343\n",
            "Training:  Epoch No:  10 Iteration No:  526 \n",
            " Loss:  0.0008531106286682189\n",
            "Training:  Epoch No:  10 Iteration No:  527 \n",
            " Loss:  0.013869642280042171\n",
            "Training:  Epoch No:  10 Iteration No:  528 \n",
            " Loss:  0.000604259897954762\n",
            "Training:  Epoch No:  10 Iteration No:  529 \n",
            " Loss:  0.0005795651231892407\n",
            "Training:  Epoch No:  10 Iteration No:  530 \n",
            " Loss:  0.00191695976536721\n",
            "Training:  Epoch No:  10 Iteration No:  531 \n",
            " Loss:  0.0008375090546905994\n",
            "Training:  Epoch No:  10 Iteration No:  532 \n",
            " Loss:  0.0007830678950995207\n",
            "Training:  Epoch No:  10 Iteration No:  533 \n",
            " Loss:  0.045668475329875946\n",
            "Training:  Epoch No:  10 Iteration No:  534 \n",
            " Loss:  0.005572996102273464\n",
            "Training:  Epoch No:  10 Iteration No:  535 \n",
            " Loss:  0.013472367078065872\n",
            "Training:  Epoch No:  10 Iteration No:  536 \n",
            " Loss:  0.0010652740020304918\n",
            "Training:  Epoch No:  10 Iteration No:  537 \n",
            " Loss:  0.0007888139225542545\n",
            "Training:  Epoch No:  10 Iteration No:  538 \n",
            " Loss:  0.00040418404387310147\n",
            "Training:  Epoch No:  10 Iteration No:  539 \n",
            " Loss:  0.008953569456934929\n",
            "Training:  Epoch No:  10 Iteration No:  540 \n",
            " Loss:  0.001400874461978674\n",
            "Training:  Epoch No:  10 Iteration No:  541 \n",
            " Loss:  3.1888808734947816e-05\n",
            "Training:  Epoch No:  10 Iteration No:  542 \n",
            " Loss:  0.002817234955728054\n",
            "Training:  Epoch No:  10 Iteration No:  543 \n",
            " Loss:  0.00020223495084792376\n",
            "Training:  Epoch No:  10 Iteration No:  544 \n",
            " Loss:  0.003738831263035536\n",
            "Training:  Epoch No:  10 Iteration No:  545 \n",
            " Loss:  0.011484227143228054\n",
            "Training:  Epoch No:  10 Iteration No:  546 \n",
            " Loss:  0.0006337082595564425\n",
            "Training:  Epoch No:  10 Iteration No:  547 \n",
            " Loss:  0.0025321932043880224\n",
            "Training:  Epoch No:  10 Iteration No:  548 \n",
            " Loss:  0.00029534497298300266\n",
            "Training:  Epoch No:  10 Iteration No:  549 \n",
            " Loss:  0.001369372708722949\n",
            "Training:  Epoch No:  10 Iteration No:  550 \n",
            " Loss:  0.0019761587027460337\n",
            "Training:  Epoch No:  10 Iteration No:  551 \n",
            " Loss:  6.636561738559976e-05\n",
            "Training:  Epoch No:  10 Iteration No:  552 \n",
            " Loss:  0.001375719322822988\n",
            "Training:  Epoch No:  10 Iteration No:  553 \n",
            " Loss:  0.006394829601049423\n",
            "Training:  Epoch No:  10 Iteration No:  554 \n",
            " Loss:  0.00024447133182547987\n",
            "Training:  Epoch No:  10 Iteration No:  555 \n",
            " Loss:  0.002717302180826664\n",
            "Training:  Epoch No:  10 Iteration No:  556 \n",
            " Loss:  0.0004683321458287537\n",
            "Training:  Epoch No:  10 Iteration No:  557 \n",
            " Loss:  0.0012509757652878761\n",
            "Training:  Epoch No:  10 Iteration No:  558 \n",
            " Loss:  0.007160844746977091\n",
            "Training:  Epoch No:  10 Iteration No:  559 \n",
            " Loss:  3.471919262665324e-05\n",
            "Training:  Epoch No:  10 Iteration No:  560 \n",
            " Loss:  0.0009629563428461552\n",
            "Training:  Epoch No:  10 Iteration No:  561 \n",
            " Loss:  0.00028484396170824766\n",
            "Training:  Epoch No:  10 Iteration No:  562 \n",
            " Loss:  0.00015048382920213044\n",
            "Training:  Epoch No:  10 Iteration No:  563 \n",
            " Loss:  0.01605585403740406\n",
            "Training:  Epoch No:  10 Iteration No:  564 \n",
            " Loss:  0.0013917102478444576\n",
            "Training:  Epoch No:  10 Iteration No:  565 \n",
            " Loss:  0.00013048859545961022\n",
            "Training:  Epoch No:  10 Iteration No:  566 \n",
            " Loss:  0.0004680634301621467\n",
            "Training:  Epoch No:  10 Iteration No:  567 \n",
            " Loss:  0.0012111166724935174\n",
            "Training:  Epoch No:  10 Iteration No:  568 \n",
            " Loss:  0.0005529138725250959\n",
            "Training:  Epoch No:  10 Iteration No:  569 \n",
            " Loss:  0.007204716559499502\n",
            "Training:  Epoch No:  10 Iteration No:  570 \n",
            " Loss:  0.0004322167078498751\n",
            "Training:  Epoch No:  10 Iteration No:  571 \n",
            " Loss:  0.00048536897520534694\n",
            "Training:  Epoch No:  10 Iteration No:  572 \n",
            " Loss:  0.11026761680841446\n",
            "Training:  Epoch No:  10 Iteration No:  573 \n",
            " Loss:  0.002463678829371929\n",
            "Training:  Epoch No:  10 Iteration No:  574 \n",
            " Loss:  0.005866344552487135\n",
            "Training:  Epoch No:  10 Iteration No:  575 \n",
            " Loss:  0.00373007426969707\n",
            "Training:  Epoch No:  10 Iteration No:  576 \n",
            " Loss:  0.0008466409053653479\n",
            "Training:  Epoch No:  10 Iteration No:  577 \n",
            " Loss:  0.0016076061874628067\n",
            "Training:  Epoch No:  10 Iteration No:  578 \n",
            " Loss:  0.0012095237616449594\n",
            "Training:  Epoch No:  10 Iteration No:  579 \n",
            " Loss:  0.0014741691993549466\n",
            "Training:  Epoch No:  10 Iteration No:  580 \n",
            " Loss:  0.007794027216732502\n",
            "Training:  Epoch No:  10 Iteration No:  581 \n",
            " Loss:  0.0026620314456522465\n",
            "Training:  Epoch No:  10 Iteration No:  582 \n",
            " Loss:  0.002021242631599307\n",
            "Training:  Epoch No:  10 Iteration No:  583 \n",
            " Loss:  0.0010660706320777535\n",
            "Training:  Epoch No:  10 Iteration No:  584 \n",
            " Loss:  0.001929152524098754\n",
            "Training:  Epoch No:  10 Iteration No:  585 \n",
            " Loss:  3.540752732078545e-05\n",
            "Training:  Epoch No:  10 Iteration No:  586 \n",
            " Loss:  0.00013514696911443025\n",
            "Training:  Epoch No:  10 Iteration No:  587 \n",
            " Loss:  0.017895443364977837\n",
            "Training:  Epoch No:  10 Iteration No:  588 \n",
            " Loss:  0.00041849477565847337\n",
            "Training:  Epoch No:  10 Iteration No:  589 \n",
            " Loss:  0.0033754450269043446\n",
            "Training:  Epoch No:  10 Iteration No:  590 \n",
            " Loss:  0.0007244421285577118\n",
            "Training:  Epoch No:  10 Iteration No:  591 \n",
            " Loss:  0.000872683827765286\n",
            "Training:  Epoch No:  10 Iteration No:  592 \n",
            " Loss:  0.01762697473168373\n",
            "Training:  Epoch No:  10 Iteration No:  593 \n",
            " Loss:  0.0015437715919688344\n",
            "Training:  Epoch No:  10 Iteration No:  594 \n",
            " Loss:  0.02875412628054619\n",
            "Training:  Epoch No:  10 Iteration No:  595 \n",
            " Loss:  0.003770610084757209\n",
            "Training:  Epoch No:  10 Iteration No:  596 \n",
            " Loss:  0.008818050846457481\n",
            "Training:  Epoch No:  10 Iteration No:  597 \n",
            " Loss:  0.00457499735057354\n",
            "Training:  Epoch No:  10 Iteration No:  598 \n",
            " Loss:  0.01879151351749897\n",
            "Training:  Epoch No:  10 Iteration No:  599 \n",
            " Loss:  0.0006337263621389866\n",
            "Training:  Epoch No:  10 Iteration No:  600 \n",
            " Loss:  0.007825788110494614\n",
            "Training:  Epoch No:  10 Iteration No:  601 \n",
            " Loss:  0.000873376673553139\n",
            "Training:  Epoch No:  10 Iteration No:  602 \n",
            " Loss:  0.0019446563674136996\n",
            "Training:  Epoch No:  10 Iteration No:  603 \n",
            " Loss:  0.0004618202510755509\n",
            "Training:  Epoch No:  10 Iteration No:  604 \n",
            " Loss:  0.0004819142632186413\n",
            "Training:  Epoch No:  10 Iteration No:  605 \n",
            " Loss:  0.00223229150287807\n",
            "Training:  Epoch No:  10 Iteration No:  606 \n",
            " Loss:  0.000376556214177981\n",
            "Training:  Epoch No:  10 Iteration No:  607 \n",
            " Loss:  0.0028371235821396112\n",
            "Training:  Epoch No:  10 Iteration No:  608 \n",
            " Loss:  0.0006717604701407254\n",
            "Training:  Epoch No:  10 Iteration No:  609 \n",
            " Loss:  0.00045735822641290724\n",
            "Training:  Epoch No:  10 Iteration No:  610 \n",
            " Loss:  0.0012920607114210725\n",
            "Training:  Epoch No:  10 Iteration No:  611 \n",
            " Loss:  3.213311356375925e-05\n",
            "Training:  Epoch No:  10 Iteration No:  612 \n",
            " Loss:  0.002011242089793086\n",
            "Training:  Epoch No:  10 Iteration No:  613 \n",
            " Loss:  0.0007537798373959959\n",
            "Training:  Epoch No:  10 Iteration No:  614 \n",
            " Loss:  0.0032384060323238373\n",
            "Training:  Epoch No:  10 Iteration No:  615 \n",
            " Loss:  0.003831499256193638\n",
            "Training:  Epoch No:  10 Iteration No:  616 \n",
            " Loss:  0.00048071061610244215\n",
            "Training:  Epoch No:  10 Iteration No:  617 \n",
            " Loss:  0.0016244378639385104\n",
            "Training:  Epoch No:  10 Iteration No:  618 \n",
            " Loss:  0.0024123270995914936\n",
            "Training:  Epoch No:  10 Iteration No:  619 \n",
            " Loss:  0.04048651084303856\n",
            "Training:  Epoch No:  10 Iteration No:  620 \n",
            " Loss:  0.0019257012754678726\n",
            "Training:  Epoch No:  10 Iteration No:  621 \n",
            " Loss:  0.009568498469889164\n",
            "Training:  Epoch No:  10 Iteration No:  622 \n",
            " Loss:  7.231716881506145e-05\n",
            "Training:  Epoch No:  10 Iteration No:  623 \n",
            " Loss:  0.007759268395602703\n",
            "Training:  Epoch No:  10 Iteration No:  624 \n",
            " Loss:  0.08101943880319595\n",
            "Training:  Epoch No:  10 Iteration No:  625 \n",
            " Loss:  0.0029997718520462513\n",
            "Training:  Epoch No:  10 Iteration No:  626 \n",
            " Loss:  0.0008683043415658176\n",
            "Training:  Epoch No:  10 Iteration No:  627 \n",
            " Loss:  0.0012078166473656893\n",
            "Training:  Epoch No:  10 Iteration No:  628 \n",
            " Loss:  0.0022890146356076\n",
            "Training:  Epoch No:  10 Iteration No:  629 \n",
            " Loss:  0.003309614025056362\n",
            "Training:  Epoch No:  10 Iteration No:  630 \n",
            " Loss:  0.0023715635761618614\n",
            "Training:  Epoch No:  10 Iteration No:  631 \n",
            " Loss:  0.0005999633576720953\n",
            "Training:  Epoch No:  10 Iteration No:  632 \n",
            " Loss:  0.006648418493568897\n",
            "Training:  Epoch No:  10 Iteration No:  633 \n",
            " Loss:  0.011499104090034962\n",
            "Training:  Epoch No:  10 Iteration No:  634 \n",
            " Loss:  0.0007885059458203614\n",
            "Training:  Epoch No:  10 Iteration No:  635 \n",
            " Loss:  0.010251992382109165\n",
            "Training:  Epoch No:  10 Iteration No:  636 \n",
            " Loss:  7.765908958390355e-05\n",
            "Training:  Epoch No:  10 Iteration No:  637 \n",
            " Loss:  0.005063238553702831\n",
            "Training:  Epoch No:  10 Iteration No:  638 \n",
            " Loss:  0.0020530829206109047\n",
            "Training:  Epoch No:  10 Iteration No:  639 \n",
            " Loss:  0.0011219047009944916\n",
            "Training:  Epoch No:  10 Iteration No:  640 \n",
            " Loss:  0.004035002086311579\n",
            "Training:  Epoch No:  10 Iteration No:  641 \n",
            " Loss:  0.007316372822970152\n",
            "Training:  Epoch No:  10 Iteration No:  642 \n",
            " Loss:  0.004009196534752846\n",
            "Training:  Epoch No:  10 Iteration No:  643 \n",
            " Loss:  0.0006591051933355629\n",
            "Training:  Epoch No:  10 Iteration No:  644 \n",
            " Loss:  0.0019403566839173436\n",
            "Training:  Epoch No:  10 Iteration No:  645 \n",
            " Loss:  0.0014021797105669975\n",
            "Training:  Epoch No:  10 Iteration No:  646 \n",
            " Loss:  0.008944007568061352\n",
            "Training:  Epoch No:  10 Iteration No:  647 \n",
            " Loss:  0.0004018212202936411\n",
            "Training:  Epoch No:  10 Iteration No:  648 \n",
            " Loss:  0.0008341293432749808\n",
            "Training:  Epoch No:  10 Iteration No:  649 \n",
            " Loss:  0.0031947409734129906\n",
            "Training:  Epoch No:  10 Iteration No:  650 \n",
            " Loss:  0.0004525884287431836\n",
            "Training:  Epoch No:  10 Iteration No:  651 \n",
            " Loss:  0.0011630724184215069\n",
            "Training:  Epoch No:  10 Iteration No:  652 \n",
            " Loss:  0.001315816305577755\n",
            "Training:  Epoch No:  10 Iteration No:  653 \n",
            " Loss:  0.0007750795921310782\n",
            "Training:  Epoch No:  10 Iteration No:  654 \n",
            " Loss:  0.02207300439476967\n",
            "Training:  Epoch No:  10 Iteration No:  655 \n",
            " Loss:  0.001108483993448317\n",
            "Training:  Epoch No:  10 Iteration No:  656 \n",
            " Loss:  0.01215953379869461\n",
            "Training:  Epoch No:  10 Iteration No:  657 \n",
            " Loss:  0.0006431796937249601\n",
            "Training:  Epoch No:  10 Iteration No:  658 \n",
            " Loss:  0.0012568146921694279\n",
            "Training:  Epoch No:  10 Iteration No:  659 \n",
            " Loss:  0.0450233593583107\n",
            "Training:  Epoch No:  10 Iteration No:  660 \n",
            " Loss:  0.005799002014100552\n",
            "Training:  Epoch No:  10 Iteration No:  661 \n",
            " Loss:  0.0003629437414929271\n",
            "Training:  Epoch No:  10 Iteration No:  662 \n",
            " Loss:  0.0008603516034781933\n",
            "Training:  Epoch No:  10 Iteration No:  663 \n",
            " Loss:  0.10616530478000641\n",
            "Training:  Epoch No:  10 Iteration No:  664 \n",
            " Loss:  0.001075507141649723\n",
            "Training:  Epoch No:  10 Iteration No:  665 \n",
            " Loss:  0.0032018686179071665\n",
            "Training:  Epoch No:  10 Iteration No:  666 \n",
            " Loss:  0.0015602995408698916\n",
            "Training:  Epoch No:  10 Iteration No:  667 \n",
            " Loss:  0.002023762557655573\n",
            "Training:  Epoch No:  10 Iteration No:  668 \n",
            " Loss:  0.0006030135555192828\n",
            "Training:  Epoch No:  10 Iteration No:  669 \n",
            " Loss:  0.0010929026175290346\n",
            "Training:  Epoch No:  10 Iteration No:  670 \n",
            " Loss:  0.0004521223600022495\n",
            "Training:  Epoch No:  10 Iteration No:  671 \n",
            " Loss:  0.0013584091793745756\n",
            "Training:  Epoch No:  10 Iteration No:  672 \n",
            " Loss:  0.008925197646021843\n",
            "Training:  Epoch No:  10 Iteration No:  673 \n",
            " Loss:  0.0243165772408247\n",
            "Training:  Epoch No:  10 Iteration No:  674 \n",
            " Loss:  0.000813396240118891\n",
            "Training:  Epoch No:  10 Iteration No:  675 \n",
            " Loss:  0.00028276373632252216\n",
            "Training:  Epoch No:  10 Iteration No:  676 \n",
            " Loss:  0.02233795076608658\n",
            "Training:  Epoch No:  10 Iteration No:  677 \n",
            " Loss:  0.0009219393832609057\n",
            "Training:  Epoch No:  10 Iteration No:  678 \n",
            " Loss:  0.0085006607696414\n",
            "Training:  Epoch No:  10 Iteration No:  679 \n",
            " Loss:  0.03240976482629776\n",
            "Training:  Epoch No:  10 Iteration No:  680 \n",
            " Loss:  0.0015020458959043026\n",
            "Training:  Epoch No:  10 Iteration No:  681 \n",
            " Loss:  0.003802142571657896\n",
            "Training:  Epoch No:  10 Iteration No:  682 \n",
            " Loss:  0.0010441815247759223\n",
            "Training:  Epoch No:  10 Iteration No:  683 \n",
            " Loss:  0.0009028645581565797\n",
            "Training:  Epoch No:  10 Iteration No:  684 \n",
            " Loss:  0.001131210126914084\n",
            "Training:  Epoch No:  10 Iteration No:  685 \n",
            " Loss:  0.0089667784050107\n",
            "Training:  Epoch No:  10 Iteration No:  686 \n",
            " Loss:  0.002472760621458292\n",
            "Training:  Epoch No:  10 Iteration No:  687 \n",
            " Loss:  0.00028957659378647804\n",
            "Training:  Epoch No:  10 Iteration No:  688 \n",
            " Loss:  0.00239751348271966\n",
            "Training:  Epoch No:  10 Iteration No:  689 \n",
            " Loss:  0.00029770293622277677\n",
            "Training:  Epoch No:  10 Iteration No:  690 \n",
            " Loss:  0.001674880739301443\n",
            "Training:  Epoch No:  10 Iteration No:  691 \n",
            " Loss:  0.004694445990025997\n",
            "Training:  Epoch No:  10 Iteration No:  692 \n",
            " Loss:  0.0003126663214061409\n",
            "Training:  Epoch No:  10 Iteration No:  693 \n",
            " Loss:  0.001687923213467002\n",
            "Training:  Epoch No:  10 Iteration No:  694 \n",
            " Loss:  0.0016188916051760316\n",
            "Training:  Epoch No:  10 Iteration No:  695 \n",
            " Loss:  0.00010782982280943543\n",
            "Training:  Epoch No:  10 Iteration No:  696 \n",
            " Loss:  0.0017849657451733947\n",
            "Training:  Epoch No:  10 Iteration No:  697 \n",
            " Loss:  0.005835239309817553\n",
            "Training:  Epoch No:  10 Iteration No:  698 \n",
            " Loss:  0.011120998300611973\n",
            "Training:  Epoch No:  10 Iteration No:  699 \n",
            " Loss:  0.010379232466220856\n",
            "Training:  Epoch No:  10 Iteration No:  700 \n",
            " Loss:  0.01358376257121563\n",
            "Training:  Epoch No:  10 Iteration No:  701 \n",
            " Loss:  0.0018384618451818824\n",
            "Training:  Epoch No:  10 Iteration No:  702 \n",
            " Loss:  0.0008920017280615866\n",
            "Training:  Epoch No:  10 Iteration No:  703 \n",
            " Loss:  0.0011189512442797422\n",
            "Training:  Epoch No:  10 Iteration No:  704 \n",
            " Loss:  0.00010515690519241616\n",
            "Training:  Epoch No:  10 Iteration No:  705 \n",
            " Loss:  0.026817819103598595\n",
            "Training:  Epoch No:  10 Iteration No:  706 \n",
            " Loss:  0.0017215789994224906\n",
            "Training:  Epoch No:  10 Iteration No:  707 \n",
            " Loss:  0.009179705753922462\n",
            "Training:  Epoch No:  10 Iteration No:  708 \n",
            " Loss:  0.015432717278599739\n",
            "Training:  Epoch No:  10 Iteration No:  709 \n",
            " Loss:  0.00528601324185729\n",
            "Training:  Epoch No:  10 Iteration No:  710 \n",
            " Loss:  0.0008867006981745362\n",
            "Training:  Epoch No:  10 Iteration No:  711 \n",
            " Loss:  0.013382080011069775\n",
            "Training:  Epoch No:  10 Iteration No:  712 \n",
            " Loss:  0.0010189387248829007\n",
            "Training:  Epoch No:  10 Iteration No:  713 \n",
            " Loss:  0.0004664472653530538\n",
            "Training:  Epoch No:  10 Iteration No:  714 \n",
            " Loss:  0.017684340476989746\n",
            "Training:  Epoch No:  10 Iteration No:  715 \n",
            " Loss:  0.00021557885338552296\n",
            "Training:  Epoch No:  10 Iteration No:  716 \n",
            " Loss:  0.00023173038789536804\n",
            "Training:  Epoch No:  10 Iteration No:  717 \n",
            " Loss:  0.004379131831228733\n",
            "Training:  Epoch No:  10 Iteration No:  718 \n",
            " Loss:  0.005801589228212833\n",
            "Training:  Epoch No:  10 Iteration No:  719 \n",
            " Loss:  0.00586327537894249\n",
            "Training:  Epoch No:  10 Iteration No:  720 \n",
            " Loss:  0.0029047683347016573\n",
            "Training:  Epoch No:  10 Iteration No:  721 \n",
            " Loss:  0.008126375265419483\n",
            "Training:  Epoch No:  10 Iteration No:  722 \n",
            " Loss:  0.07690903544425964\n",
            "Training:  Epoch No:  10 Iteration No:  723 \n",
            " Loss:  0.06617803126573563\n",
            "Training:  Epoch No:  10 Iteration No:  724 \n",
            " Loss:  0.010456409305334091\n",
            "Training:  Epoch No:  10 Iteration No:  725 \n",
            " Loss:  0.001484261709265411\n",
            "Training:  Epoch No:  10 Iteration No:  726 \n",
            " Loss:  0.0024866475723683834\n",
            "Training:  Epoch No:  10 Iteration No:  727 \n",
            " Loss:  0.00042542192386463284\n",
            "Training:  Epoch No:  10 Iteration No:  728 \n",
            " Loss:  0.013673071749508381\n",
            "Training:  Epoch No:  10 Iteration No:  729 \n",
            " Loss:  0.0005102331633679569\n",
            "Training:  Epoch No:  10 Iteration No:  730 \n",
            " Loss:  0.0002040890685748309\n",
            "Training:  Epoch No:  10 Iteration No:  731 \n",
            " Loss:  0.013343618251383305\n",
            "Training:  Epoch No:  10 Iteration No:  732 \n",
            " Loss:  0.0013561596861109138\n",
            "Training:  Epoch No:  10 Iteration No:  733 \n",
            " Loss:  0.0009575624717399478\n",
            "Training:  Epoch No:  10 Iteration No:  734 \n",
            " Loss:  0.0019670745823532343\n",
            "Training:  Epoch No:  10 Iteration No:  735 \n",
            " Loss:  0.00044498080387711525\n",
            "Training:  Epoch No:  10 Iteration No:  736 \n",
            " Loss:  0.0012303270632401109\n",
            "Training:  Epoch No:  10 Iteration No:  737 \n",
            " Loss:  0.004136912990361452\n",
            "Training:  Epoch No:  10 Iteration No:  738 \n",
            " Loss:  0.00020832147856708616\n",
            "Training:  Epoch No:  10 Iteration No:  739 \n",
            " Loss:  0.00222985097207129\n",
            "Training:  Epoch No:  10 Iteration No:  740 \n",
            " Loss:  0.0014588646590709686\n",
            "Training:  Epoch No:  10 Iteration No:  741 \n",
            " Loss:  0.002012302866205573\n",
            "Training:  Epoch No:  10 Iteration No:  742 \n",
            " Loss:  0.0157763808965683\n",
            "Training:  Epoch No:  10 Iteration No:  743 \n",
            " Loss:  0.0004084186803083867\n",
            "Training:  Epoch No:  10 Iteration No:  744 \n",
            " Loss:  0.0006810978520661592\n",
            "Training:  Epoch No:  10 Iteration No:  745 \n",
            " Loss:  0.00039076953544281423\n",
            "Training:  Epoch No:  10 Iteration No:  746 \n",
            " Loss:  0.0017903706757351756\n",
            "Training:  Epoch No:  10 Iteration No:  747 \n",
            " Loss:  0.0050420816987752914\n",
            "Training:  Epoch No:  10 Iteration No:  748 \n",
            " Loss:  0.0005289568798616529\n",
            "Training:  Epoch No:  10 Iteration No:  749 \n",
            " Loss:  0.010318568907678127\n",
            "Training:  Epoch No:  10 Iteration No:  750 \n",
            " Loss:  0.0046881865710020065\n",
            "Training:  Epoch No:  10 Iteration No:  751 \n",
            " Loss:  0.00055194046581164\n",
            "Training:  Epoch No:  10 Iteration No:  752 \n",
            " Loss:  0.003086430486291647\n",
            "Training:  Epoch No:  10 Iteration No:  753 \n",
            " Loss:  0.00016806807252578437\n",
            "Training:  Epoch No:  10 Iteration No:  754 \n",
            " Loss:  0.0016949892742559314\n",
            "Training:  Epoch No:  10 Iteration No:  755 \n",
            " Loss:  0.00010973333701258525\n",
            "Training:  Epoch No:  10 Iteration No:  756 \n",
            " Loss:  0.00035738732549361885\n",
            "Training:  Epoch No:  10 Iteration No:  757 \n",
            " Loss:  0.02027425728738308\n",
            "Training:  Epoch No:  10 Iteration No:  758 \n",
            " Loss:  0.0018096014391630888\n",
            "Training:  Epoch No:  10 Iteration No:  759 \n",
            " Loss:  0.001335550332441926\n",
            "Training:  Epoch No:  10 Iteration No:  760 \n",
            " Loss:  0.004237642511725426\n",
            "Training:  Epoch No:  10 Iteration No:  761 \n",
            " Loss:  0.00047135018394328654\n",
            "Training:  Epoch No:  10 Iteration No:  762 \n",
            " Loss:  0.0004502507799770683\n",
            "Training:  Epoch No:  10 Iteration No:  763 \n",
            " Loss:  0.02661697193980217\n",
            "Training:  Epoch No:  10 Iteration No:  764 \n",
            " Loss:  0.0011691136751323938\n",
            "Training:  Epoch No:  10 Iteration No:  765 \n",
            " Loss:  0.045462459325790405\n",
            "Training:  Epoch No:  10 Iteration No:  766 \n",
            " Loss:  0.0004608860472217202\n",
            "Training:  Epoch No:  10 Iteration No:  767 \n",
            " Loss:  0.0006516725406982005\n",
            "Training:  Epoch No:  10 Iteration No:  768 \n",
            " Loss:  0.0003535513242240995\n",
            "Training:  Epoch No:  10 Iteration No:  769 \n",
            " Loss:  0.015662770718336105\n",
            "Training:  Epoch No:  10 Iteration No:  770 \n",
            " Loss:  0.0005145070026628673\n",
            "Training:  Epoch No:  10 Iteration No:  771 \n",
            " Loss:  0.0006237171473912895\n",
            "Training:  Epoch No:  10 Iteration No:  772 \n",
            " Loss:  0.0031522391363978386\n",
            "Training:  Epoch No:  10 Iteration No:  773 \n",
            " Loss:  0.0003309667808935046\n",
            "Training:  Epoch No:  10 Iteration No:  774 \n",
            " Loss:  0.0008359379135072231\n",
            "Training:  Epoch No:  10 Iteration No:  775 \n",
            " Loss:  0.0002766404068097472\n",
            "Training:  Epoch No:  10 Iteration No:  776 \n",
            " Loss:  0.0011395909823477268\n",
            "Training:  Epoch No:  10 Iteration No:  777 \n",
            " Loss:  0.003970257937908173\n",
            "Training:  Epoch No:  10 Iteration No:  778 \n",
            " Loss:  0.0013690171763300896\n",
            "Training:  Epoch No:  10 Iteration No:  779 \n",
            " Loss:  0.0019458860624581575\n",
            "Training:  Epoch No:  10 Iteration No:  780 \n",
            " Loss:  0.00261145131662488\n",
            "Training:  Epoch No:  10 Iteration No:  781 \n",
            " Loss:  0.0009817900136113167\n",
            "Training:  Epoch No:  10 Iteration No:  782 \n",
            " Loss:  0.003993513528257608\n",
            "Training:  Epoch No:  10 Iteration No:  783 \n",
            " Loss:  0.001329783583059907\n",
            "Training:  Epoch No:  10 Iteration No:  784 \n",
            " Loss:  0.0007661837735213339\n",
            "Training:  Epoch No:  10 Iteration No:  785 \n",
            " Loss:  0.007390029262751341\n",
            "Training:  Epoch No:  10 Iteration No:  786 \n",
            " Loss:  0.0280477125197649\n",
            "Training:  Epoch No:  10 Iteration No:  787 \n",
            " Loss:  0.0030548761133104563\n",
            "Training:  Epoch No:  10 Iteration No:  788 \n",
            " Loss:  0.005266535561531782\n",
            "Training:  Epoch No:  10 Iteration No:  789 \n",
            " Loss:  0.010598788037896156\n",
            "Training:  Epoch No:  10 Iteration No:  790 \n",
            " Loss:  9.157870226772502e-05\n",
            "Training:  Epoch No:  10 Iteration No:  791 \n",
            " Loss:  0.004774673376232386\n",
            "Training:  Epoch No:  10 Iteration No:  792 \n",
            " Loss:  0.00011105117300758138\n",
            "Training:  Epoch No:  10 Iteration No:  793 \n",
            " Loss:  0.0019099797355011106\n",
            "Training:  Epoch No:  10 Iteration No:  794 \n",
            " Loss:  0.00028118217596784234\n",
            "Training:  Epoch No:  10 Iteration No:  795 \n",
            " Loss:  0.0006572366692125797\n",
            "Training:  Epoch No:  10 Iteration No:  796 \n",
            " Loss:  0.0003965750220231712\n",
            "Training:  Epoch No:  10 Iteration No:  797 \n",
            " Loss:  0.00016238987154792994\n",
            "Training:  Epoch No:  10 Iteration No:  798 \n",
            " Loss:  0.0005981610738672316\n",
            "Training:  Epoch No:  10 Iteration No:  799 \n",
            " Loss:  0.00036862355773337185\n",
            "Training:  Epoch No:  10 Iteration No:  800 \n",
            " Loss:  0.003136301413178444\n",
            "Training:  Epoch No:  10 Iteration No:  801 \n",
            " Loss:  0.00668361596763134\n",
            "Training:  Epoch No:  10 Iteration No:  802 \n",
            " Loss:  0.021300172433257103\n",
            "Training:  Epoch No:  10 Iteration No:  803 \n",
            " Loss:  0.00121359305921942\n",
            "Training:  Epoch No:  10 Iteration No:  804 \n",
            " Loss:  0.002651089569553733\n",
            "Training:  Epoch No:  10 Iteration No:  805 \n",
            " Loss:  0.00018709205323830247\n",
            "Training:  Epoch No:  10 Iteration No:  806 \n",
            " Loss:  0.0007387250661849976\n",
            "Training:  Epoch No:  10 Iteration No:  807 \n",
            " Loss:  0.012490230612456799\n",
            "Training:  Epoch No:  10 Iteration No:  808 \n",
            " Loss:  0.000763454707339406\n",
            "Training:  Epoch No:  10 Iteration No:  809 \n",
            " Loss:  0.047422561794519424\n",
            "Training:  Epoch No:  10 Iteration No:  810 \n",
            " Loss:  0.0006238599307835102\n",
            "Training:  Epoch No:  10 Iteration No:  811 \n",
            " Loss:  0.0038225201424211264\n",
            "Training:  Epoch No:  10 Iteration No:  812 \n",
            " Loss:  0.005301997065544128\n",
            "Training:  Epoch No:  10 Iteration No:  813 \n",
            " Loss:  0.002169328974559903\n",
            "Training:  Epoch No:  10 Iteration No:  814 \n",
            " Loss:  0.00033728877315297723\n",
            "Training:  Epoch No:  10 Iteration No:  815 \n",
            " Loss:  0.002364661078900099\n",
            "Training:  Epoch No:  10 Iteration No:  816 \n",
            " Loss:  0.0017143131699413061\n",
            "Training:  Epoch No:  10 Iteration No:  817 \n",
            " Loss:  0.0125718479976058\n",
            "Training:  Epoch No:  10 Iteration No:  818 \n",
            " Loss:  0.0005755206220783293\n",
            "Training:  Epoch No:  10 Iteration No:  819 \n",
            " Loss:  0.000256716477451846\n",
            "Training:  Epoch No:  10 Iteration No:  820 \n",
            " Loss:  0.0011000285157933831\n",
            "Training:  Epoch No:  10 Iteration No:  821 \n",
            " Loss:  0.002997945062816143\n",
            "Training:  Epoch No:  10 Iteration No:  822 \n",
            " Loss:  0.0003931485116481781\n",
            "Training:  Epoch No:  10 Iteration No:  823 \n",
            " Loss:  0.010652104392647743\n",
            "Training:  Epoch No:  10 Iteration No:  824 \n",
            " Loss:  0.0001459497434552759\n",
            "Training:  Epoch No:  10 Iteration No:  825 \n",
            " Loss:  0.00018334401829633862\n",
            "Training:  Epoch No:  10 Iteration No:  826 \n",
            " Loss:  0.0029583140276372433\n",
            "Training:  Epoch No:  10 Iteration No:  827 \n",
            " Loss:  0.000770472688600421\n",
            "Training:  Epoch No:  10 Iteration No:  828 \n",
            " Loss:  0.0011812306474894285\n",
            "Training:  Epoch No:  10 Iteration No:  829 \n",
            " Loss:  0.049156442284584045\n",
            "Training:  Epoch No:  10 Iteration No:  830 \n",
            " Loss:  0.0008746154489926994\n",
            "Training:  Epoch No:  10 Iteration No:  831 \n",
            " Loss:  0.037986211478710175\n",
            "Training:  Epoch No:  10 Iteration No:  832 \n",
            " Loss:  0.011019556783139706\n",
            "Training:  Epoch No:  10 Iteration No:  833 \n",
            " Loss:  0.004407759290188551\n",
            "Training:  Epoch No:  10 Iteration No:  834 \n",
            " Loss:  0.06074269860982895\n",
            "Training:  Epoch No:  10 Iteration No:  835 \n",
            " Loss:  0.00021295117039699107\n",
            "Training:  Epoch No:  10 Iteration No:  836 \n",
            " Loss:  0.0004525173280853778\n",
            "Training:  Epoch No:  10 Iteration No:  837 \n",
            " Loss:  0.0004548901051748544\n",
            "Training:  Epoch No:  10 Iteration No:  838 \n",
            " Loss:  0.0016560395015403628\n",
            "Training:  Epoch No:  10 Iteration No:  839 \n",
            " Loss:  0.000540350389201194\n",
            "Training:  Epoch No:  10 Iteration No:  840 \n",
            " Loss:  0.0007336625712923706\n",
            "Training:  Epoch No:  10 Iteration No:  841 \n",
            " Loss:  0.004264191258698702\n",
            "Training:  Epoch No:  10 Iteration No:  842 \n",
            " Loss:  0.0007487498223781586\n",
            "Training:  Epoch No:  10 Iteration No:  843 \n",
            " Loss:  0.005545467603951693\n",
            "Training:  Epoch No:  10 Iteration No:  844 \n",
            " Loss:  0.0027311923913657665\n",
            "Training:  Epoch No:  10 Iteration No:  845 \n",
            " Loss:  0.022663883864879608\n",
            "Training:  Epoch No:  10 Iteration No:  846 \n",
            " Loss:  7.683847070438787e-05\n",
            "Training:  Epoch No:  10 Iteration No:  847 \n",
            " Loss:  0.001965734176337719\n",
            "Training:  Epoch No:  10 Iteration No:  848 \n",
            " Loss:  0.0014168568886816502\n",
            "Training:  Epoch No:  10 Iteration No:  849 \n",
            " Loss:  0.006580385845154524\n",
            "Training:  Epoch No:  10 Iteration No:  850 \n",
            " Loss:  0.0029380463529378176\n",
            "Training:  Epoch No:  10 Iteration No:  851 \n",
            " Loss:  0.01371843833476305\n",
            "Training:  Epoch No:  10 Iteration No:  852 \n",
            " Loss:  0.013535196892917156\n",
            "Training:  Epoch No:  10 Iteration No:  853 \n",
            " Loss:  0.004681557882577181\n",
            "Training:  Epoch No:  10 Iteration No:  854 \n",
            " Loss:  7.027018727967516e-05\n",
            "Training:  Epoch No:  10 Iteration No:  855 \n",
            " Loss:  0.011093861423432827\n",
            "Training:  Epoch No:  10 Iteration No:  856 \n",
            " Loss:  0.0001380125613650307\n",
            "Training:  Epoch No:  10 Iteration No:  857 \n",
            " Loss:  0.0003411451179999858\n",
            "Training:  Epoch No:  10 Iteration No:  858 \n",
            " Loss:  0.0002177157875848934\n",
            "Training:  Epoch No:  10 Iteration No:  859 \n",
            " Loss:  0.0003075128479395062\n",
            "Training:  Epoch No:  10 Iteration No:  860 \n",
            " Loss:  0.0007895508897490799\n",
            "Training:  Epoch No:  10 Iteration No:  861 \n",
            " Loss:  0.00568115059286356\n",
            "Training:  Epoch No:  10 Iteration No:  862 \n",
            " Loss:  0.023175641894340515\n",
            "Training:  Epoch No:  10 Iteration No:  863 \n",
            " Loss:  0.00028191361343488097\n",
            "Training:  Epoch No:  10 Iteration No:  864 \n",
            " Loss:  0.03801104053854942\n",
            "Training:  Epoch No:  10 Iteration No:  865 \n",
            " Loss:  0.0014112787321209908\n",
            "Training:  Epoch No:  10 Iteration No:  866 \n",
            " Loss:  0.02275865152478218\n",
            "Training:  Epoch No:  10 Iteration No:  867 \n",
            " Loss:  0.005000281613320112\n",
            "Training:  Epoch No:  10 Iteration No:  868 \n",
            " Loss:  0.011942272074520588\n",
            "Training:  Epoch No:  10 Iteration No:  869 \n",
            " Loss:  0.0006163713987916708\n",
            "Training:  Epoch No:  10 Iteration No:  870 \n",
            " Loss:  0.000792789738625288\n",
            "Training:  Epoch No:  10 Iteration No:  871 \n",
            " Loss:  0.00859449990093708\n",
            "Training:  Epoch No:  10 Iteration No:  872 \n",
            " Loss:  0.0003029356012120843\n",
            "Training:  Epoch No:  10 Iteration No:  873 \n",
            " Loss:  0.0007853164570406079\n",
            "Training:  Epoch No:  10 Iteration No:  874 \n",
            " Loss:  0.02046327292919159\n",
            "Training:  Epoch No:  10 Iteration No:  875 \n",
            " Loss:  0.002796401269733906\n",
            "Training:  Epoch No:  10 Iteration No:  876 \n",
            " Loss:  3.181227657478303e-05\n",
            "Training:  Epoch No:  10 Iteration No:  877 \n",
            " Loss:  0.0020018506329506636\n",
            "Training:  Epoch No:  10 Iteration No:  878 \n",
            " Loss:  0.0034159021452069283\n",
            "Training:  Epoch No:  10 Iteration No:  879 \n",
            " Loss:  0.0014486446743831038\n",
            "Training:  Epoch No:  10 Iteration No:  880 \n",
            " Loss:  0.010281422175467014\n",
            "Training:  Epoch No:  10 Iteration No:  881 \n",
            " Loss:  0.00021748224389739335\n",
            "Training:  Epoch No:  10 Iteration No:  882 \n",
            " Loss:  0.0041355653665959835\n",
            "Training:  Epoch No:  10 Iteration No:  883 \n",
            " Loss:  0.0006378399557434022\n",
            "Training:  Epoch No:  10 Iteration No:  884 \n",
            " Loss:  0.0005517450044862926\n",
            "Training:  Epoch No:  10 Iteration No:  885 \n",
            " Loss:  0.0013330260990187526\n",
            "Training:  Epoch No:  10 Iteration No:  886 \n",
            " Loss:  0.0007176228100433946\n",
            "Training:  Epoch No:  10 Iteration No:  887 \n",
            " Loss:  0.0026095949579030275\n",
            "Training:  Epoch No:  10 Iteration No:  888 \n",
            " Loss:  0.0019272429635748267\n",
            "Training:  Epoch No:  10 Iteration No:  889 \n",
            " Loss:  0.0008275961154140532\n",
            "Training:  Epoch No:  10 Iteration No:  890 \n",
            " Loss:  0.010718034580349922\n",
            "Training:  Epoch No:  10 Iteration No:  891 \n",
            " Loss:  0.0022988442797213793\n",
            "Training:  Epoch No:  10 Iteration No:  892 \n",
            " Loss:  0.0004484603996388614\n",
            "Training:  Epoch No:  10 Iteration No:  893 \n",
            " Loss:  0.0003955405263695866\n",
            "Training:  Epoch No:  10 Iteration No:  894 \n",
            " Loss:  0.008623664267361164\n",
            "Training:  Epoch No:  10 Iteration No:  895 \n",
            " Loss:  0.0004853396094404161\n",
            "Training:  Epoch No:  10 Iteration No:  896 \n",
            " Loss:  0.0032591454219073057\n",
            "Training:  Epoch No:  10 Iteration No:  897 \n",
            " Loss:  0.001598438946530223\n",
            "Training:  Epoch No:  10 Iteration No:  898 \n",
            " Loss:  0.002626074245199561\n",
            "Training:  Epoch No:  10 Iteration No:  899 \n",
            " Loss:  0.0009576770826242864\n",
            "Training:  Epoch No:  10 Iteration No:  900 \n",
            " Loss:  0.0011592264054343104\n",
            "Training:  Epoch No:  10 Iteration No:  901 \n",
            " Loss:  0.0008587075863033533\n",
            "Training:  Epoch No:  10 Iteration No:  902 \n",
            " Loss:  0.0023944173008203506\n",
            "Training:  Epoch No:  10 Iteration No:  903 \n",
            " Loss:  0.0004377463774289936\n",
            "Training:  Epoch No:  10 Iteration No:  904 \n",
            " Loss:  0.03691493719816208\n",
            "Training:  Epoch No:  10 Iteration No:  905 \n",
            " Loss:  0.0005463396082632244\n",
            "Training:  Epoch No:  10 Iteration No:  906 \n",
            " Loss:  0.00023471056192647666\n",
            "Training:  Epoch No:  10 Iteration No:  907 \n",
            " Loss:  0.0025344074238091707\n",
            "Training:  Epoch No:  10 Iteration No:  908 \n",
            " Loss:  0.0010678877588361502\n",
            "Training:  Epoch No:  10 Iteration No:  909 \n",
            " Loss:  0.0009316169307567179\n",
            "Training:  Epoch No:  10 Iteration No:  910 \n",
            " Loss:  0.0020942892879247665\n",
            "Training:  Epoch No:  10 Iteration No:  911 \n",
            " Loss:  0.0002658941375557333\n",
            "Training:  Epoch No:  10 Iteration No:  912 \n",
            " Loss:  0.0035198910627514124\n",
            "Training:  Epoch No:  10 Iteration No:  913 \n",
            " Loss:  0.001640920527279377\n",
            "Training:  Epoch No:  10 Iteration No:  914 \n",
            " Loss:  0.00311086792498827\n",
            "Training:  Epoch No:  10 Iteration No:  915 \n",
            " Loss:  0.0031661083921790123\n",
            "Training:  Epoch No:  10 Iteration No:  916 \n",
            " Loss:  0.04287080094218254\n",
            "Training:  Epoch No:  10 Iteration No:  917 \n",
            " Loss:  0.0025859319139271975\n",
            "Training:  Epoch No:  10 Iteration No:  918 \n",
            " Loss:  0.009054158814251423\n",
            "Training:  Epoch No:  10 Iteration No:  919 \n",
            " Loss:  0.0030473293736577034\n",
            "Training:  Epoch No:  10 Iteration No:  920 \n",
            " Loss:  0.002888747025281191\n",
            "Training:  Epoch No:  10 Iteration No:  921 \n",
            " Loss:  0.0012718643993139267\n",
            "Training:  Epoch No:  10 Iteration No:  922 \n",
            " Loss:  0.004387394059449434\n",
            "Training:  Epoch No:  10 Iteration No:  923 \n",
            " Loss:  0.016237014904618263\n",
            "Training:  Epoch No:  10 Iteration No:  924 \n",
            " Loss:  0.0020829609129577875\n",
            "Training:  Epoch No:  10 Iteration No:  925 \n",
            " Loss:  0.002354185562580824\n",
            "Training:  Epoch No:  10 Iteration No:  926 \n",
            " Loss:  0.0002723475336097181\n",
            "Training:  Epoch No:  10 Iteration No:  927 \n",
            " Loss:  0.00331403617747128\n",
            "Training:  Epoch No:  10 Iteration No:  928 \n",
            " Loss:  0.004197337199002504\n",
            "Training:  Epoch No:  10 Iteration No:  929 \n",
            " Loss:  0.001214122399687767\n",
            "Training:  Epoch No:  10 Iteration No:  930 \n",
            " Loss:  0.016687210649251938\n",
            "Training:  Epoch No:  10 Iteration No:  931 \n",
            " Loss:  0.049507007002830505\n",
            "Training:  Epoch No:  10 Iteration No:  932 \n",
            " Loss:  0.008600329048931599\n",
            "Training:  Epoch No:  10 Iteration No:  933 \n",
            " Loss:  0.0038179790135473013\n",
            "Training:  Epoch No:  10 Iteration No:  934 \n",
            " Loss:  0.0038912114687263966\n",
            "Training:  Epoch No:  10 Iteration No:  935 \n",
            " Loss:  0.002547954674810171\n",
            "Training:  Epoch No:  10 Iteration No:  936 \n",
            " Loss:  0.0003840838326141238\n",
            "Training:  Epoch No:  10 Iteration No:  937 \n",
            " Loss:  0.0010210402542725205\n",
            "Training:  Epoch No:  10 Iteration No:  938 \n",
            " Loss:  0.00016581885574851185\n",
            "Validation:  Epoch No:  10 \n",
            " Loss:  0.04099648925289779\n",
            "validation accuracy:  98.9\n",
            "Training:  Epoch No:  11 Iteration No:  1 \n",
            " Loss:  0.00017073965864256024\n",
            "Training:  Epoch No:  11 Iteration No:  2 \n",
            " Loss:  0.0014721842017024755\n",
            "Training:  Epoch No:  11 Iteration No:  3 \n",
            " Loss:  0.0006274422630667686\n",
            "Training:  Epoch No:  11 Iteration No:  4 \n",
            " Loss:  0.0024805983994156122\n",
            "Training:  Epoch No:  11 Iteration No:  5 \n",
            " Loss:  0.002261846326291561\n",
            "Training:  Epoch No:  11 Iteration No:  6 \n",
            " Loss:  0.0021188114769756794\n",
            "Training:  Epoch No:  11 Iteration No:  7 \n",
            " Loss:  0.0006987174856476486\n",
            "Training:  Epoch No:  11 Iteration No:  8 \n",
            " Loss:  0.007192609366029501\n",
            "Training:  Epoch No:  11 Iteration No:  9 \n",
            " Loss:  0.014229781925678253\n",
            "Training:  Epoch No:  11 Iteration No:  10 \n",
            " Loss:  0.00020978960674256086\n",
            "Training:  Epoch No:  11 Iteration No:  11 \n",
            " Loss:  0.0013666190207004547\n",
            "Training:  Epoch No:  11 Iteration No:  12 \n",
            " Loss:  0.0014315168373286724\n",
            "Training:  Epoch No:  11 Iteration No:  13 \n",
            " Loss:  0.0014214611146599054\n",
            "Training:  Epoch No:  11 Iteration No:  14 \n",
            " Loss:  0.001354696461930871\n",
            "Training:  Epoch No:  11 Iteration No:  15 \n",
            " Loss:  0.002056552330031991\n",
            "Training:  Epoch No:  11 Iteration No:  16 \n",
            " Loss:  0.0006374377990141511\n",
            "Training:  Epoch No:  11 Iteration No:  17 \n",
            " Loss:  0.00568864680826664\n",
            "Training:  Epoch No:  11 Iteration No:  18 \n",
            " Loss:  0.0015640464844182134\n",
            "Training:  Epoch No:  11 Iteration No:  19 \n",
            " Loss:  0.0008734501316212118\n",
            "Training:  Epoch No:  11 Iteration No:  20 \n",
            " Loss:  0.002179427072405815\n",
            "Training:  Epoch No:  11 Iteration No:  21 \n",
            " Loss:  0.0007620235555805266\n",
            "Training:  Epoch No:  11 Iteration No:  22 \n",
            " Loss:  8.523316500941291e-05\n",
            "Training:  Epoch No:  11 Iteration No:  23 \n",
            " Loss:  0.0019635786302387714\n",
            "Training:  Epoch No:  11 Iteration No:  24 \n",
            " Loss:  0.030188506469130516\n",
            "Training:  Epoch No:  11 Iteration No:  25 \n",
            " Loss:  0.020183539018034935\n",
            "Training:  Epoch No:  11 Iteration No:  26 \n",
            " Loss:  0.029366470873355865\n",
            "Training:  Epoch No:  11 Iteration No:  27 \n",
            " Loss:  0.00013805697381030768\n",
            "Training:  Epoch No:  11 Iteration No:  28 \n",
            " Loss:  0.004508406855165958\n",
            "Training:  Epoch No:  11 Iteration No:  29 \n",
            " Loss:  0.0018821844132617116\n",
            "Training:  Epoch No:  11 Iteration No:  30 \n",
            " Loss:  0.0018243900267407298\n",
            "Training:  Epoch No:  11 Iteration No:  31 \n",
            " Loss:  0.0007110423757694662\n",
            "Training:  Epoch No:  11 Iteration No:  32 \n",
            " Loss:  0.0008358590421266854\n",
            "Training:  Epoch No:  11 Iteration No:  33 \n",
            " Loss:  0.00043686272692866623\n",
            "Training:  Epoch No:  11 Iteration No:  34 \n",
            " Loss:  0.002846293617039919\n",
            "Training:  Epoch No:  11 Iteration No:  35 \n",
            " Loss:  0.0005215219571255147\n",
            "Training:  Epoch No:  11 Iteration No:  36 \n",
            " Loss:  0.0014234150294214487\n",
            "Training:  Epoch No:  11 Iteration No:  37 \n",
            " Loss:  0.0001645138836465776\n",
            "Training:  Epoch No:  11 Iteration No:  38 \n",
            " Loss:  0.000402587786084041\n",
            "Training:  Epoch No:  11 Iteration No:  39 \n",
            " Loss:  0.00020992336794734\n",
            "Training:  Epoch No:  11 Iteration No:  40 \n",
            " Loss:  0.0010632926132529974\n",
            "Training:  Epoch No:  11 Iteration No:  41 \n",
            " Loss:  0.008389457128942013\n",
            "Training:  Epoch No:  11 Iteration No:  42 \n",
            " Loss:  0.0006513897678814828\n",
            "Training:  Epoch No:  11 Iteration No:  43 \n",
            " Loss:  0.0017197219422087073\n",
            "Training:  Epoch No:  11 Iteration No:  44 \n",
            " Loss:  0.003883661702275276\n",
            "Training:  Epoch No:  11 Iteration No:  45 \n",
            " Loss:  0.00032518067746423185\n",
            "Training:  Epoch No:  11 Iteration No:  46 \n",
            " Loss:  0.006985430605709553\n",
            "Training:  Epoch No:  11 Iteration No:  47 \n",
            " Loss:  0.04288243129849434\n",
            "Training:  Epoch No:  11 Iteration No:  48 \n",
            " Loss:  0.000757181434892118\n",
            "Training:  Epoch No:  11 Iteration No:  49 \n",
            " Loss:  0.018813760951161385\n",
            "Training:  Epoch No:  11 Iteration No:  50 \n",
            " Loss:  0.001375617110170424\n",
            "Training:  Epoch No:  11 Iteration No:  51 \n",
            " Loss:  0.0010006456868723035\n",
            "Training:  Epoch No:  11 Iteration No:  52 \n",
            " Loss:  0.0006334183854050934\n",
            "Training:  Epoch No:  11 Iteration No:  53 \n",
            " Loss:  0.008476953953504562\n",
            "Training:  Epoch No:  11 Iteration No:  54 \n",
            " Loss:  0.007256528828293085\n",
            "Training:  Epoch No:  11 Iteration No:  55 \n",
            " Loss:  0.0003966763033531606\n",
            "Training:  Epoch No:  11 Iteration No:  56 \n",
            " Loss:  0.02111712470650673\n",
            "Training:  Epoch No:  11 Iteration No:  57 \n",
            " Loss:  0.0022283941507339478\n",
            "Training:  Epoch No:  11 Iteration No:  58 \n",
            " Loss:  0.0002535223902668804\n",
            "Training:  Epoch No:  11 Iteration No:  59 \n",
            " Loss:  0.0007586442516185343\n",
            "Training:  Epoch No:  11 Iteration No:  60 \n",
            " Loss:  0.00770656019449234\n",
            "Training:  Epoch No:  11 Iteration No:  61 \n",
            " Loss:  0.0020244845654815435\n",
            "Training:  Epoch No:  11 Iteration No:  62 \n",
            " Loss:  3.5290420782985166e-05\n",
            "Training:  Epoch No:  11 Iteration No:  63 \n",
            " Loss:  0.0014144792221486568\n",
            "Training:  Epoch No:  11 Iteration No:  64 \n",
            " Loss:  0.03275898098945618\n",
            "Training:  Epoch No:  11 Iteration No:  65 \n",
            " Loss:  0.0006165344384498894\n",
            "Training:  Epoch No:  11 Iteration No:  66 \n",
            " Loss:  0.0020685545168817043\n",
            "Training:  Epoch No:  11 Iteration No:  67 \n",
            " Loss:  2.9570199330919422e-05\n",
            "Training:  Epoch No:  11 Iteration No:  68 \n",
            " Loss:  0.0031406276393681765\n",
            "Training:  Epoch No:  11 Iteration No:  69 \n",
            " Loss:  0.0034074203576892614\n",
            "Training:  Epoch No:  11 Iteration No:  70 \n",
            " Loss:  0.0037704817950725555\n",
            "Training:  Epoch No:  11 Iteration No:  71 \n",
            " Loss:  0.001508620218373835\n",
            "Training:  Epoch No:  11 Iteration No:  72 \n",
            " Loss:  0.002003054367378354\n",
            "Training:  Epoch No:  11 Iteration No:  73 \n",
            " Loss:  0.0028455329593271017\n",
            "Training:  Epoch No:  11 Iteration No:  74 \n",
            " Loss:  0.010208161547780037\n",
            "Training:  Epoch No:  11 Iteration No:  75 \n",
            " Loss:  0.00165788852609694\n",
            "Training:  Epoch No:  11 Iteration No:  76 \n",
            " Loss:  0.00029741687467321754\n",
            "Training:  Epoch No:  11 Iteration No:  77 \n",
            " Loss:  0.0006605667294934392\n",
            "Training:  Epoch No:  11 Iteration No:  78 \n",
            " Loss:  0.0029199249111115932\n",
            "Training:  Epoch No:  11 Iteration No:  79 \n",
            " Loss:  0.0001219379118992947\n",
            "Training:  Epoch No:  11 Iteration No:  80 \n",
            " Loss:  0.0007335148984566331\n",
            "Training:  Epoch No:  11 Iteration No:  81 \n",
            " Loss:  0.003961540758609772\n",
            "Training:  Epoch No:  11 Iteration No:  82 \n",
            " Loss:  0.0010403466876596212\n",
            "Training:  Epoch No:  11 Iteration No:  83 \n",
            " Loss:  0.0025325475726276636\n",
            "Training:  Epoch No:  11 Iteration No:  84 \n",
            " Loss:  0.0019127115374431014\n",
            "Training:  Epoch No:  11 Iteration No:  85 \n",
            " Loss:  0.002467211103066802\n",
            "Training:  Epoch No:  11 Iteration No:  86 \n",
            " Loss:  0.006149670574814081\n",
            "Training:  Epoch No:  11 Iteration No:  87 \n",
            " Loss:  0.03051689825952053\n",
            "Training:  Epoch No:  11 Iteration No:  88 \n",
            " Loss:  0.008187126368284225\n",
            "Training:  Epoch No:  11 Iteration No:  89 \n",
            " Loss:  0.0017057196237146854\n",
            "Training:  Epoch No:  11 Iteration No:  90 \n",
            " Loss:  0.001020539435558021\n",
            "Training:  Epoch No:  11 Iteration No:  91 \n",
            " Loss:  0.0010442010825499892\n",
            "Training:  Epoch No:  11 Iteration No:  92 \n",
            " Loss:  0.001990320160984993\n",
            "Training:  Epoch No:  11 Iteration No:  93 \n",
            " Loss:  0.00033431214978918433\n",
            "Training:  Epoch No:  11 Iteration No:  94 \n",
            " Loss:  0.0016921525821089745\n",
            "Training:  Epoch No:  11 Iteration No:  95 \n",
            " Loss:  0.0023967158049345016\n",
            "Training:  Epoch No:  11 Iteration No:  96 \n",
            " Loss:  0.00011337234172970057\n",
            "Training:  Epoch No:  11 Iteration No:  97 \n",
            " Loss:  0.00016950155259110034\n",
            "Training:  Epoch No:  11 Iteration No:  98 \n",
            " Loss:  0.002124740742146969\n",
            "Training:  Epoch No:  11 Iteration No:  99 \n",
            " Loss:  0.00029710616217926145\n",
            "Training:  Epoch No:  11 Iteration No:  100 \n",
            " Loss:  0.0001260544522665441\n",
            "Training:  Epoch No:  11 Iteration No:  101 \n",
            " Loss:  0.0003981402260251343\n",
            "Training:  Epoch No:  11 Iteration No:  102 \n",
            " Loss:  0.0007086861878633499\n",
            "Training:  Epoch No:  11 Iteration No:  103 \n",
            " Loss:  0.0009438149863854051\n",
            "Training:  Epoch No:  11 Iteration No:  104 \n",
            " Loss:  0.0010582136455923319\n",
            "Training:  Epoch No:  11 Iteration No:  105 \n",
            " Loss:  0.0011963481083512306\n",
            "Training:  Epoch No:  11 Iteration No:  106 \n",
            " Loss:  0.00021620982442982495\n",
            "Training:  Epoch No:  11 Iteration No:  107 \n",
            " Loss:  0.00864805094897747\n",
            "Training:  Epoch No:  11 Iteration No:  108 \n",
            " Loss:  0.0027236873283982277\n",
            "Training:  Epoch No:  11 Iteration No:  109 \n",
            " Loss:  0.0005990723730064929\n",
            "Training:  Epoch No:  11 Iteration No:  110 \n",
            " Loss:  0.00017342880892101675\n",
            "Training:  Epoch No:  11 Iteration No:  111 \n",
            " Loss:  0.002861183602362871\n",
            "Training:  Epoch No:  11 Iteration No:  112 \n",
            " Loss:  0.001962951384484768\n",
            "Training:  Epoch No:  11 Iteration No:  113 \n",
            " Loss:  0.0004192852065898478\n",
            "Training:  Epoch No:  11 Iteration No:  114 \n",
            " Loss:  0.0010014300933107734\n",
            "Training:  Epoch No:  11 Iteration No:  115 \n",
            " Loss:  0.049453962594270706\n",
            "Training:  Epoch No:  11 Iteration No:  116 \n",
            " Loss:  0.00012752125621773303\n",
            "Training:  Epoch No:  11 Iteration No:  117 \n",
            " Loss:  0.007638959214091301\n",
            "Training:  Epoch No:  11 Iteration No:  118 \n",
            " Loss:  0.025217615067958832\n",
            "Training:  Epoch No:  11 Iteration No:  119 \n",
            " Loss:  0.0013460112968459725\n",
            "Training:  Epoch No:  11 Iteration No:  120 \n",
            " Loss:  0.0017131036147475243\n",
            "Training:  Epoch No:  11 Iteration No:  121 \n",
            " Loss:  0.0009885943727567792\n",
            "Training:  Epoch No:  11 Iteration No:  122 \n",
            " Loss:  0.00030519775464199483\n",
            "Training:  Epoch No:  11 Iteration No:  123 \n",
            " Loss:  0.0035488831344991922\n",
            "Training:  Epoch No:  11 Iteration No:  124 \n",
            " Loss:  0.000658825971186161\n",
            "Training:  Epoch No:  11 Iteration No:  125 \n",
            " Loss:  0.008646107278764248\n",
            "Training:  Epoch No:  11 Iteration No:  126 \n",
            " Loss:  0.00043878055294044316\n",
            "Training:  Epoch No:  11 Iteration No:  127 \n",
            " Loss:  0.01132695097476244\n",
            "Training:  Epoch No:  11 Iteration No:  128 \n",
            " Loss:  0.00028100801864638925\n",
            "Training:  Epoch No:  11 Iteration No:  129 \n",
            " Loss:  0.0002049053437076509\n",
            "Training:  Epoch No:  11 Iteration No:  130 \n",
            " Loss:  0.00014705034845974296\n",
            "Training:  Epoch No:  11 Iteration No:  131 \n",
            " Loss:  0.001507659675553441\n",
            "Training:  Epoch No:  11 Iteration No:  132 \n",
            " Loss:  0.00021242056391201913\n",
            "Training:  Epoch No:  11 Iteration No:  133 \n",
            " Loss:  0.00046461084275506437\n",
            "Training:  Epoch No:  11 Iteration No:  134 \n",
            " Loss:  0.027486348524689674\n",
            "Training:  Epoch No:  11 Iteration No:  135 \n",
            " Loss:  1.658809014770668e-05\n",
            "Training:  Epoch No:  11 Iteration No:  136 \n",
            " Loss:  0.0003714333870448172\n",
            "Training:  Epoch No:  11 Iteration No:  137 \n",
            " Loss:  0.00021447385370265692\n",
            "Training:  Epoch No:  11 Iteration No:  138 \n",
            " Loss:  0.0005235947319306433\n",
            "Training:  Epoch No:  11 Iteration No:  139 \n",
            " Loss:  0.001433464465662837\n",
            "Training:  Epoch No:  11 Iteration No:  140 \n",
            " Loss:  0.0010899915359914303\n",
            "Training:  Epoch No:  11 Iteration No:  141 \n",
            " Loss:  0.002956278156489134\n",
            "Training:  Epoch No:  11 Iteration No:  142 \n",
            " Loss:  0.002178644062951207\n",
            "Training:  Epoch No:  11 Iteration No:  143 \n",
            " Loss:  0.0022451302502304316\n",
            "Training:  Epoch No:  11 Iteration No:  144 \n",
            " Loss:  0.0013594478368759155\n",
            "Training:  Epoch No:  11 Iteration No:  145 \n",
            " Loss:  0.011636955663561821\n",
            "Training:  Epoch No:  11 Iteration No:  146 \n",
            " Loss:  0.005832558497786522\n",
            "Training:  Epoch No:  11 Iteration No:  147 \n",
            " Loss:  0.00033945534960366786\n",
            "Training:  Epoch No:  11 Iteration No:  148 \n",
            " Loss:  0.014974384568631649\n",
            "Training:  Epoch No:  11 Iteration No:  149 \n",
            " Loss:  0.003728131065145135\n",
            "Training:  Epoch No:  11 Iteration No:  150 \n",
            " Loss:  0.0015933671966195107\n",
            "Training:  Epoch No:  11 Iteration No:  151 \n",
            " Loss:  0.006797446869313717\n",
            "Training:  Epoch No:  11 Iteration No:  152 \n",
            " Loss:  0.00026292470283806324\n",
            "Training:  Epoch No:  11 Iteration No:  153 \n",
            " Loss:  0.00012812491331715137\n",
            "Training:  Epoch No:  11 Iteration No:  154 \n",
            " Loss:  0.00013286438479553908\n",
            "Training:  Epoch No:  11 Iteration No:  155 \n",
            " Loss:  0.0004334295226726681\n",
            "Training:  Epoch No:  11 Iteration No:  156 \n",
            " Loss:  0.00022557764896191657\n",
            "Training:  Epoch No:  11 Iteration No:  157 \n",
            " Loss:  0.07232971489429474\n",
            "Training:  Epoch No:  11 Iteration No:  158 \n",
            " Loss:  0.01785966381430626\n",
            "Training:  Epoch No:  11 Iteration No:  159 \n",
            " Loss:  0.0025921366177499294\n",
            "Training:  Epoch No:  11 Iteration No:  160 \n",
            " Loss:  0.006107747089117765\n",
            "Training:  Epoch No:  11 Iteration No:  161 \n",
            " Loss:  0.007228106260299683\n",
            "Training:  Epoch No:  11 Iteration No:  162 \n",
            " Loss:  0.0019307262264192104\n",
            "Training:  Epoch No:  11 Iteration No:  163 \n",
            " Loss:  0.000506746000610292\n",
            "Training:  Epoch No:  11 Iteration No:  164 \n",
            " Loss:  0.00022726917813997716\n",
            "Training:  Epoch No:  11 Iteration No:  165 \n",
            " Loss:  6.461378507083282e-05\n",
            "Training:  Epoch No:  11 Iteration No:  166 \n",
            " Loss:  0.0003001897712238133\n",
            "Training:  Epoch No:  11 Iteration No:  167 \n",
            " Loss:  0.03704719990491867\n",
            "Training:  Epoch No:  11 Iteration No:  168 \n",
            " Loss:  0.00077895971480757\n",
            "Training:  Epoch No:  11 Iteration No:  169 \n",
            " Loss:  0.00017239325097762048\n",
            "Training:  Epoch No:  11 Iteration No:  170 \n",
            " Loss:  0.002455891575664282\n",
            "Training:  Epoch No:  11 Iteration No:  171 \n",
            " Loss:  0.00331347668543458\n",
            "Training:  Epoch No:  11 Iteration No:  172 \n",
            " Loss:  0.004020983818918467\n",
            "Training:  Epoch No:  11 Iteration No:  173 \n",
            " Loss:  0.09702066332101822\n",
            "Training:  Epoch No:  11 Iteration No:  174 \n",
            " Loss:  0.002904863329604268\n",
            "Training:  Epoch No:  11 Iteration No:  175 \n",
            " Loss:  0.006291523575782776\n",
            "Training:  Epoch No:  11 Iteration No:  176 \n",
            " Loss:  0.0009372870554216206\n",
            "Training:  Epoch No:  11 Iteration No:  177 \n",
            " Loss:  0.0001513475290266797\n",
            "Training:  Epoch No:  11 Iteration No:  178 \n",
            " Loss:  0.0006413735682144761\n",
            "Training:  Epoch No:  11 Iteration No:  179 \n",
            " Loss:  0.0005252760020084679\n",
            "Training:  Epoch No:  11 Iteration No:  180 \n",
            " Loss:  0.007543143816292286\n",
            "Training:  Epoch No:  11 Iteration No:  181 \n",
            " Loss:  0.020682550966739655\n",
            "Training:  Epoch No:  11 Iteration No:  182 \n",
            " Loss:  0.0007325068581849337\n",
            "Training:  Epoch No:  11 Iteration No:  183 \n",
            " Loss:  0.0022895149886608124\n",
            "Training:  Epoch No:  11 Iteration No:  184 \n",
            " Loss:  0.002922419458627701\n",
            "Training:  Epoch No:  11 Iteration No:  185 \n",
            " Loss:  0.00013991654850542545\n",
            "Training:  Epoch No:  11 Iteration No:  186 \n",
            " Loss:  0.00019715659436769783\n",
            "Training:  Epoch No:  11 Iteration No:  187 \n",
            " Loss:  0.01753610372543335\n",
            "Training:  Epoch No:  11 Iteration No:  188 \n",
            " Loss:  0.0003815832606051117\n",
            "Training:  Epoch No:  11 Iteration No:  189 \n",
            " Loss:  0.00015635063755325973\n",
            "Training:  Epoch No:  11 Iteration No:  190 \n",
            " Loss:  3.686490163090639e-05\n",
            "Training:  Epoch No:  11 Iteration No:  191 \n",
            " Loss:  0.0025458652526140213\n",
            "Training:  Epoch No:  11 Iteration No:  192 \n",
            " Loss:  0.0017839795909821987\n",
            "Training:  Epoch No:  11 Iteration No:  193 \n",
            " Loss:  0.0003147959359921515\n",
            "Training:  Epoch No:  11 Iteration No:  194 \n",
            " Loss:  0.0009700775262899697\n",
            "Training:  Epoch No:  11 Iteration No:  195 \n",
            " Loss:  0.0014834991889074445\n",
            "Training:  Epoch No:  11 Iteration No:  196 \n",
            " Loss:  0.001964074792340398\n",
            "Training:  Epoch No:  11 Iteration No:  197 \n",
            " Loss:  0.0019331321818754077\n",
            "Training:  Epoch No:  11 Iteration No:  198 \n",
            " Loss:  0.0002708295942284167\n",
            "Training:  Epoch No:  11 Iteration No:  199 \n",
            " Loss:  0.0002251410624012351\n",
            "Training:  Epoch No:  11 Iteration No:  200 \n",
            " Loss:  0.00016018078895285726\n",
            "Training:  Epoch No:  11 Iteration No:  201 \n",
            " Loss:  0.00013196827785577625\n",
            "Training:  Epoch No:  11 Iteration No:  202 \n",
            " Loss:  0.032652512192726135\n",
            "Training:  Epoch No:  11 Iteration No:  203 \n",
            " Loss:  0.00023990425688680261\n",
            "Training:  Epoch No:  11 Iteration No:  204 \n",
            " Loss:  0.0001267371408175677\n",
            "Training:  Epoch No:  11 Iteration No:  205 \n",
            " Loss:  0.0013534741010516882\n",
            "Training:  Epoch No:  11 Iteration No:  206 \n",
            " Loss:  0.005390005186200142\n",
            "Training:  Epoch No:  11 Iteration No:  207 \n",
            " Loss:  0.00014343565271701664\n",
            "Training:  Epoch No:  11 Iteration No:  208 \n",
            " Loss:  0.0001152584736701101\n",
            "Training:  Epoch No:  11 Iteration No:  209 \n",
            " Loss:  0.006545568350702524\n",
            "Training:  Epoch No:  11 Iteration No:  210 \n",
            " Loss:  0.00036382421967573464\n",
            "Training:  Epoch No:  11 Iteration No:  211 \n",
            " Loss:  0.027146704494953156\n",
            "Training:  Epoch No:  11 Iteration No:  212 \n",
            " Loss:  0.007840190082788467\n",
            "Training:  Epoch No:  11 Iteration No:  213 \n",
            " Loss:  0.0034932394046336412\n",
            "Training:  Epoch No:  11 Iteration No:  214 \n",
            " Loss:  0.00030266179237514734\n",
            "Training:  Epoch No:  11 Iteration No:  215 \n",
            " Loss:  0.00042917593964375556\n",
            "Training:  Epoch No:  11 Iteration No:  216 \n",
            " Loss:  0.001854726579040289\n",
            "Training:  Epoch No:  11 Iteration No:  217 \n",
            " Loss:  0.0004194217617623508\n",
            "Training:  Epoch No:  11 Iteration No:  218 \n",
            " Loss:  0.001649930840358138\n",
            "Training:  Epoch No:  11 Iteration No:  219 \n",
            " Loss:  0.0017882632091641426\n",
            "Training:  Epoch No:  11 Iteration No:  220 \n",
            " Loss:  0.0019852204713970423\n",
            "Training:  Epoch No:  11 Iteration No:  221 \n",
            " Loss:  0.0009387426543980837\n",
            "Training:  Epoch No:  11 Iteration No:  222 \n",
            " Loss:  0.0018867960898205638\n",
            "Training:  Epoch No:  11 Iteration No:  223 \n",
            " Loss:  0.0005579397547990084\n",
            "Training:  Epoch No:  11 Iteration No:  224 \n",
            " Loss:  0.00043810330680571496\n",
            "Training:  Epoch No:  11 Iteration No:  225 \n",
            " Loss:  0.00026492742472328246\n",
            "Training:  Epoch No:  11 Iteration No:  226 \n",
            " Loss:  0.0006322182598523796\n",
            "Training:  Epoch No:  11 Iteration No:  227 \n",
            " Loss:  0.000727280683349818\n",
            "Training:  Epoch No:  11 Iteration No:  228 \n",
            " Loss:  0.00911532249301672\n",
            "Training:  Epoch No:  11 Iteration No:  229 \n",
            " Loss:  0.0012035738909617066\n",
            "Training:  Epoch No:  11 Iteration No:  230 \n",
            " Loss:  0.005285615101456642\n",
            "Training:  Epoch No:  11 Iteration No:  231 \n",
            " Loss:  0.0005154083482921124\n",
            "Training:  Epoch No:  11 Iteration No:  232 \n",
            " Loss:  0.0003728141891770065\n",
            "Training:  Epoch No:  11 Iteration No:  233 \n",
            " Loss:  0.00013432468404062092\n",
            "Training:  Epoch No:  11 Iteration No:  234 \n",
            " Loss:  0.004562827758491039\n",
            "Training:  Epoch No:  11 Iteration No:  235 \n",
            " Loss:  0.0010422969935461879\n",
            "Training:  Epoch No:  11 Iteration No:  236 \n",
            " Loss:  0.004804798401892185\n",
            "Training:  Epoch No:  11 Iteration No:  237 \n",
            " Loss:  0.0005764251109212637\n",
            "Training:  Epoch No:  11 Iteration No:  238 \n",
            " Loss:  0.006464044097810984\n",
            "Training:  Epoch No:  11 Iteration No:  239 \n",
            " Loss:  0.00021095166448503733\n",
            "Training:  Epoch No:  11 Iteration No:  240 \n",
            " Loss:  0.00016722884902264923\n",
            "Training:  Epoch No:  11 Iteration No:  241 \n",
            " Loss:  0.000293718563625589\n",
            "Training:  Epoch No:  11 Iteration No:  242 \n",
            " Loss:  0.009896491654217243\n",
            "Training:  Epoch No:  11 Iteration No:  243 \n",
            " Loss:  0.0014950649347156286\n",
            "Training:  Epoch No:  11 Iteration No:  244 \n",
            " Loss:  0.0027151487302035093\n",
            "Training:  Epoch No:  11 Iteration No:  245 \n",
            " Loss:  0.000364674226148054\n",
            "Training:  Epoch No:  11 Iteration No:  246 \n",
            " Loss:  0.006601232569664717\n",
            "Training:  Epoch No:  11 Iteration No:  247 \n",
            " Loss:  0.0026813815347850323\n",
            "Training:  Epoch No:  11 Iteration No:  248 \n",
            " Loss:  0.0022898779716342688\n",
            "Training:  Epoch No:  11 Iteration No:  249 \n",
            " Loss:  0.0001973541220650077\n",
            "Training:  Epoch No:  11 Iteration No:  250 \n",
            " Loss:  0.00032343636848963797\n",
            "Training:  Epoch No:  11 Iteration No:  251 \n",
            " Loss:  0.0023513948544859886\n",
            "Training:  Epoch No:  11 Iteration No:  252 \n",
            " Loss:  0.0012601448688656092\n",
            "Training:  Epoch No:  11 Iteration No:  253 \n",
            " Loss:  0.00026211055228486657\n",
            "Training:  Epoch No:  11 Iteration No:  254 \n",
            " Loss:  0.0004617402155417949\n",
            "Training:  Epoch No:  11 Iteration No:  255 \n",
            " Loss:  0.008559790439903736\n",
            "Training:  Epoch No:  11 Iteration No:  256 \n",
            " Loss:  0.023023445159196854\n",
            "Training:  Epoch No:  11 Iteration No:  257 \n",
            " Loss:  0.0015157361049205065\n",
            "Training:  Epoch No:  11 Iteration No:  258 \n",
            " Loss:  0.00019059202168136835\n",
            "Training:  Epoch No:  11 Iteration No:  259 \n",
            " Loss:  0.0014784176601096988\n",
            "Training:  Epoch No:  11 Iteration No:  260 \n",
            " Loss:  0.0010817593429237604\n",
            "Training:  Epoch No:  11 Iteration No:  261 \n",
            " Loss:  0.0015082939062267542\n",
            "Training:  Epoch No:  11 Iteration No:  262 \n",
            " Loss:  0.0004753513785544783\n",
            "Training:  Epoch No:  11 Iteration No:  263 \n",
            " Loss:  0.004201652482151985\n",
            "Training:  Epoch No:  11 Iteration No:  264 \n",
            " Loss:  0.002462614793330431\n",
            "Training:  Epoch No:  11 Iteration No:  265 \n",
            " Loss:  0.0011982673313468695\n",
            "Training:  Epoch No:  11 Iteration No:  266 \n",
            " Loss:  0.00022840089513920248\n",
            "Training:  Epoch No:  11 Iteration No:  267 \n",
            " Loss:  0.005773333366960287\n",
            "Training:  Epoch No:  11 Iteration No:  268 \n",
            " Loss:  0.01164273452013731\n",
            "Training:  Epoch No:  11 Iteration No:  269 \n",
            " Loss:  0.0008668201044201851\n",
            "Training:  Epoch No:  11 Iteration No:  270 \n",
            " Loss:  0.0007030058186501265\n",
            "Training:  Epoch No:  11 Iteration No:  271 \n",
            " Loss:  0.0007970606675371528\n",
            "Training:  Epoch No:  11 Iteration No:  272 \n",
            " Loss:  0.00036132032983005047\n",
            "Training:  Epoch No:  11 Iteration No:  273 \n",
            " Loss:  0.0029942821711301804\n",
            "Training:  Epoch No:  11 Iteration No:  274 \n",
            " Loss:  0.034364279359579086\n",
            "Training:  Epoch No:  11 Iteration No:  275 \n",
            " Loss:  0.0001696049585007131\n",
            "Training:  Epoch No:  11 Iteration No:  276 \n",
            " Loss:  0.0013069389387965202\n",
            "Training:  Epoch No:  11 Iteration No:  277 \n",
            " Loss:  0.0003939768357668072\n",
            "Training:  Epoch No:  11 Iteration No:  278 \n",
            " Loss:  0.02093745395541191\n",
            "Training:  Epoch No:  11 Iteration No:  279 \n",
            " Loss:  0.0012916543055325747\n",
            "Training:  Epoch No:  11 Iteration No:  280 \n",
            " Loss:  3.8028807466616854e-05\n",
            "Training:  Epoch No:  11 Iteration No:  281 \n",
            " Loss:  0.008428406901657581\n",
            "Training:  Epoch No:  11 Iteration No:  282 \n",
            " Loss:  0.000476684799650684\n",
            "Training:  Epoch No:  11 Iteration No:  283 \n",
            " Loss:  0.004099630750715733\n",
            "Training:  Epoch No:  11 Iteration No:  284 \n",
            " Loss:  0.00030570602393709123\n",
            "Training:  Epoch No:  11 Iteration No:  285 \n",
            " Loss:  0.005003036465495825\n",
            "Training:  Epoch No:  11 Iteration No:  286 \n",
            " Loss:  0.00476954085752368\n",
            "Training:  Epoch No:  11 Iteration No:  287 \n",
            " Loss:  0.003021577140316367\n",
            "Training:  Epoch No:  11 Iteration No:  288 \n",
            " Loss:  0.024405743926763535\n",
            "Training:  Epoch No:  11 Iteration No:  289 \n",
            " Loss:  0.006339275743812323\n",
            "Training:  Epoch No:  11 Iteration No:  290 \n",
            " Loss:  0.0010299098212271929\n",
            "Training:  Epoch No:  11 Iteration No:  291 \n",
            " Loss:  0.0034470364917069674\n",
            "Training:  Epoch No:  11 Iteration No:  292 \n",
            " Loss:  0.004664753098040819\n",
            "Training:  Epoch No:  11 Iteration No:  293 \n",
            " Loss:  0.008278810419142246\n",
            "Training:  Epoch No:  11 Iteration No:  294 \n",
            " Loss:  0.0021171390544623137\n",
            "Training:  Epoch No:  11 Iteration No:  295 \n",
            " Loss:  0.0014566073659807444\n",
            "Training:  Epoch No:  11 Iteration No:  296 \n",
            " Loss:  0.0022990938741713762\n",
            "Training:  Epoch No:  11 Iteration No:  297 \n",
            " Loss:  0.00027666936512105167\n",
            "Training:  Epoch No:  11 Iteration No:  298 \n",
            " Loss:  0.02259761281311512\n",
            "Training:  Epoch No:  11 Iteration No:  299 \n",
            " Loss:  9.272841271013021e-05\n",
            "Training:  Epoch No:  11 Iteration No:  300 \n",
            " Loss:  0.00015222979709506035\n",
            "Training:  Epoch No:  11 Iteration No:  301 \n",
            " Loss:  0.0003090993850491941\n",
            "Training:  Epoch No:  11 Iteration No:  302 \n",
            " Loss:  0.0003307356091681868\n",
            "Training:  Epoch No:  11 Iteration No:  303 \n",
            " Loss:  3.677705899463035e-05\n",
            "Training:  Epoch No:  11 Iteration No:  304 \n",
            " Loss:  0.0008005398558452725\n",
            "Training:  Epoch No:  11 Iteration No:  305 \n",
            " Loss:  0.00011343634105287492\n",
            "Training:  Epoch No:  11 Iteration No:  306 \n",
            " Loss:  0.00031579536153003573\n",
            "Training:  Epoch No:  11 Iteration No:  307 \n",
            " Loss:  0.0005929392646066844\n",
            "Training:  Epoch No:  11 Iteration No:  308 \n",
            " Loss:  0.0007526589324697852\n",
            "Training:  Epoch No:  11 Iteration No:  309 \n",
            " Loss:  0.005245084874331951\n",
            "Training:  Epoch No:  11 Iteration No:  310 \n",
            " Loss:  0.0015063902828842402\n",
            "Training:  Epoch No:  11 Iteration No:  311 \n",
            " Loss:  0.004693367052823305\n",
            "Training:  Epoch No:  11 Iteration No:  312 \n",
            " Loss:  0.006161481607705355\n",
            "Training:  Epoch No:  11 Iteration No:  313 \n",
            " Loss:  0.0014068183954805136\n",
            "Training:  Epoch No:  11 Iteration No:  314 \n",
            " Loss:  0.00269722961820662\n",
            "Training:  Epoch No:  11 Iteration No:  315 \n",
            " Loss:  0.002546419622376561\n",
            "Training:  Epoch No:  11 Iteration No:  316 \n",
            " Loss:  0.004243627656251192\n",
            "Training:  Epoch No:  11 Iteration No:  317 \n",
            " Loss:  0.00033659933251328766\n",
            "Training:  Epoch No:  11 Iteration No:  318 \n",
            " Loss:  0.0004908873816020787\n",
            "Training:  Epoch No:  11 Iteration No:  319 \n",
            " Loss:  0.0005700462497770786\n",
            "Training:  Epoch No:  11 Iteration No:  320 \n",
            " Loss:  0.0003555582952685654\n",
            "Training:  Epoch No:  11 Iteration No:  321 \n",
            " Loss:  0.0037377034313976765\n",
            "Training:  Epoch No:  11 Iteration No:  322 \n",
            " Loss:  0.004927700851112604\n",
            "Training:  Epoch No:  11 Iteration No:  323 \n",
            " Loss:  0.0007225559675134718\n",
            "Training:  Epoch No:  11 Iteration No:  324 \n",
            " Loss:  0.001576180336996913\n",
            "Training:  Epoch No:  11 Iteration No:  325 \n",
            " Loss:  0.00034933528513647616\n",
            "Training:  Epoch No:  11 Iteration No:  326 \n",
            " Loss:  0.001387434545904398\n",
            "Training:  Epoch No:  11 Iteration No:  327 \n",
            " Loss:  0.0007332157692871988\n",
            "Training:  Epoch No:  11 Iteration No:  328 \n",
            " Loss:  8.728508692001924e-05\n",
            "Training:  Epoch No:  11 Iteration No:  329 \n",
            " Loss:  0.0007729445351287723\n",
            "Training:  Epoch No:  11 Iteration No:  330 \n",
            " Loss:  0.0026996282394975424\n",
            "Training:  Epoch No:  11 Iteration No:  331 \n",
            " Loss:  0.0009198681800626218\n",
            "Training:  Epoch No:  11 Iteration No:  332 \n",
            " Loss:  0.0004662134451791644\n",
            "Training:  Epoch No:  11 Iteration No:  333 \n",
            " Loss:  0.0030404082499444485\n",
            "Training:  Epoch No:  11 Iteration No:  334 \n",
            " Loss:  0.00014457930228672922\n",
            "Training:  Epoch No:  11 Iteration No:  335 \n",
            " Loss:  0.00027773610781878233\n",
            "Training:  Epoch No:  11 Iteration No:  336 \n",
            " Loss:  0.0054366388358175755\n",
            "Training:  Epoch No:  11 Iteration No:  337 \n",
            " Loss:  0.0010621079709380865\n",
            "Training:  Epoch No:  11 Iteration No:  338 \n",
            " Loss:  0.0003126135270576924\n",
            "Training:  Epoch No:  11 Iteration No:  339 \n",
            " Loss:  0.0008150440407916903\n",
            "Training:  Epoch No:  11 Iteration No:  340 \n",
            " Loss:  0.0022503500804305077\n",
            "Training:  Epoch No:  11 Iteration No:  341 \n",
            " Loss:  0.0024641675408929586\n",
            "Training:  Epoch No:  11 Iteration No:  342 \n",
            " Loss:  0.0021373911295086145\n",
            "Training:  Epoch No:  11 Iteration No:  343 \n",
            " Loss:  0.0013435563305392861\n",
            "Training:  Epoch No:  11 Iteration No:  344 \n",
            " Loss:  0.00147370551712811\n",
            "Training:  Epoch No:  11 Iteration No:  345 \n",
            " Loss:  0.0004325693880673498\n",
            "Training:  Epoch No:  11 Iteration No:  346 \n",
            " Loss:  0.00043901000753976405\n",
            "Training:  Epoch No:  11 Iteration No:  347 \n",
            " Loss:  0.008215047419071198\n",
            "Training:  Epoch No:  11 Iteration No:  348 \n",
            " Loss:  0.002259728731587529\n",
            "Training:  Epoch No:  11 Iteration No:  349 \n",
            " Loss:  0.0033173172269016504\n",
            "Training:  Epoch No:  11 Iteration No:  350 \n",
            " Loss:  0.00018593372078612447\n",
            "Training:  Epoch No:  11 Iteration No:  351 \n",
            " Loss:  0.00114334502723068\n",
            "Training:  Epoch No:  11 Iteration No:  352 \n",
            " Loss:  0.0003296207869425416\n",
            "Training:  Epoch No:  11 Iteration No:  353 \n",
            " Loss:  0.00034803838934749365\n",
            "Training:  Epoch No:  11 Iteration No:  354 \n",
            " Loss:  0.0013708038022741675\n",
            "Training:  Epoch No:  11 Iteration No:  355 \n",
            " Loss:  0.0031363817397505045\n",
            "Training:  Epoch No:  11 Iteration No:  356 \n",
            " Loss:  0.0036807514261454344\n",
            "Training:  Epoch No:  11 Iteration No:  357 \n",
            " Loss:  0.0006367700407281518\n",
            "Training:  Epoch No:  11 Iteration No:  358 \n",
            " Loss:  0.04603647068142891\n",
            "Training:  Epoch No:  11 Iteration No:  359 \n",
            " Loss:  0.062311477959156036\n",
            "Training:  Epoch No:  11 Iteration No:  360 \n",
            " Loss:  0.004248127341270447\n",
            "Training:  Epoch No:  11 Iteration No:  361 \n",
            " Loss:  0.0017020704690366983\n",
            "Training:  Epoch No:  11 Iteration No:  362 \n",
            " Loss:  0.032123565673828125\n",
            "Training:  Epoch No:  11 Iteration No:  363 \n",
            " Loss:  0.001247699256055057\n",
            "Training:  Epoch No:  11 Iteration No:  364 \n",
            " Loss:  0.030249781906604767\n",
            "Training:  Epoch No:  11 Iteration No:  365 \n",
            " Loss:  0.001865582074970007\n",
            "Training:  Epoch No:  11 Iteration No:  366 \n",
            " Loss:  0.017880879342556\n",
            "Training:  Epoch No:  11 Iteration No:  367 \n",
            " Loss:  0.0007131088059395552\n",
            "Training:  Epoch No:  11 Iteration No:  368 \n",
            " Loss:  0.0030932952649891376\n",
            "Training:  Epoch No:  11 Iteration No:  369 \n",
            " Loss:  0.005113782361149788\n",
            "Training:  Epoch No:  11 Iteration No:  370 \n",
            " Loss:  0.0008902545087039471\n",
            "Training:  Epoch No:  11 Iteration No:  371 \n",
            " Loss:  0.0021326832938939333\n",
            "Training:  Epoch No:  11 Iteration No:  372 \n",
            " Loss:  0.00045628449879586697\n",
            "Training:  Epoch No:  11 Iteration No:  373 \n",
            " Loss:  0.0003527485823724419\n",
            "Training:  Epoch No:  11 Iteration No:  374 \n",
            " Loss:  0.002935919212177396\n",
            "Training:  Epoch No:  11 Iteration No:  375 \n",
            " Loss:  0.0005154777900315821\n",
            "Training:  Epoch No:  11 Iteration No:  376 \n",
            " Loss:  0.0005481652915477753\n",
            "Training:  Epoch No:  11 Iteration No:  377 \n",
            " Loss:  0.000513328006491065\n",
            "Training:  Epoch No:  11 Iteration No:  378 \n",
            " Loss:  0.003223386825993657\n",
            "Training:  Epoch No:  11 Iteration No:  379 \n",
            " Loss:  0.007479981519281864\n",
            "Training:  Epoch No:  11 Iteration No:  380 \n",
            " Loss:  0.0010684552835300565\n",
            "Training:  Epoch No:  11 Iteration No:  381 \n",
            " Loss:  0.00346051761880517\n",
            "Training:  Epoch No:  11 Iteration No:  382 \n",
            " Loss:  0.00020172596850898117\n",
            "Training:  Epoch No:  11 Iteration No:  383 \n",
            " Loss:  0.00021269291755743325\n",
            "Training:  Epoch No:  11 Iteration No:  384 \n",
            " Loss:  0.015230082906782627\n",
            "Training:  Epoch No:  11 Iteration No:  385 \n",
            " Loss:  0.000587725720833987\n",
            "Training:  Epoch No:  11 Iteration No:  386 \n",
            " Loss:  0.001979760592803359\n",
            "Training:  Epoch No:  11 Iteration No:  387 \n",
            " Loss:  0.0033203954808413982\n",
            "Training:  Epoch No:  11 Iteration No:  388 \n",
            " Loss:  0.003592811059206724\n",
            "Training:  Epoch No:  11 Iteration No:  389 \n",
            " Loss:  0.0028002860490232706\n",
            "Training:  Epoch No:  11 Iteration No:  390 \n",
            " Loss:  0.00042417566874064505\n",
            "Training:  Epoch No:  11 Iteration No:  391 \n",
            " Loss:  0.0014865781413391232\n",
            "Training:  Epoch No:  11 Iteration No:  392 \n",
            " Loss:  0.0008544203010387719\n",
            "Training:  Epoch No:  11 Iteration No:  393 \n",
            " Loss:  0.00037814921233803034\n",
            "Training:  Epoch No:  11 Iteration No:  394 \n",
            " Loss:  0.0171945970505476\n",
            "Training:  Epoch No:  11 Iteration No:  395 \n",
            " Loss:  0.0009269281872548163\n",
            "Training:  Epoch No:  11 Iteration No:  396 \n",
            " Loss:  5.594975846179295e-06\n",
            "Training:  Epoch No:  11 Iteration No:  397 \n",
            " Loss:  0.00011746202653739601\n",
            "Training:  Epoch No:  11 Iteration No:  398 \n",
            " Loss:  0.00020902861433569342\n",
            "Training:  Epoch No:  11 Iteration No:  399 \n",
            " Loss:  2.2839989469503053e-05\n",
            "Training:  Epoch No:  11 Iteration No:  400 \n",
            " Loss:  0.0008217403665184975\n",
            "Training:  Epoch No:  11 Iteration No:  401 \n",
            " Loss:  0.0004784409829881042\n",
            "Training:  Epoch No:  11 Iteration No:  402 \n",
            " Loss:  0.020669318735599518\n",
            "Training:  Epoch No:  11 Iteration No:  403 \n",
            " Loss:  0.04783223196864128\n",
            "Training:  Epoch No:  11 Iteration No:  404 \n",
            " Loss:  0.00569119630381465\n",
            "Training:  Epoch No:  11 Iteration No:  405 \n",
            " Loss:  0.0010673311771824956\n",
            "Training:  Epoch No:  11 Iteration No:  406 \n",
            " Loss:  0.020384889096021652\n",
            "Training:  Epoch No:  11 Iteration No:  407 \n",
            " Loss:  0.01403370127081871\n",
            "Training:  Epoch No:  11 Iteration No:  408 \n",
            " Loss:  0.0007299659191630781\n",
            "Training:  Epoch No:  11 Iteration No:  409 \n",
            " Loss:  0.0032551924232393503\n",
            "Training:  Epoch No:  11 Iteration No:  410 \n",
            " Loss:  0.006070960778743029\n",
            "Training:  Epoch No:  11 Iteration No:  411 \n",
            " Loss:  0.001471025636419654\n",
            "Training:  Epoch No:  11 Iteration No:  412 \n",
            " Loss:  0.0007465129601769149\n",
            "Training:  Epoch No:  11 Iteration No:  413 \n",
            " Loss:  0.0009245519759133458\n",
            "Training:  Epoch No:  11 Iteration No:  414 \n",
            " Loss:  0.00022172753233462572\n",
            "Training:  Epoch No:  11 Iteration No:  415 \n",
            " Loss:  0.0023977849632501602\n",
            "Training:  Epoch No:  11 Iteration No:  416 \n",
            " Loss:  0.0005840524681843817\n",
            "Training:  Epoch No:  11 Iteration No:  417 \n",
            " Loss:  0.01146151963621378\n",
            "Training:  Epoch No:  11 Iteration No:  418 \n",
            " Loss:  0.007332629058510065\n",
            "Training:  Epoch No:  11 Iteration No:  419 \n",
            " Loss:  0.003922711592167616\n",
            "Training:  Epoch No:  11 Iteration No:  420 \n",
            " Loss:  0.00013775176194030792\n",
            "Training:  Epoch No:  11 Iteration No:  421 \n",
            " Loss:  0.0010003071511164308\n",
            "Training:  Epoch No:  11 Iteration No:  422 \n",
            " Loss:  0.0004618661478161812\n",
            "Training:  Epoch No:  11 Iteration No:  423 \n",
            " Loss:  0.0031250924803316593\n",
            "Training:  Epoch No:  11 Iteration No:  424 \n",
            " Loss:  0.0001714832615107298\n",
            "Training:  Epoch No:  11 Iteration No:  425 \n",
            " Loss:  0.0008515316876582801\n",
            "Training:  Epoch No:  11 Iteration No:  426 \n",
            " Loss:  0.014385515823960304\n",
            "Training:  Epoch No:  11 Iteration No:  427 \n",
            " Loss:  0.0033393625635653734\n",
            "Training:  Epoch No:  11 Iteration No:  428 \n",
            " Loss:  0.0001963707763934508\n",
            "Training:  Epoch No:  11 Iteration No:  429 \n",
            " Loss:  0.00025368243223056197\n",
            "Training:  Epoch No:  11 Iteration No:  430 \n",
            " Loss:  0.0001407045201631263\n",
            "Training:  Epoch No:  11 Iteration No:  431 \n",
            " Loss:  0.011388655751943588\n",
            "Training:  Epoch No:  11 Iteration No:  432 \n",
            " Loss:  0.0023444804828613997\n",
            "Training:  Epoch No:  11 Iteration No:  433 \n",
            " Loss:  0.00023406176478601992\n",
            "Training:  Epoch No:  11 Iteration No:  434 \n",
            " Loss:  0.0015189204132184386\n",
            "Training:  Epoch No:  11 Iteration No:  435 \n",
            " Loss:  0.00024333802866749465\n",
            "Training:  Epoch No:  11 Iteration No:  436 \n",
            " Loss:  0.0003145953523926437\n",
            "Training:  Epoch No:  11 Iteration No:  437 \n",
            " Loss:  0.003064035205170512\n",
            "Training:  Epoch No:  11 Iteration No:  438 \n",
            " Loss:  0.0009075274574570358\n",
            "Training:  Epoch No:  11 Iteration No:  439 \n",
            " Loss:  0.0002272164128953591\n",
            "Training:  Epoch No:  11 Iteration No:  440 \n",
            " Loss:  0.0012896000407636166\n",
            "Training:  Epoch No:  11 Iteration No:  441 \n",
            " Loss:  0.0019039040198549628\n",
            "Training:  Epoch No:  11 Iteration No:  442 \n",
            " Loss:  0.004162529017776251\n",
            "Training:  Epoch No:  11 Iteration No:  443 \n",
            " Loss:  0.0012045687763020396\n",
            "Training:  Epoch No:  11 Iteration No:  444 \n",
            " Loss:  0.004708725493401289\n",
            "Training:  Epoch No:  11 Iteration No:  445 \n",
            " Loss:  0.0016636665677651763\n",
            "Training:  Epoch No:  11 Iteration No:  446 \n",
            " Loss:  0.00037140867789275944\n",
            "Training:  Epoch No:  11 Iteration No:  447 \n",
            " Loss:  7.534103588113794e-06\n",
            "Training:  Epoch No:  11 Iteration No:  448 \n",
            " Loss:  0.00041075487388297915\n",
            "Training:  Epoch No:  11 Iteration No:  449 \n",
            " Loss:  0.0008852892788127065\n",
            "Training:  Epoch No:  11 Iteration No:  450 \n",
            " Loss:  0.03450576588511467\n",
            "Training:  Epoch No:  11 Iteration No:  451 \n",
            " Loss:  0.0003654889005701989\n",
            "Training:  Epoch No:  11 Iteration No:  452 \n",
            " Loss:  0.000431141525041312\n",
            "Training:  Epoch No:  11 Iteration No:  453 \n",
            " Loss:  0.001448123948648572\n",
            "Training:  Epoch No:  11 Iteration No:  454 \n",
            " Loss:  0.06759013235569\n",
            "Training:  Epoch No:  11 Iteration No:  455 \n",
            " Loss:  0.0033271529246121645\n",
            "Training:  Epoch No:  11 Iteration No:  456 \n",
            " Loss:  0.0006138015887700021\n",
            "Training:  Epoch No:  11 Iteration No:  457 \n",
            " Loss:  0.0006456970004364848\n",
            "Training:  Epoch No:  11 Iteration No:  458 \n",
            " Loss:  0.0015190880512818694\n",
            "Training:  Epoch No:  11 Iteration No:  459 \n",
            " Loss:  0.00023388078261632472\n",
            "Training:  Epoch No:  11 Iteration No:  460 \n",
            " Loss:  0.06778758764266968\n",
            "Training:  Epoch No:  11 Iteration No:  461 \n",
            " Loss:  0.0019842779729515314\n",
            "Training:  Epoch No:  11 Iteration No:  462 \n",
            " Loss:  0.011656973510980606\n",
            "Training:  Epoch No:  11 Iteration No:  463 \n",
            " Loss:  0.0006900802254676819\n",
            "Training:  Epoch No:  11 Iteration No:  464 \n",
            " Loss:  0.011519020423293114\n",
            "Training:  Epoch No:  11 Iteration No:  465 \n",
            " Loss:  0.00011037605872843415\n",
            "Training:  Epoch No:  11 Iteration No:  466 \n",
            " Loss:  0.000152668115333654\n",
            "Training:  Epoch No:  11 Iteration No:  467 \n",
            " Loss:  0.002246400574222207\n",
            "Training:  Epoch No:  11 Iteration No:  468 \n",
            " Loss:  0.0004517374327406287\n",
            "Training:  Epoch No:  11 Iteration No:  469 \n",
            " Loss:  0.0017475439235568047\n",
            "Training:  Epoch No:  11 Iteration No:  470 \n",
            " Loss:  0.0016233830247074366\n",
            "Training:  Epoch No:  11 Iteration No:  471 \n",
            " Loss:  0.0018768500303849578\n",
            "Training:  Epoch No:  11 Iteration No:  472 \n",
            " Loss:  0.0010399444727227092\n",
            "Training:  Epoch No:  11 Iteration No:  473 \n",
            " Loss:  0.00033372361212968826\n",
            "Training:  Epoch No:  11 Iteration No:  474 \n",
            " Loss:  0.0014507690211758018\n",
            "Training:  Epoch No:  11 Iteration No:  475 \n",
            " Loss:  0.0006448233616538346\n",
            "Training:  Epoch No:  11 Iteration No:  476 \n",
            " Loss:  0.009925518184900284\n",
            "Training:  Epoch No:  11 Iteration No:  477 \n",
            " Loss:  0.00041957179200835526\n",
            "Training:  Epoch No:  11 Iteration No:  478 \n",
            " Loss:  0.0018939005676656961\n",
            "Training:  Epoch No:  11 Iteration No:  479 \n",
            " Loss:  0.004867319483309984\n",
            "Training:  Epoch No:  11 Iteration No:  480 \n",
            " Loss:  0.0014632913516834378\n",
            "Training:  Epoch No:  11 Iteration No:  481 \n",
            " Loss:  0.001509400550276041\n",
            "Training:  Epoch No:  11 Iteration No:  482 \n",
            " Loss:  0.0010059191845357418\n",
            "Training:  Epoch No:  11 Iteration No:  483 \n",
            " Loss:  0.000139500800287351\n",
            "Training:  Epoch No:  11 Iteration No:  484 \n",
            " Loss:  0.0003366006421856582\n",
            "Training:  Epoch No:  11 Iteration No:  485 \n",
            " Loss:  0.010133284144103527\n",
            "Training:  Epoch No:  11 Iteration No:  486 \n",
            " Loss:  0.0009700044174678624\n",
            "Training:  Epoch No:  11 Iteration No:  487 \n",
            " Loss:  0.04818492755293846\n",
            "Training:  Epoch No:  11 Iteration No:  488 \n",
            " Loss:  0.020685479044914246\n",
            "Training:  Epoch No:  11 Iteration No:  489 \n",
            " Loss:  0.0011354470625519753\n",
            "Training:  Epoch No:  11 Iteration No:  490 \n",
            " Loss:  0.0010144555708393455\n",
            "Training:  Epoch No:  11 Iteration No:  491 \n",
            " Loss:  0.00015181412163656205\n",
            "Training:  Epoch No:  11 Iteration No:  492 \n",
            " Loss:  0.00500451261177659\n",
            "Training:  Epoch No:  11 Iteration No:  493 \n",
            " Loss:  0.0001602094853296876\n",
            "Training:  Epoch No:  11 Iteration No:  494 \n",
            " Loss:  0.008773984387516975\n",
            "Training:  Epoch No:  11 Iteration No:  495 \n",
            " Loss:  0.006755143869668245\n",
            "Training:  Epoch No:  11 Iteration No:  496 \n",
            " Loss:  0.0012681082589551806\n",
            "Training:  Epoch No:  11 Iteration No:  497 \n",
            " Loss:  0.0017910863971337676\n",
            "Training:  Epoch No:  11 Iteration No:  498 \n",
            " Loss:  0.001064280979335308\n",
            "Training:  Epoch No:  11 Iteration No:  499 \n",
            " Loss:  0.0027216647285968065\n",
            "Training:  Epoch No:  11 Iteration No:  500 \n",
            " Loss:  0.009276429191231728\n",
            "Training:  Epoch No:  11 Iteration No:  501 \n",
            " Loss:  0.0006624460220336914\n",
            "Training:  Epoch No:  11 Iteration No:  502 \n",
            " Loss:  0.0015298679936677217\n",
            "Training:  Epoch No:  11 Iteration No:  503 \n",
            " Loss:  0.0012919961009174585\n",
            "Training:  Epoch No:  11 Iteration No:  504 \n",
            " Loss:  0.0009647727711126208\n",
            "Training:  Epoch No:  11 Iteration No:  505 \n",
            " Loss:  0.00013810199743602425\n",
            "Training:  Epoch No:  11 Iteration No:  506 \n",
            " Loss:  0.0018465719185769558\n",
            "Training:  Epoch No:  11 Iteration No:  507 \n",
            " Loss:  0.010072536766529083\n",
            "Training:  Epoch No:  11 Iteration No:  508 \n",
            " Loss:  0.008222952485084534\n",
            "Training:  Epoch No:  11 Iteration No:  509 \n",
            " Loss:  9.535887511447072e-05\n",
            "Training:  Epoch No:  11 Iteration No:  510 \n",
            " Loss:  0.006076707039028406\n",
            "Training:  Epoch No:  11 Iteration No:  511 \n",
            " Loss:  0.0015808246098458767\n",
            "Training:  Epoch No:  11 Iteration No:  512 \n",
            " Loss:  0.000596372876316309\n",
            "Training:  Epoch No:  11 Iteration No:  513 \n",
            " Loss:  0.0004931372823193669\n",
            "Training:  Epoch No:  11 Iteration No:  514 \n",
            " Loss:  0.016129733994603157\n",
            "Training:  Epoch No:  11 Iteration No:  515 \n",
            " Loss:  3.813082730630413e-05\n",
            "Training:  Epoch No:  11 Iteration No:  516 \n",
            " Loss:  0.0014651608653366566\n",
            "Training:  Epoch No:  11 Iteration No:  517 \n",
            " Loss:  0.0006647652480751276\n",
            "Training:  Epoch No:  11 Iteration No:  518 \n",
            " Loss:  0.0013926245737820864\n",
            "Training:  Epoch No:  11 Iteration No:  519 \n",
            " Loss:  0.002187898615375161\n",
            "Training:  Epoch No:  11 Iteration No:  520 \n",
            " Loss:  0.0006459674914367497\n",
            "Training:  Epoch No:  11 Iteration No:  521 \n",
            " Loss:  0.007303342688828707\n",
            "Training:  Epoch No:  11 Iteration No:  522 \n",
            " Loss:  0.00036091540823690593\n",
            "Training:  Epoch No:  11 Iteration No:  523 \n",
            " Loss:  0.0003951239341404289\n",
            "Training:  Epoch No:  11 Iteration No:  524 \n",
            " Loss:  0.0001419589971192181\n",
            "Training:  Epoch No:  11 Iteration No:  525 \n",
            " Loss:  0.00019383610924705863\n",
            "Training:  Epoch No:  11 Iteration No:  526 \n",
            " Loss:  0.0005053426721133292\n",
            "Training:  Epoch No:  11 Iteration No:  527 \n",
            " Loss:  0.0018738179933279753\n",
            "Training:  Epoch No:  11 Iteration No:  528 \n",
            " Loss:  0.0052100373432040215\n",
            "Training:  Epoch No:  11 Iteration No:  529 \n",
            " Loss:  0.001590837724506855\n",
            "Training:  Epoch No:  11 Iteration No:  530 \n",
            " Loss:  0.0041952296160161495\n",
            "Training:  Epoch No:  11 Iteration No:  531 \n",
            " Loss:  0.00024508326896466315\n",
            "Training:  Epoch No:  11 Iteration No:  532 \n",
            " Loss:  8.909578173188493e-05\n",
            "Training:  Epoch No:  11 Iteration No:  533 \n",
            " Loss:  0.0007800046005286276\n",
            "Training:  Epoch No:  11 Iteration No:  534 \n",
            " Loss:  0.0028827697969973087\n",
            "Training:  Epoch No:  11 Iteration No:  535 \n",
            " Loss:  0.004694385454058647\n",
            "Training:  Epoch No:  11 Iteration No:  536 \n",
            " Loss:  0.0018147224327549338\n",
            "Training:  Epoch No:  11 Iteration No:  537 \n",
            " Loss:  0.000977582880295813\n",
            "Training:  Epoch No:  11 Iteration No:  538 \n",
            " Loss:  0.006301433313637972\n",
            "Training:  Epoch No:  11 Iteration No:  539 \n",
            " Loss:  0.00024814289645291865\n",
            "Training:  Epoch No:  11 Iteration No:  540 \n",
            " Loss:  0.0007923811790533364\n",
            "Training:  Epoch No:  11 Iteration No:  541 \n",
            " Loss:  0.0008246673969551921\n",
            "Training:  Epoch No:  11 Iteration No:  542 \n",
            " Loss:  0.006056434009224176\n",
            "Training:  Epoch No:  11 Iteration No:  543 \n",
            " Loss:  0.00044172717025503516\n",
            "Training:  Epoch No:  11 Iteration No:  544 \n",
            " Loss:  0.0006105966167524457\n",
            "Training:  Epoch No:  11 Iteration No:  545 \n",
            " Loss:  0.006746870931237936\n",
            "Training:  Epoch No:  11 Iteration No:  546 \n",
            " Loss:  0.003212698269635439\n",
            "Training:  Epoch No:  11 Iteration No:  547 \n",
            " Loss:  0.0017374904127791524\n",
            "Training:  Epoch No:  11 Iteration No:  548 \n",
            " Loss:  0.0001463043299736455\n",
            "Training:  Epoch No:  11 Iteration No:  549 \n",
            " Loss:  0.0032788983080536127\n",
            "Training:  Epoch No:  11 Iteration No:  550 \n",
            " Loss:  0.00754122668877244\n",
            "Training:  Epoch No:  11 Iteration No:  551 \n",
            " Loss:  0.00023179092386271805\n",
            "Training:  Epoch No:  11 Iteration No:  552 \n",
            " Loss:  0.0019216551445424557\n",
            "Training:  Epoch No:  11 Iteration No:  553 \n",
            " Loss:  0.0002307388640474528\n",
            "Training:  Epoch No:  11 Iteration No:  554 \n",
            " Loss:  0.0001452302240068093\n",
            "Training:  Epoch No:  11 Iteration No:  555 \n",
            " Loss:  0.00022308659390546381\n",
            "Training:  Epoch No:  11 Iteration No:  556 \n",
            " Loss:  0.002158575924113393\n",
            "Training:  Epoch No:  11 Iteration No:  557 \n",
            " Loss:  0.004766420461237431\n",
            "Training:  Epoch No:  11 Iteration No:  558 \n",
            " Loss:  0.0011527520837262273\n",
            "Training:  Epoch No:  11 Iteration No:  559 \n",
            " Loss:  0.006352548953145742\n",
            "Training:  Epoch No:  11 Iteration No:  560 \n",
            " Loss:  0.000328402646118775\n",
            "Training:  Epoch No:  11 Iteration No:  561 \n",
            " Loss:  5.030195097788237e-05\n",
            "Training:  Epoch No:  11 Iteration No:  562 \n",
            " Loss:  6.515945278806612e-05\n",
            "Training:  Epoch No:  11 Iteration No:  563 \n",
            " Loss:  0.004796586465090513\n",
            "Training:  Epoch No:  11 Iteration No:  564 \n",
            " Loss:  0.004087542183697224\n",
            "Training:  Epoch No:  11 Iteration No:  565 \n",
            " Loss:  0.00020448093710001558\n",
            "Training:  Epoch No:  11 Iteration No:  566 \n",
            " Loss:  0.0038672201335430145\n",
            "Training:  Epoch No:  11 Iteration No:  567 \n",
            " Loss:  0.03460158407688141\n",
            "Training:  Epoch No:  11 Iteration No:  568 \n",
            " Loss:  0.0019378091674298048\n",
            "Training:  Epoch No:  11 Iteration No:  569 \n",
            " Loss:  0.0006956396391615272\n",
            "Training:  Epoch No:  11 Iteration No:  570 \n",
            " Loss:  0.0020038397051393986\n",
            "Training:  Epoch No:  11 Iteration No:  571 \n",
            " Loss:  0.0018481825245544314\n",
            "Training:  Epoch No:  11 Iteration No:  572 \n",
            " Loss:  0.00019279874686617404\n",
            "Training:  Epoch No:  11 Iteration No:  573 \n",
            " Loss:  0.0011328475084155798\n",
            "Training:  Epoch No:  11 Iteration No:  574 \n",
            " Loss:  0.0010265421587973833\n",
            "Training:  Epoch No:  11 Iteration No:  575 \n",
            " Loss:  0.0002513225772418082\n",
            "Training:  Epoch No:  11 Iteration No:  576 \n",
            " Loss:  0.026417812332510948\n",
            "Training:  Epoch No:  11 Iteration No:  577 \n",
            " Loss:  0.006941564381122589\n",
            "Training:  Epoch No:  11 Iteration No:  578 \n",
            " Loss:  0.0008236938156187534\n",
            "Training:  Epoch No:  11 Iteration No:  579 \n",
            " Loss:  0.002967615146189928\n",
            "Training:  Epoch No:  11 Iteration No:  580 \n",
            " Loss:  0.001327191828750074\n",
            "Training:  Epoch No:  11 Iteration No:  581 \n",
            " Loss:  9.62933772825636e-05\n",
            "Training:  Epoch No:  11 Iteration No:  582 \n",
            " Loss:  0.0009063673787750304\n",
            "Training:  Epoch No:  11 Iteration No:  583 \n",
            " Loss:  0.001381746376864612\n",
            "Training:  Epoch No:  11 Iteration No:  584 \n",
            " Loss:  0.00043961999472230673\n",
            "Training:  Epoch No:  11 Iteration No:  585 \n",
            " Loss:  0.002947509055957198\n",
            "Training:  Epoch No:  11 Iteration No:  586 \n",
            " Loss:  0.0008963536820374429\n",
            "Training:  Epoch No:  11 Iteration No:  587 \n",
            " Loss:  0.001995844766497612\n",
            "Training:  Epoch No:  11 Iteration No:  588 \n",
            " Loss:  0.0019130370346829295\n",
            "Training:  Epoch No:  11 Iteration No:  589 \n",
            " Loss:  0.0007999037625268102\n",
            "Training:  Epoch No:  11 Iteration No:  590 \n",
            " Loss:  0.00044501046068035066\n",
            "Training:  Epoch No:  11 Iteration No:  591 \n",
            " Loss:  0.0001549696025904268\n",
            "Training:  Epoch No:  11 Iteration No:  592 \n",
            " Loss:  0.00021262402879074216\n",
            "Training:  Epoch No:  11 Iteration No:  593 \n",
            " Loss:  0.00024048000341281295\n",
            "Training:  Epoch No:  11 Iteration No:  594 \n",
            " Loss:  0.0004447572573553771\n",
            "Training:  Epoch No:  11 Iteration No:  595 \n",
            " Loss:  0.00010122134699486196\n",
            "Training:  Epoch No:  11 Iteration No:  596 \n",
            " Loss:  0.0018643393414095044\n",
            "Training:  Epoch No:  11 Iteration No:  597 \n",
            " Loss:  0.00034272592165507376\n",
            "Training:  Epoch No:  11 Iteration No:  598 \n",
            " Loss:  0.0005699163302779198\n",
            "Training:  Epoch No:  11 Iteration No:  599 \n",
            " Loss:  0.00038825906813144684\n",
            "Training:  Epoch No:  11 Iteration No:  600 \n",
            " Loss:  0.0024535900447517633\n",
            "Training:  Epoch No:  11 Iteration No:  601 \n",
            " Loss:  0.0010677038226276636\n",
            "Training:  Epoch No:  11 Iteration No:  602 \n",
            " Loss:  0.0010003447532653809\n",
            "Training:  Epoch No:  11 Iteration No:  603 \n",
            " Loss:  0.0005774837918579578\n",
            "Training:  Epoch No:  11 Iteration No:  604 \n",
            " Loss:  0.00013502282672561705\n",
            "Training:  Epoch No:  11 Iteration No:  605 \n",
            " Loss:  4.786833596881479e-05\n",
            "Training:  Epoch No:  11 Iteration No:  606 \n",
            " Loss:  0.0009417384280823171\n",
            "Training:  Epoch No:  11 Iteration No:  607 \n",
            " Loss:  0.0005123461596667767\n",
            "Training:  Epoch No:  11 Iteration No:  608 \n",
            " Loss:  0.00703146867454052\n",
            "Training:  Epoch No:  11 Iteration No:  609 \n",
            " Loss:  0.00027511961525306106\n",
            "Training:  Epoch No:  11 Iteration No:  610 \n",
            " Loss:  0.001996682258322835\n",
            "Training:  Epoch No:  11 Iteration No:  611 \n",
            " Loss:  0.00034683389822021127\n",
            "Training:  Epoch No:  11 Iteration No:  612 \n",
            " Loss:  0.0009609924163669348\n",
            "Training:  Epoch No:  11 Iteration No:  613 \n",
            " Loss:  0.0006330552278086543\n",
            "Training:  Epoch No:  11 Iteration No:  614 \n",
            " Loss:  4.6054661652306095e-05\n",
            "Training:  Epoch No:  11 Iteration No:  615 \n",
            " Loss:  0.00015988497762009501\n",
            "Training:  Epoch No:  11 Iteration No:  616 \n",
            " Loss:  0.00514465244486928\n",
            "Training:  Epoch No:  11 Iteration No:  617 \n",
            " Loss:  2.553575723140966e-05\n",
            "Training:  Epoch No:  11 Iteration No:  618 \n",
            " Loss:  0.00017706782091408968\n",
            "Training:  Epoch No:  11 Iteration No:  619 \n",
            " Loss:  0.005304501857608557\n",
            "Training:  Epoch No:  11 Iteration No:  620 \n",
            " Loss:  0.00047807919327169657\n",
            "Training:  Epoch No:  11 Iteration No:  621 \n",
            " Loss:  0.0023505946155637503\n",
            "Training:  Epoch No:  11 Iteration No:  622 \n",
            " Loss:  0.0003469160874374211\n",
            "Training:  Epoch No:  11 Iteration No:  623 \n",
            " Loss:  0.0012009850470349193\n",
            "Training:  Epoch No:  11 Iteration No:  624 \n",
            " Loss:  0.002625127788633108\n",
            "Training:  Epoch No:  11 Iteration No:  625 \n",
            " Loss:  0.006472080480307341\n",
            "Training:  Epoch No:  11 Iteration No:  626 \n",
            " Loss:  0.01738160476088524\n",
            "Training:  Epoch No:  11 Iteration No:  627 \n",
            " Loss:  1.7239364751731046e-05\n",
            "Training:  Epoch No:  11 Iteration No:  628 \n",
            " Loss:  0.0009838651167228818\n",
            "Training:  Epoch No:  11 Iteration No:  629 \n",
            " Loss:  0.0014612771337851882\n",
            "Training:  Epoch No:  11 Iteration No:  630 \n",
            " Loss:  0.0003852293884847313\n",
            "Training:  Epoch No:  11 Iteration No:  631 \n",
            " Loss:  0.0002604922337923199\n",
            "Training:  Epoch No:  11 Iteration No:  632 \n",
            " Loss:  0.0003695051127579063\n",
            "Training:  Epoch No:  11 Iteration No:  633 \n",
            " Loss:  0.006568532437086105\n",
            "Training:  Epoch No:  11 Iteration No:  634 \n",
            " Loss:  0.0030206625815480947\n",
            "Training:  Epoch No:  11 Iteration No:  635 \n",
            " Loss:  0.0028635398484766483\n",
            "Training:  Epoch No:  11 Iteration No:  636 \n",
            " Loss:  6.0855458286823705e-05\n",
            "Training:  Epoch No:  11 Iteration No:  637 \n",
            " Loss:  0.001392766018398106\n",
            "Training:  Epoch No:  11 Iteration No:  638 \n",
            " Loss:  4.628192982636392e-05\n",
            "Training:  Epoch No:  11 Iteration No:  639 \n",
            " Loss:  0.010281389579176903\n",
            "Training:  Epoch No:  11 Iteration No:  640 \n",
            " Loss:  0.00028852015384472907\n",
            "Training:  Epoch No:  11 Iteration No:  641 \n",
            " Loss:  0.0017588903428986669\n",
            "Training:  Epoch No:  11 Iteration No:  642 \n",
            " Loss:  0.0005300508346408606\n",
            "Training:  Epoch No:  11 Iteration No:  643 \n",
            " Loss:  0.005040919408202171\n",
            "Training:  Epoch No:  11 Iteration No:  644 \n",
            " Loss:  5.7665260101202875e-05\n",
            "Training:  Epoch No:  11 Iteration No:  645 \n",
            " Loss:  0.00035646697506308556\n",
            "Training:  Epoch No:  11 Iteration No:  646 \n",
            " Loss:  0.001073898863978684\n",
            "Training:  Epoch No:  11 Iteration No:  647 \n",
            " Loss:  0.002272530924528837\n",
            "Training:  Epoch No:  11 Iteration No:  648 \n",
            " Loss:  0.0003562820784281939\n",
            "Training:  Epoch No:  11 Iteration No:  649 \n",
            " Loss:  8.393372991122305e-05\n",
            "Training:  Epoch No:  11 Iteration No:  650 \n",
            " Loss:  0.0006651723524555564\n",
            "Training:  Epoch No:  11 Iteration No:  651 \n",
            " Loss:  0.004374450072646141\n",
            "Training:  Epoch No:  11 Iteration No:  652 \n",
            " Loss:  0.000599535764195025\n",
            "Training:  Epoch No:  11 Iteration No:  653 \n",
            " Loss:  0.0009757365332916379\n",
            "Training:  Epoch No:  11 Iteration No:  654 \n",
            " Loss:  0.0004520981456153095\n",
            "Training:  Epoch No:  11 Iteration No:  655 \n",
            " Loss:  0.00014226628991309553\n",
            "Training:  Epoch No:  11 Iteration No:  656 \n",
            " Loss:  0.0006751353503204882\n",
            "Training:  Epoch No:  11 Iteration No:  657 \n",
            " Loss:  7.673240179428831e-05\n",
            "Training:  Epoch No:  11 Iteration No:  658 \n",
            " Loss:  0.00019414884445723146\n",
            "Training:  Epoch No:  11 Iteration No:  659 \n",
            " Loss:  0.005783563945442438\n",
            "Training:  Epoch No:  11 Iteration No:  660 \n",
            " Loss:  0.00019228796008974314\n",
            "Training:  Epoch No:  11 Iteration No:  661 \n",
            " Loss:  0.0002037998056039214\n",
            "Training:  Epoch No:  11 Iteration No:  662 \n",
            " Loss:  0.0030240037012845278\n",
            "Training:  Epoch No:  11 Iteration No:  663 \n",
            " Loss:  7.042029028525576e-05\n",
            "Training:  Epoch No:  11 Iteration No:  664 \n",
            " Loss:  0.0018831042107194662\n",
            "Training:  Epoch No:  11 Iteration No:  665 \n",
            " Loss:  0.000490715610794723\n",
            "Training:  Epoch No:  11 Iteration No:  666 \n",
            " Loss:  0.001983060734346509\n",
            "Training:  Epoch No:  11 Iteration No:  667 \n",
            " Loss:  0.00016988336574286222\n",
            "Training:  Epoch No:  11 Iteration No:  668 \n",
            " Loss:  5.615366171696223e-05\n",
            "Training:  Epoch No:  11 Iteration No:  669 \n",
            " Loss:  0.0008369661518372595\n",
            "Training:  Epoch No:  11 Iteration No:  670 \n",
            " Loss:  0.0010816252324730158\n",
            "Training:  Epoch No:  11 Iteration No:  671 \n",
            " Loss:  0.00032160209957510233\n",
            "Training:  Epoch No:  11 Iteration No:  672 \n",
            " Loss:  0.0008217527065426111\n",
            "Training:  Epoch No:  11 Iteration No:  673 \n",
            " Loss:  7.848189125070348e-05\n",
            "Training:  Epoch No:  11 Iteration No:  674 \n",
            " Loss:  0.0026349497493356466\n",
            "Training:  Epoch No:  11 Iteration No:  675 \n",
            " Loss:  0.0028434765990823507\n",
            "Training:  Epoch No:  11 Iteration No:  676 \n",
            " Loss:  0.00016290477651637048\n",
            "Training:  Epoch No:  11 Iteration No:  677 \n",
            " Loss:  0.0008183932513929904\n",
            "Training:  Epoch No:  11 Iteration No:  678 \n",
            " Loss:  0.005664939992129803\n",
            "Training:  Epoch No:  11 Iteration No:  679 \n",
            " Loss:  0.001984392059966922\n",
            "Training:  Epoch No:  11 Iteration No:  680 \n",
            " Loss:  0.001866956357844174\n",
            "Training:  Epoch No:  11 Iteration No:  681 \n",
            " Loss:  0.002251905621960759\n",
            "Training:  Epoch No:  11 Iteration No:  682 \n",
            " Loss:  0.0014222220052033663\n",
            "Training:  Epoch No:  11 Iteration No:  683 \n",
            " Loss:  0.0003887672210112214\n",
            "Training:  Epoch No:  11 Iteration No:  684 \n",
            " Loss:  0.001517732278443873\n",
            "Training:  Epoch No:  11 Iteration No:  685 \n",
            " Loss:  0.000695063965395093\n",
            "Training:  Epoch No:  11 Iteration No:  686 \n",
            " Loss:  0.01473970152437687\n",
            "Training:  Epoch No:  11 Iteration No:  687 \n",
            " Loss:  0.005456259939819574\n",
            "Training:  Epoch No:  11 Iteration No:  688 \n",
            " Loss:  1.0744476639956702e-05\n",
            "Training:  Epoch No:  11 Iteration No:  689 \n",
            " Loss:  0.006059081293642521\n",
            "Training:  Epoch No:  11 Iteration No:  690 \n",
            " Loss:  0.017813362181186676\n",
            "Training:  Epoch No:  11 Iteration No:  691 \n",
            " Loss:  8.524247095920146e-05\n",
            "Training:  Epoch No:  11 Iteration No:  692 \n",
            " Loss:  0.0028271644841879606\n",
            "Training:  Epoch No:  11 Iteration No:  693 \n",
            " Loss:  0.00025101733626797795\n",
            "Training:  Epoch No:  11 Iteration No:  694 \n",
            " Loss:  0.0005366647383198142\n",
            "Training:  Epoch No:  11 Iteration No:  695 \n",
            " Loss:  0.005599986296147108\n",
            "Training:  Epoch No:  11 Iteration No:  696 \n",
            " Loss:  0.003896051086485386\n",
            "Training:  Epoch No:  11 Iteration No:  697 \n",
            " Loss:  0.0006903757457621396\n",
            "Training:  Epoch No:  11 Iteration No:  698 \n",
            " Loss:  0.00205977912992239\n",
            "Training:  Epoch No:  11 Iteration No:  699 \n",
            " Loss:  0.0005440521636046469\n",
            "Training:  Epoch No:  11 Iteration No:  700 \n",
            " Loss:  0.00041054681059904397\n",
            "Training:  Epoch No:  11 Iteration No:  701 \n",
            " Loss:  0.00017699187446851283\n",
            "Training:  Epoch No:  11 Iteration No:  702 \n",
            " Loss:  0.0017937677912414074\n",
            "Training:  Epoch No:  11 Iteration No:  703 \n",
            " Loss:  0.001979819731786847\n",
            "Training:  Epoch No:  11 Iteration No:  704 \n",
            " Loss:  0.00013816596765536815\n",
            "Training:  Epoch No:  11 Iteration No:  705 \n",
            " Loss:  0.001169600640423596\n",
            "Training:  Epoch No:  11 Iteration No:  706 \n",
            " Loss:  0.002951802685856819\n",
            "Training:  Epoch No:  11 Iteration No:  707 \n",
            " Loss:  0.004716894123703241\n",
            "Training:  Epoch No:  11 Iteration No:  708 \n",
            " Loss:  0.0008339050691574812\n",
            "Training:  Epoch No:  11 Iteration No:  709 \n",
            " Loss:  0.0002975646057166159\n",
            "Training:  Epoch No:  11 Iteration No:  710 \n",
            " Loss:  0.0005803634412586689\n",
            "Training:  Epoch No:  11 Iteration No:  711 \n",
            " Loss:  0.0008925978909246624\n",
            "Training:  Epoch No:  11 Iteration No:  712 \n",
            " Loss:  0.0002279184409417212\n",
            "Training:  Epoch No:  11 Iteration No:  713 \n",
            " Loss:  0.001635476597584784\n",
            "Training:  Epoch No:  11 Iteration No:  714 \n",
            " Loss:  0.015701064839959145\n",
            "Training:  Epoch No:  11 Iteration No:  715 \n",
            " Loss:  0.0005108872428536415\n",
            "Training:  Epoch No:  11 Iteration No:  716 \n",
            " Loss:  7.535654731327668e-05\n",
            "Training:  Epoch No:  11 Iteration No:  717 \n",
            " Loss:  0.005387647543102503\n",
            "Training:  Epoch No:  11 Iteration No:  718 \n",
            " Loss:  0.00036564914626069367\n",
            "Training:  Epoch No:  11 Iteration No:  719 \n",
            " Loss:  0.0032755578868091106\n",
            "Training:  Epoch No:  11 Iteration No:  720 \n",
            " Loss:  0.00039447189192287624\n",
            "Training:  Epoch No:  11 Iteration No:  721 \n",
            " Loss:  0.0014227356296032667\n",
            "Training:  Epoch No:  11 Iteration No:  722 \n",
            " Loss:  0.0010093781165778637\n",
            "Training:  Epoch No:  11 Iteration No:  723 \n",
            " Loss:  0.00020410249999258667\n",
            "Training:  Epoch No:  11 Iteration No:  724 \n",
            " Loss:  0.00367410178296268\n",
            "Training:  Epoch No:  11 Iteration No:  725 \n",
            " Loss:  0.02927260287106037\n",
            "Training:  Epoch No:  11 Iteration No:  726 \n",
            " Loss:  0.0006833910592831671\n",
            "Training:  Epoch No:  11 Iteration No:  727 \n",
            " Loss:  0.011305101215839386\n",
            "Training:  Epoch No:  11 Iteration No:  728 \n",
            " Loss:  0.013652254827320576\n",
            "Training:  Epoch No:  11 Iteration No:  729 \n",
            " Loss:  0.000943720166105777\n",
            "Training:  Epoch No:  11 Iteration No:  730 \n",
            " Loss:  0.000857171427924186\n",
            "Training:  Epoch No:  11 Iteration No:  731 \n",
            " Loss:  9.623996447771788e-05\n",
            "Training:  Epoch No:  11 Iteration No:  732 \n",
            " Loss:  0.001312242355197668\n",
            "Training:  Epoch No:  11 Iteration No:  733 \n",
            " Loss:  0.001696840743534267\n",
            "Training:  Epoch No:  11 Iteration No:  734 \n",
            " Loss:  8.990159403765574e-05\n",
            "Training:  Epoch No:  11 Iteration No:  735 \n",
            " Loss:  0.0006383393192663789\n",
            "Training:  Epoch No:  11 Iteration No:  736 \n",
            " Loss:  0.016279082745313644\n",
            "Training:  Epoch No:  11 Iteration No:  737 \n",
            " Loss:  0.0036068193148821592\n",
            "Training:  Epoch No:  11 Iteration No:  738 \n",
            " Loss:  0.001937242690473795\n",
            "Training:  Epoch No:  11 Iteration No:  739 \n",
            " Loss:  0.0034688040614128113\n",
            "Training:  Epoch No:  11 Iteration No:  740 \n",
            " Loss:  0.0010814537527039647\n",
            "Training:  Epoch No:  11 Iteration No:  741 \n",
            " Loss:  0.0022100850474089384\n",
            "Training:  Epoch No:  11 Iteration No:  742 \n",
            " Loss:  0.01826597936451435\n",
            "Training:  Epoch No:  11 Iteration No:  743 \n",
            " Loss:  0.0011086908634752035\n",
            "Training:  Epoch No:  11 Iteration No:  744 \n",
            " Loss:  0.0011850890005007386\n",
            "Training:  Epoch No:  11 Iteration No:  745 \n",
            " Loss:  0.006140839774161577\n",
            "Training:  Epoch No:  11 Iteration No:  746 \n",
            " Loss:  0.0006510545499622822\n",
            "Training:  Epoch No:  11 Iteration No:  747 \n",
            " Loss:  0.001260656281374395\n",
            "Training:  Epoch No:  11 Iteration No:  748 \n",
            " Loss:  8.768390398472548e-05\n",
            "Training:  Epoch No:  11 Iteration No:  749 \n",
            " Loss:  0.00031262534321285784\n",
            "Training:  Epoch No:  11 Iteration No:  750 \n",
            " Loss:  0.001427218783646822\n",
            "Training:  Epoch No:  11 Iteration No:  751 \n",
            " Loss:  0.0008462078403681517\n",
            "Training:  Epoch No:  11 Iteration No:  752 \n",
            " Loss:  0.00562566053122282\n",
            "Training:  Epoch No:  11 Iteration No:  753 \n",
            " Loss:  7.98646651674062e-05\n",
            "Training:  Epoch No:  11 Iteration No:  754 \n",
            " Loss:  0.00015346292639151216\n",
            "Training:  Epoch No:  11 Iteration No:  755 \n",
            " Loss:  5.442362817120738e-05\n",
            "Training:  Epoch No:  11 Iteration No:  756 \n",
            " Loss:  0.0010893878061324358\n",
            "Training:  Epoch No:  11 Iteration No:  757 \n",
            " Loss:  0.0035719680599868298\n",
            "Training:  Epoch No:  11 Iteration No:  758 \n",
            " Loss:  0.0006292816833592951\n",
            "Training:  Epoch No:  11 Iteration No:  759 \n",
            " Loss:  0.00026438134955242276\n",
            "Training:  Epoch No:  11 Iteration No:  760 \n",
            " Loss:  0.0003996356681454927\n",
            "Training:  Epoch No:  11 Iteration No:  761 \n",
            " Loss:  0.0022220939863473177\n",
            "Training:  Epoch No:  11 Iteration No:  762 \n",
            " Loss:  0.0030039623379707336\n",
            "Training:  Epoch No:  11 Iteration No:  763 \n",
            " Loss:  0.003585307626053691\n",
            "Training:  Epoch No:  11 Iteration No:  764 \n",
            " Loss:  0.0007208640454337001\n",
            "Training:  Epoch No:  11 Iteration No:  765 \n",
            " Loss:  0.00042320069042034447\n",
            "Training:  Epoch No:  11 Iteration No:  766 \n",
            " Loss:  0.0003849743807222694\n",
            "Training:  Epoch No:  11 Iteration No:  767 \n",
            " Loss:  0.0015071411617100239\n",
            "Training:  Epoch No:  11 Iteration No:  768 \n",
            " Loss:  0.0057352446019649506\n",
            "Training:  Epoch No:  11 Iteration No:  769 \n",
            " Loss:  0.00021042925072833896\n",
            "Training:  Epoch No:  11 Iteration No:  770 \n",
            " Loss:  0.01888643018901348\n",
            "Training:  Epoch No:  11 Iteration No:  771 \n",
            " Loss:  0.0008459690725430846\n",
            "Training:  Epoch No:  11 Iteration No:  772 \n",
            " Loss:  0.005111574195325375\n",
            "Training:  Epoch No:  11 Iteration No:  773 \n",
            " Loss:  0.0006424513994716108\n",
            "Training:  Epoch No:  11 Iteration No:  774 \n",
            " Loss:  0.015325604937970638\n",
            "Training:  Epoch No:  11 Iteration No:  775 \n",
            " Loss:  0.0022298048716038465\n",
            "Training:  Epoch No:  11 Iteration No:  776 \n",
            " Loss:  0.0008697140729054809\n",
            "Training:  Epoch No:  11 Iteration No:  777 \n",
            " Loss:  0.0009360008989460766\n",
            "Training:  Epoch No:  11 Iteration No:  778 \n",
            " Loss:  0.0079969372600317\n",
            "Training:  Epoch No:  11 Iteration No:  779 \n",
            " Loss:  0.0008559530251659453\n",
            "Training:  Epoch No:  11 Iteration No:  780 \n",
            " Loss:  0.0005302854697220027\n",
            "Training:  Epoch No:  11 Iteration No:  781 \n",
            " Loss:  0.0006507061771117151\n",
            "Training:  Epoch No:  11 Iteration No:  782 \n",
            " Loss:  0.0004209507897030562\n",
            "Training:  Epoch No:  11 Iteration No:  783 \n",
            " Loss:  0.0022563051898032427\n",
            "Training:  Epoch No:  11 Iteration No:  784 \n",
            " Loss:  4.1829120164038613e-05\n",
            "Training:  Epoch No:  11 Iteration No:  785 \n",
            " Loss:  0.00032519447267986834\n",
            "Training:  Epoch No:  11 Iteration No:  786 \n",
            " Loss:  0.0028708544559776783\n",
            "Training:  Epoch No:  11 Iteration No:  787 \n",
            " Loss:  0.004980891477316618\n",
            "Training:  Epoch No:  11 Iteration No:  788 \n",
            " Loss:  0.0006274563493207097\n",
            "Training:  Epoch No:  11 Iteration No:  789 \n",
            " Loss:  0.0022859591990709305\n",
            "Training:  Epoch No:  11 Iteration No:  790 \n",
            " Loss:  0.0018584670033305883\n",
            "Training:  Epoch No:  11 Iteration No:  791 \n",
            " Loss:  0.00977023970335722\n",
            "Training:  Epoch No:  11 Iteration No:  792 \n",
            " Loss:  0.0008580653229728341\n",
            "Training:  Epoch No:  11 Iteration No:  793 \n",
            " Loss:  0.0007329130894504488\n",
            "Training:  Epoch No:  11 Iteration No:  794 \n",
            " Loss:  0.0031487070955336094\n",
            "Training:  Epoch No:  11 Iteration No:  795 \n",
            " Loss:  9.42678889259696e-05\n",
            "Training:  Epoch No:  11 Iteration No:  796 \n",
            " Loss:  0.00040646237903274596\n",
            "Training:  Epoch No:  11 Iteration No:  797 \n",
            " Loss:  0.003563008503988385\n",
            "Training:  Epoch No:  11 Iteration No:  798 \n",
            " Loss:  0.0002800186921376735\n",
            "Training:  Epoch No:  11 Iteration No:  799 \n",
            " Loss:  0.0003530705871526152\n",
            "Training:  Epoch No:  11 Iteration No:  800 \n",
            " Loss:  0.0030595927964895964\n",
            "Training:  Epoch No:  11 Iteration No:  801 \n",
            " Loss:  0.00020864783436991274\n",
            "Training:  Epoch No:  11 Iteration No:  802 \n",
            " Loss:  0.001970031764358282\n",
            "Training:  Epoch No:  11 Iteration No:  803 \n",
            " Loss:  0.007387347985059023\n",
            "Training:  Epoch No:  11 Iteration No:  804 \n",
            " Loss:  0.0011490843025967479\n",
            "Training:  Epoch No:  11 Iteration No:  805 \n",
            " Loss:  0.0027142614126205444\n",
            "Training:  Epoch No:  11 Iteration No:  806 \n",
            " Loss:  0.0006924821645952761\n",
            "Training:  Epoch No:  11 Iteration No:  807 \n",
            " Loss:  0.0005831273156218231\n",
            "Training:  Epoch No:  11 Iteration No:  808 \n",
            " Loss:  0.0002134792011929676\n",
            "Training:  Epoch No:  11 Iteration No:  809 \n",
            " Loss:  0.015112237073481083\n",
            "Training:  Epoch No:  11 Iteration No:  810 \n",
            " Loss:  0.01560465432703495\n",
            "Training:  Epoch No:  11 Iteration No:  811 \n",
            " Loss:  0.0005034201894886792\n",
            "Training:  Epoch No:  11 Iteration No:  812 \n",
            " Loss:  0.0003091029357165098\n",
            "Training:  Epoch No:  11 Iteration No:  813 \n",
            " Loss:  0.002292043063789606\n",
            "Training:  Epoch No:  11 Iteration No:  814 \n",
            " Loss:  0.007917558774352074\n",
            "Training:  Epoch No:  11 Iteration No:  815 \n",
            " Loss:  8.992444782052189e-05\n",
            "Training:  Epoch No:  11 Iteration No:  816 \n",
            " Loss:  0.0007044964004307985\n",
            "Training:  Epoch No:  11 Iteration No:  817 \n",
            " Loss:  0.0016834500711411238\n",
            "Training:  Epoch No:  11 Iteration No:  818 \n",
            " Loss:  0.0005452774930745363\n",
            "Training:  Epoch No:  11 Iteration No:  819 \n",
            " Loss:  0.005046822130680084\n",
            "Training:  Epoch No:  11 Iteration No:  820 \n",
            " Loss:  0.0006868245545774698\n",
            "Training:  Epoch No:  11 Iteration No:  821 \n",
            " Loss:  0.010024946182966232\n",
            "Training:  Epoch No:  11 Iteration No:  822 \n",
            " Loss:  5.3648491302737966e-05\n",
            "Training:  Epoch No:  11 Iteration No:  823 \n",
            " Loss:  0.005598221439868212\n",
            "Training:  Epoch No:  11 Iteration No:  824 \n",
            " Loss:  0.0008341041975654662\n",
            "Training:  Epoch No:  11 Iteration No:  825 \n",
            " Loss:  0.012619206681847572\n",
            "Training:  Epoch No:  11 Iteration No:  826 \n",
            " Loss:  0.0006735158967785537\n",
            "Training:  Epoch No:  11 Iteration No:  827 \n",
            " Loss:  0.009291209280490875\n",
            "Training:  Epoch No:  11 Iteration No:  828 \n",
            " Loss:  0.002032333519309759\n",
            "Training:  Epoch No:  11 Iteration No:  829 \n",
            " Loss:  0.0006626969552598894\n",
            "Training:  Epoch No:  11 Iteration No:  830 \n",
            " Loss:  0.0001966389245353639\n",
            "Training:  Epoch No:  11 Iteration No:  831 \n",
            " Loss:  0.0007885533850640059\n",
            "Training:  Epoch No:  11 Iteration No:  832 \n",
            " Loss:  0.00015453860396519303\n",
            "Training:  Epoch No:  11 Iteration No:  833 \n",
            " Loss:  0.00011675672431010753\n",
            "Training:  Epoch No:  11 Iteration No:  834 \n",
            " Loss:  0.00031329382909461856\n",
            "Training:  Epoch No:  11 Iteration No:  835 \n",
            " Loss:  0.0005863976548425853\n",
            "Training:  Epoch No:  11 Iteration No:  836 \n",
            " Loss:  0.0004164786369074136\n",
            "Training:  Epoch No:  11 Iteration No:  837 \n",
            " Loss:  0.0004145106940995902\n",
            "Training:  Epoch No:  11 Iteration No:  838 \n",
            " Loss:  0.00039922833093442023\n",
            "Training:  Epoch No:  11 Iteration No:  839 \n",
            " Loss:  5.19144487043377e-05\n",
            "Training:  Epoch No:  11 Iteration No:  840 \n",
            " Loss:  0.00037754015647806227\n",
            "Training:  Epoch No:  11 Iteration No:  841 \n",
            " Loss:  0.003927027806639671\n",
            "Training:  Epoch No:  11 Iteration No:  842 \n",
            " Loss:  0.00037396891275420785\n",
            "Training:  Epoch No:  11 Iteration No:  843 \n",
            " Loss:  0.00010664817091310397\n",
            "Training:  Epoch No:  11 Iteration No:  844 \n",
            " Loss:  0.0003271726309321821\n",
            "Training:  Epoch No:  11 Iteration No:  845 \n",
            " Loss:  0.0015506070340052247\n",
            "Training:  Epoch No:  11 Iteration No:  846 \n",
            " Loss:  0.00046084087807685137\n",
            "Training:  Epoch No:  11 Iteration No:  847 \n",
            " Loss:  0.00032564112916588783\n",
            "Training:  Epoch No:  11 Iteration No:  848 \n",
            " Loss:  0.0017609052592888474\n",
            "Training:  Epoch No:  11 Iteration No:  849 \n",
            " Loss:  0.002599498024210334\n",
            "Training:  Epoch No:  11 Iteration No:  850 \n",
            " Loss:  0.0013317266711965203\n",
            "Training:  Epoch No:  11 Iteration No:  851 \n",
            " Loss:  0.00022496419842354953\n",
            "Training:  Epoch No:  11 Iteration No:  852 \n",
            " Loss:  0.0005804511602036655\n",
            "Training:  Epoch No:  11 Iteration No:  853 \n",
            " Loss:  0.0004312470555305481\n",
            "Training:  Epoch No:  11 Iteration No:  854 \n",
            " Loss:  0.0008348621777258813\n",
            "Training:  Epoch No:  11 Iteration No:  855 \n",
            " Loss:  0.001046047662384808\n",
            "Training:  Epoch No:  11 Iteration No:  856 \n",
            " Loss:  0.0017287316732108593\n",
            "Training:  Epoch No:  11 Iteration No:  857 \n",
            " Loss:  0.001483412110246718\n",
            "Training:  Epoch No:  11 Iteration No:  858 \n",
            " Loss:  0.00010044373630080372\n",
            "Training:  Epoch No:  11 Iteration No:  859 \n",
            " Loss:  0.0005922453128732741\n",
            "Training:  Epoch No:  11 Iteration No:  860 \n",
            " Loss:  0.00033728338894434273\n",
            "Training:  Epoch No:  11 Iteration No:  861 \n",
            " Loss:  0.0002187406935263425\n",
            "Training:  Epoch No:  11 Iteration No:  862 \n",
            " Loss:  0.00011815464677056298\n",
            "Training:  Epoch No:  11 Iteration No:  863 \n",
            " Loss:  0.000422193028498441\n",
            "Training:  Epoch No:  11 Iteration No:  864 \n",
            " Loss:  0.0048183659091591835\n",
            "Training:  Epoch No:  11 Iteration No:  865 \n",
            " Loss:  0.0006717565120197833\n",
            "Training:  Epoch No:  11 Iteration No:  866 \n",
            " Loss:  0.002740677911788225\n",
            "Training:  Epoch No:  11 Iteration No:  867 \n",
            " Loss:  0.00017692381516098976\n",
            "Training:  Epoch No:  11 Iteration No:  868 \n",
            " Loss:  0.028263721615076065\n",
            "Training:  Epoch No:  11 Iteration No:  869 \n",
            " Loss:  0.00013226889132056385\n",
            "Training:  Epoch No:  11 Iteration No:  870 \n",
            " Loss:  9.856809629127383e-05\n",
            "Training:  Epoch No:  11 Iteration No:  871 \n",
            " Loss:  0.004338805563747883\n",
            "Training:  Epoch No:  11 Iteration No:  872 \n",
            " Loss:  0.00016791780944913626\n",
            "Training:  Epoch No:  11 Iteration No:  873 \n",
            " Loss:  0.004826957825571299\n",
            "Training:  Epoch No:  11 Iteration No:  874 \n",
            " Loss:  0.0020320010371506214\n",
            "Training:  Epoch No:  11 Iteration No:  875 \n",
            " Loss:  0.00013311485236044973\n",
            "Training:  Epoch No:  11 Iteration No:  876 \n",
            " Loss:  0.0005340672796592116\n",
            "Training:  Epoch No:  11 Iteration No:  877 \n",
            " Loss:  4.6868535719113424e-05\n",
            "Training:  Epoch No:  11 Iteration No:  878 \n",
            " Loss:  0.02261950634419918\n",
            "Training:  Epoch No:  11 Iteration No:  879 \n",
            " Loss:  0.003682726062834263\n",
            "Training:  Epoch No:  11 Iteration No:  880 \n",
            " Loss:  0.0014576563844457269\n",
            "Training:  Epoch No:  11 Iteration No:  881 \n",
            " Loss:  0.0009202041546814144\n",
            "Training:  Epoch No:  11 Iteration No:  882 \n",
            " Loss:  0.0019150966545566916\n",
            "Training:  Epoch No:  11 Iteration No:  883 \n",
            " Loss:  0.0021189164835959673\n",
            "Training:  Epoch No:  11 Iteration No:  884 \n",
            " Loss:  0.004288474563509226\n",
            "Training:  Epoch No:  11 Iteration No:  885 \n",
            " Loss:  0.003844976658001542\n",
            "Training:  Epoch No:  11 Iteration No:  886 \n",
            " Loss:  0.0002210482198279351\n",
            "Training:  Epoch No:  11 Iteration No:  887 \n",
            " Loss:  0.00045816352940164506\n",
            "Training:  Epoch No:  11 Iteration No:  888 \n",
            " Loss:  0.033249013125896454\n",
            "Training:  Epoch No:  11 Iteration No:  889 \n",
            " Loss:  8.490792970405892e-05\n",
            "Training:  Epoch No:  11 Iteration No:  890 \n",
            " Loss:  0.0010334649123251438\n",
            "Training:  Epoch No:  11 Iteration No:  891 \n",
            " Loss:  0.0001793600822566077\n",
            "Training:  Epoch No:  11 Iteration No:  892 \n",
            " Loss:  0.001556539093144238\n",
            "Training:  Epoch No:  11 Iteration No:  893 \n",
            " Loss:  0.0057868072763085365\n",
            "Training:  Epoch No:  11 Iteration No:  894 \n",
            " Loss:  0.003396223997697234\n",
            "Training:  Epoch No:  11 Iteration No:  895 \n",
            " Loss:  0.008445091545581818\n",
            "Training:  Epoch No:  11 Iteration No:  896 \n",
            " Loss:  0.0008496876107528806\n",
            "Training:  Epoch No:  11 Iteration No:  897 \n",
            " Loss:  0.0001233640214195475\n",
            "Training:  Epoch No:  11 Iteration No:  898 \n",
            " Loss:  0.010554912500083447\n",
            "Training:  Epoch No:  11 Iteration No:  899 \n",
            " Loss:  0.0038712245877832174\n",
            "Training:  Epoch No:  11 Iteration No:  900 \n",
            " Loss:  0.00010919358464889228\n",
            "Training:  Epoch No:  11 Iteration No:  901 \n",
            " Loss:  0.004678292665630579\n",
            "Training:  Epoch No:  11 Iteration No:  902 \n",
            " Loss:  0.0006408269400708377\n",
            "Training:  Epoch No:  11 Iteration No:  903 \n",
            " Loss:  0.002000773325562477\n",
            "Training:  Epoch No:  11 Iteration No:  904 \n",
            " Loss:  0.001253369846381247\n",
            "Training:  Epoch No:  11 Iteration No:  905 \n",
            " Loss:  3.77430951630231e-05\n",
            "Training:  Epoch No:  11 Iteration No:  906 \n",
            " Loss:  0.00875511858612299\n",
            "Training:  Epoch No:  11 Iteration No:  907 \n",
            " Loss:  0.014987733215093613\n",
            "Training:  Epoch No:  11 Iteration No:  908 \n",
            " Loss:  0.0029281796887516975\n",
            "Training:  Epoch No:  11 Iteration No:  909 \n",
            " Loss:  0.0004123893741052598\n",
            "Training:  Epoch No:  11 Iteration No:  910 \n",
            " Loss:  0.0010842274641618133\n",
            "Training:  Epoch No:  11 Iteration No:  911 \n",
            " Loss:  0.0022416089195758104\n",
            "Training:  Epoch No:  11 Iteration No:  912 \n",
            " Loss:  0.0016143308021128178\n",
            "Training:  Epoch No:  11 Iteration No:  913 \n",
            " Loss:  0.00019005390640813857\n",
            "Training:  Epoch No:  11 Iteration No:  914 \n",
            " Loss:  0.004704499617218971\n",
            "Training:  Epoch No:  11 Iteration No:  915 \n",
            " Loss:  0.0001032471627695486\n",
            "Training:  Epoch No:  11 Iteration No:  916 \n",
            " Loss:  0.0002844066184479743\n",
            "Training:  Epoch No:  11 Iteration No:  917 \n",
            " Loss:  0.004145279061049223\n",
            "Training:  Epoch No:  11 Iteration No:  918 \n",
            " Loss:  0.0012511842651292682\n",
            "Training:  Epoch No:  11 Iteration No:  919 \n",
            " Loss:  0.03522845730185509\n",
            "Training:  Epoch No:  11 Iteration No:  920 \n",
            " Loss:  0.007866202853620052\n",
            "Training:  Epoch No:  11 Iteration No:  921 \n",
            " Loss:  0.0011826050467789173\n",
            "Training:  Epoch No:  11 Iteration No:  922 \n",
            " Loss:  0.0008217825670726597\n",
            "Training:  Epoch No:  11 Iteration No:  923 \n",
            " Loss:  0.0013479768531396985\n",
            "Training:  Epoch No:  11 Iteration No:  924 \n",
            " Loss:  0.026539290323853493\n",
            "Training:  Epoch No:  11 Iteration No:  925 \n",
            " Loss:  0.006310617551207542\n",
            "Training:  Epoch No:  11 Iteration No:  926 \n",
            " Loss:  0.001865581376478076\n",
            "Training:  Epoch No:  11 Iteration No:  927 \n",
            " Loss:  0.0025208035949617624\n",
            "Training:  Epoch No:  11 Iteration No:  928 \n",
            " Loss:  0.0021132691763341427\n",
            "Training:  Epoch No:  11 Iteration No:  929 \n",
            " Loss:  0.005745802074670792\n",
            "Training:  Epoch No:  11 Iteration No:  930 \n",
            " Loss:  0.00019371956295799464\n",
            "Training:  Epoch No:  11 Iteration No:  931 \n",
            " Loss:  0.0005860243691131473\n",
            "Training:  Epoch No:  11 Iteration No:  932 \n",
            " Loss:  0.00022436986910179257\n",
            "Training:  Epoch No:  11 Iteration No:  933 \n",
            " Loss:  0.0004668304754886776\n",
            "Training:  Epoch No:  11 Iteration No:  934 \n",
            " Loss:  0.001084814197383821\n",
            "Training:  Epoch No:  11 Iteration No:  935 \n",
            " Loss:  0.001523814513348043\n",
            "Training:  Epoch No:  11 Iteration No:  936 \n",
            " Loss:  0.0005083507858216763\n",
            "Training:  Epoch No:  11 Iteration No:  937 \n",
            " Loss:  0.021136408671736717\n",
            "Training:  Epoch No:  11 Iteration No:  938 \n",
            " Loss:  4.8431935283588246e-05\n",
            "Validation:  Epoch No:  11 \n",
            " Loss:  0.03543399085509655\n",
            "validation accuracy:  99.14\n",
            "99 % accuracy reached!\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(10)\n",
        "all_losses = {'train':[], 'test':[]}\n",
        "validation_accracy = []\n",
        "for epoch in range(num_epochs):\n",
        "    # Training\n",
        "    batch_loss = 0\n",
        "    counter = 0\n",
        "    for batch_data, batch_labels in train_loader:\n",
        "        batch_data, batch_labels = batch_data.to(device), batch_labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        batch_outputs = model(batch_data)        \n",
        "        loss = criterion(batch_outputs, batch_labels)\n",
        "        loss.backward()\n",
        "        print('Training: ', 'Epoch No: ', epoch+1, 'Iteration No: ', counter+1, '\\n',\n",
        "        'Loss: ', loss.item())\n",
        "        optimizer.step()\n",
        "        batch_loss += loss.item()\n",
        "        counter += 1\n",
        "    all_losses['train'].append(batch_loss / counter)    \n",
        "    \n",
        "    # Validation\n",
        "    with torch.set_grad_enabled(False):\n",
        "        batch_losses = []\n",
        "        counter = 0\n",
        "        total = 0\n",
        "        for batch_data, batch_labels in test_loader:\n",
        "            total += len(batch_data)\n",
        "            batch_data, batch_labels = batch_data.to(device), batch_labels.to(device)\n",
        "            batch_outputs = model(batch_data)            \n",
        "            loss = criterion(batch_outputs, batch_labels)\n",
        "            batch_losses.append(loss.item())\n",
        "            \n",
        "            for i in range(len(batch_data)):\n",
        "                true = int(batch_labels[i])\n",
        "                pred = int(torch.argmax(batch_outputs[i,:]))\n",
        "                if pred == true:\n",
        "                    counter += 1\n",
        "\n",
        "        all_losses['test'].append(sum(batch_losses) / len(batch_losses))\n",
        "        print('Validation: ', 'Epoch No: ', epoch+1,'\\n',\n",
        "            'Loss: ', all_losses['test'][-1])\n",
        "        acc = (counter*100) / total   \n",
        "        validation_accracy.append(acc)    \n",
        "        print('validation accuracy: ', acc) \n",
        "    if acc >= 99:\n",
        "        print('99 % accuracy reached!')\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "B4dIT-SVddOc",
        "outputId": "d82fc57f-f574-49a6-d2d0-e1b3431c25aa"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.plot(all_losses['train'], label='train')\n",
        "plt.plot(all_losses['test'], label='test')\n",
        "plt.legend()\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Loss VS Epochs');"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "8LfbUPb5ddRR",
        "outputId": "97d02e21-23d1-4c24-829f-eef65506602f"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.plot(validation_accracy)\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.ylim([90,100])\n",
        "plt.title('Test accuracy');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4AMCk5GBzZuy"
      },
      "source": [
        "# Primal Algorithm(FedAvg)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BooeBOtMuYTV"
      },
      "source": [
        "## Data preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "oYAMh7UEzPSj"
      },
      "outputs": [],
      "source": [
        "random.seed(10)\n",
        "\n",
        "num_users = 100\n",
        "\n",
        "s = len(mnist_trainset)\n",
        "client_data_sizes = []\n",
        "for i in range(num_users-1):\n",
        "    l = len(mnist_trainset) // num_users\n",
        "    client_data_sizes.append(l)\n",
        "    s -= l\n",
        "\n",
        "client_data_sizes.append(len(mnist_trainset) - sum(client_data_sizes))\n",
        " \n",
        "distributed_train_data = random_split(mnist_trainset, client_data_sizes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "CekkVSTSzPVH"
      },
      "outputs": [],
      "source": [
        "freq = [i/sum(client_data_sizes) for i in client_data_sizes]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "FAcaVckmzPYq"
      },
      "outputs": [],
      "source": [
        "batch_size = 10\n",
        "train_loaders = []\n",
        "for i in range(num_users):\n",
        "    train_loaders.append(torch.utils.data.DataLoader(\n",
        "                 dataset=distributed_train_data[i],\n",
        "                 batch_size=batch_size,\n",
        "                 shuffle=True\n",
        "                 ))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "WWn87Gwdgw6F"
      },
      "outputs": [],
      "source": [
        "test_loader = torch.utils.data.DataLoader(\n",
        "                dataset=mnist_testset,\n",
        "                batch_size=batch_size,\n",
        "                shuffle=False\n",
        "                )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "kJ_ssevG0FVJ"
      },
      "outputs": [],
      "source": [
        "model = CNN().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr = 0.1) \n",
        "num_epochs = 50"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QuUA2wg10FX7",
        "outputId": "7991dc52-37fd-42ff-c32b-e46417a34697"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch no 0\n",
            "Training:  Epoch No:  1 \n",
            " Loss:  0.6356174178284973\n",
            "Validation:  Epoch No:  1 \n",
            " Loss:  0.20763978674154895\n",
            "epoch no 1\n",
            "Training:  Epoch No:  2 \n",
            " Loss:  0.14341998453314095\n",
            "Validation:  Epoch No:  2 \n",
            " Loss:  0.1574986681704424\n",
            "epoch no 2\n",
            "Training:  Epoch No:  3 \n",
            " Loss:  0.11550691898645389\n",
            "Validation:  Epoch No:  3 \n",
            " Loss:  0.12257898452700124\n",
            "epoch no 3\n",
            "Training:  Epoch No:  4 \n",
            " Loss:  0.09084691827592102\n",
            "Validation:  Epoch No:  4 \n",
            " Loss:  0.10600796509594256\n",
            "epoch no 4\n",
            "Training:  Epoch No:  5 \n",
            " Loss:  0.07021566296336877\n",
            "Validation:  Epoch No:  5 \n",
            " Loss:  0.09573893159002682\n",
            "epoch no 5\n",
            "Training:  Epoch No:  6 \n",
            " Loss:  0.06438860063442314\n",
            "Validation:  Epoch No:  6 \n",
            " Loss:  0.08491580264275678\n",
            "epoch no 6\n",
            "Training:  Epoch No:  7 \n",
            " Loss:  0.05617526999132744\n",
            "Validation:  Epoch No:  7 \n",
            " Loss:  0.07814615783051848\n",
            "epoch no 7\n",
            "Training:  Epoch No:  8 \n",
            " Loss:  0.060992168172274416\n",
            "Validation:  Epoch No:  8 \n",
            " Loss:  0.06726221786825563\n",
            "epoch no 8\n",
            "Training:  Epoch No:  9 \n",
            " Loss:  0.049718513566877676\n",
            "Validation:  Epoch No:  9 \n",
            " Loss:  0.061870827828281516\n",
            "epoch no 9\n",
            "Training:  Epoch No:  10 \n",
            " Loss:  0.04582527597645731\n",
            "Validation:  Epoch No:  10 \n",
            " Loss:  0.059654664312445795\n",
            "epoch no 10\n",
            "Training:  Epoch No:  11 \n",
            " Loss:  0.02852570521476916\n",
            "Validation:  Epoch No:  11 \n",
            " Loss:  0.056676096426636055\n",
            "epoch no 11\n",
            "Training:  Epoch No:  12 \n",
            " Loss:  0.05112249812397142\n",
            "Validation:  Epoch No:  12 \n",
            " Loss:  0.05605172957247582\n",
            "epoch no 12\n",
            "Training:  Epoch No:  13 \n",
            " Loss:  0.03599520209929555\n",
            "Validation:  Epoch No:  13 \n",
            " Loss:  0.05247132603835263\n",
            "epoch no 13\n",
            "Training:  Epoch No:  14 \n",
            " Loss:  0.028160579268732337\n",
            "Validation:  Epoch No:  14 \n",
            " Loss:  0.0506344561455501\n",
            "epoch no 14\n",
            "Training:  Epoch No:  15 \n",
            " Loss:  0.030314323637210025\n",
            "Validation:  Epoch No:  15 \n",
            " Loss:  0.04713203693112811\n",
            "epoch no 15\n",
            "Training:  Epoch No:  16 \n",
            " Loss:  0.026435054535092203\n",
            "Validation:  Epoch No:  16 \n",
            " Loss:  0.04605823882103085\n",
            "epoch no 16\n",
            "Training:  Epoch No:  17 \n",
            " Loss:  0.028468059726473382\n",
            "Validation:  Epoch No:  17 \n",
            " Loss:  0.04430537066257335\n",
            "epoch no 17\n",
            "Training:  Epoch No:  18 \n",
            " Loss:  0.020448822060355826\n",
            "Validation:  Epoch No:  18 \n",
            " Loss:  0.044485774124065014\n",
            "epoch no 18\n",
            "Training:  Epoch No:  19 \n",
            " Loss:  0.02527301616937514\n",
            "Validation:  Epoch No:  19 \n",
            " Loss:  0.043747881881099286\n",
            "epoch no 19\n",
            "Training:  Epoch No:  20 \n",
            " Loss:  0.03395966602635934\n",
            "Validation:  Epoch No:  20 \n",
            " Loss:  0.04063740943319675\n",
            "epoch no 20\n",
            "Training:  Epoch No:  21 \n",
            " Loss:  0.022122187951127293\n",
            "Validation:  Epoch No:  21 \n",
            " Loss:  0.04049936043966647\n",
            "epoch no 21\n",
            "Training:  Epoch No:  22 \n",
            " Loss:  0.024935506342548836\n",
            "Validation:  Epoch No:  22 \n",
            " Loss:  0.03989478130557769\n",
            "epoch no 22\n",
            "Training:  Epoch No:  23 \n",
            " Loss:  0.03008584949435016\n",
            "Validation:  Epoch No:  23 \n",
            " Loss:  0.039118556072543276\n",
            "epoch no 23\n",
            "Training:  Epoch No:  24 \n",
            " Loss:  0.01636435601716369\n",
            "Validation:  Epoch No:  24 \n",
            " Loss:  0.039216688667666276\n",
            "epoch no 24\n",
            "Training:  Epoch No:  25 \n",
            " Loss:  0.02259331862146249\n",
            "Validation:  Epoch No:  25 \n",
            " Loss:  0.03858268722917776\n",
            "epoch no 25\n",
            "Training:  Epoch No:  26 \n",
            " Loss:  0.021982670212548773\n",
            "Validation:  Epoch No:  26 \n",
            " Loss:  0.037624235705670014\n",
            "epoch no 26\n",
            "Training:  Epoch No:  27 \n",
            " Loss:  0.018626951515179686\n",
            "Validation:  Epoch No:  27 \n",
            " Loss:  0.036612051671464516\n",
            "epoch no 27\n",
            "Training:  Epoch No:  28 \n",
            " Loss:  0.01934849975660797\n",
            "Validation:  Epoch No:  28 \n",
            " Loss:  0.03745774953212905\n",
            "epoch no 28\n",
            "Training:  Epoch No:  29 \n",
            " Loss:  0.012594017889651381\n",
            "Validation:  Epoch No:  29 \n",
            " Loss:  0.03561911512741106\n",
            "epoch no 29\n",
            "Training:  Epoch No:  30 \n",
            " Loss:  0.023263460310144493\n",
            "Validation:  Epoch No:  30 \n",
            " Loss:  0.03461612737644436\n",
            "epoch no 30\n",
            "Training:  Epoch No:  31 \n",
            " Loss:  0.019832409983205075\n",
            "Validation:  Epoch No:  31 \n",
            " Loss:  0.034396285525738736\n",
            "99 % accuracy reached!\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(10)\n",
        "all_losses = {'train':[], 'test':[]}\n",
        "validation_accracy = []\n",
        "for epoch in range(num_epochs):\n",
        "    print('epoch no', epoch)\n",
        "\n",
        "    model_weights = copy.deepcopy(model.state_dict())\n",
        "    averaged_weights = {}\n",
        "    for layer in model_weights.keys():\n",
        "        averaged_weights[layer] = torch.zeros_like(model_weights[layer])\n",
        "\n",
        "    client_losses = []\n",
        "\n",
        "    for client in random.sample([i for i in range(num_users)], 10):\n",
        "\n",
        "        client_loss = 0\n",
        "        counter = 0\n",
        "\n",
        "        for local_epoch in range(10):\n",
        "            for batch_data, batch_labels in train_loaders[client]:\n",
        "                batch_data, batch_labels = batch_data.to(device), batch_labels.to(device)\n",
        "                \n",
        "                optimizer.zero_grad()\n",
        "                \n",
        "                batch_outputs = model(batch_data)        \n",
        "                loss = criterion(batch_outputs, batch_labels)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                client_loss += loss.item()\n",
        "                counter += 1\n",
        "            client_losses.append(client_loss / counter)    \n",
        "\n",
        "        for layer in model.state_dict().keys():\n",
        "            averaged_weights[layer] += 0.1 * copy.deepcopy(model.state_dict()[layer])\n",
        "\n",
        "        model.load_state_dict(model_weights)\n",
        "        \n",
        "    model.load_state_dict(averaged_weights)  \n",
        "\n",
        "    all_losses['train'].append(sum(client_losses) / len(client_losses))\n",
        "    print('Training: ', 'Epoch No: ', epoch+1,'\\n',\n",
        "            'Loss: ', all_losses['train'][-1])\n",
        "\n",
        "\n",
        "    # Validation\n",
        "    with torch.set_grad_enabled(False):\n",
        "        batch_losses = []\n",
        "        counter = 0\n",
        "        total = 0\n",
        "        for batch_data, batch_labels in test_loader:\n",
        "            total += len(batch_data)\n",
        "            batch_data, batch_labels = batch_data.to(device), batch_labels.to(device)\n",
        "            batch_outputs = model(batch_data)            \n",
        "            loss = criterion(batch_outputs, batch_labels)\n",
        "            batch_losses.append(loss.item())\n",
        "            \n",
        "            for i in range(len(batch_data)):\n",
        "                true = int(batch_labels[i])\n",
        "                pred = int(torch.argmax(batch_outputs[i,:]))\n",
        "                if pred == true:\n",
        "                    counter += 1\n",
        "\n",
        "        all_losses['test'].append(sum(batch_losses) / len(batch_losses))\n",
        "        print('Validation: ', 'Epoch No: ', epoch+1,'\\n',\n",
        "            'Loss: ', all_losses['test'][-1])\n",
        "        acc = (counter*100) / total   \n",
        "        validation_accracy.append(acc)    \n",
        "\n",
        "    if acc >= 99:\n",
        "        print('99 % accuracy reached!')\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "WWBxbA52eX3Y",
        "outputId": "7fe34101-02a3-4005-c305-8773bf74ed3b"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.plot(all_losses['train'], label='train')\n",
        "plt.plot(all_losses['test'], label='test')\n",
        "plt.legend()\n",
        "plt.xlabel('Rounds')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Loss VS Rounds');"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "KvDfVzwWe9gi",
        "outputId": "ae205124-822a-4aba-8f62-ebfe8eccb8e4"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.plot(validation_accracy)\n",
        "plt.xlabel('Rounds')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Test accuracy');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RslVFyG2JIaN"
      },
      "source": [
        "## learning rate = 0.01"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "JORU_pKtso7C"
      },
      "outputs": [],
      "source": [
        "model = CNN().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr = 0.01) \n",
        "num_epochs = 70"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XjjtqDHkso9q",
        "outputId": "9fa18072-ef44-4b13-ea5c-8d50119ab65c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch no 0\n",
            "Training:  Epoch No:  1 \n",
            " Loss:  1.6771860360259148\n",
            "Validation:  Epoch No:  1 \n",
            " Loss:  0.3595539808999747\n",
            "epoch no 1\n",
            "Training:  Epoch No:  2 \n",
            " Loss:  0.33117774597137933\n",
            "Validation:  Epoch No:  2 \n",
            " Loss:  0.23952631389867748\n",
            "epoch no 2\n",
            "Training:  Epoch No:  3 \n",
            " Loss:  0.2147744605335205\n",
            "Validation:  Epoch No:  3 \n",
            " Loss:  0.1924066547870316\n",
            "epoch no 3\n",
            "Training:  Epoch No:  4 \n",
            " Loss:  0.16047551183824957\n",
            "Validation:  Epoch No:  4 \n",
            " Loss:  0.16026274319270306\n",
            "epoch no 4\n",
            "Training:  Epoch No:  5 \n",
            " Loss:  0.12621552847951137\n",
            "Validation:  Epoch No:  5 \n",
            " Loss:  0.1390299613149946\n",
            "epoch no 5\n",
            "Training:  Epoch No:  6 \n",
            " Loss:  0.09835507022166665\n",
            "Validation:  Epoch No:  6 \n",
            " Loss:  0.12518476954605467\n",
            "epoch no 6\n",
            "Training:  Epoch No:  7 \n",
            " Loss:  0.08768534536351266\n",
            "Validation:  Epoch No:  7 \n",
            " Loss:  0.11415003691103538\n",
            "epoch no 7\n",
            "Training:  Epoch No:  8 \n",
            " Loss:  0.07889113540240625\n",
            "Validation:  Epoch No:  8 \n",
            " Loss:  0.1037387453202773\n",
            "epoch no 8\n",
            "Training:  Epoch No:  9 \n",
            " Loss:  0.06883342542074276\n",
            "Validation:  Epoch No:  9 \n",
            " Loss:  0.09730587598728561\n",
            "epoch no 9\n",
            "Training:  Epoch No:  10 \n",
            " Loss:  0.06413500488442964\n",
            "Validation:  Epoch No:  10 \n",
            " Loss:  0.09191894770154022\n",
            "epoch no 10\n",
            "Training:  Epoch No:  11 \n",
            " Loss:  0.0652353615479349\n",
            "Validation:  Epoch No:  11 \n",
            " Loss:  0.0865757715265213\n",
            "epoch no 11\n",
            "Training:  Epoch No:  12 \n",
            " Loss:  0.05962636689603167\n",
            "Validation:  Epoch No:  12 \n",
            " Loss:  0.08406388091723056\n",
            "epoch no 12\n",
            "Training:  Epoch No:  13 \n",
            " Loss:  0.0540758741626704\n",
            "Validation:  Epoch No:  13 \n",
            " Loss:  0.07864734540295945\n",
            "epoch no 13\n",
            "Training:  Epoch No:  14 \n",
            " Loss:  0.04638023101543212\n",
            "Validation:  Epoch No:  14 \n",
            " Loss:  0.07959723071556835\n",
            "epoch no 14\n",
            "Training:  Epoch No:  15 \n",
            " Loss:  0.04930859658254407\n",
            "Validation:  Epoch No:  15 \n",
            " Loss:  0.0754656947698786\n",
            "epoch no 15\n",
            "Training:  Epoch No:  16 \n",
            " Loss:  0.050938209302142795\n",
            "Validation:  Epoch No:  16 \n",
            " Loss:  0.0714583184866326\n",
            "epoch no 16\n",
            "Training:  Epoch No:  17 \n",
            " Loss:  0.04529141203594249\n",
            "Validation:  Epoch No:  17 \n",
            " Loss:  0.07121279845418349\n",
            "epoch no 17\n",
            "Training:  Epoch No:  18 \n",
            " Loss:  0.041686757658046754\n",
            "Validation:  Epoch No:  18 \n",
            " Loss:  0.06864141846220491\n",
            "epoch no 18\n",
            "Training:  Epoch No:  19 \n",
            " Loss:  0.04397409955495299\n",
            "Validation:  Epoch No:  19 \n",
            " Loss:  0.06810668507574939\n",
            "epoch no 19\n",
            "Training:  Epoch No:  20 \n",
            " Loss:  0.047318830463943015\n",
            "Validation:  Epoch No:  20 \n",
            " Loss:  0.06464240635287524\n",
            "epoch no 20\n",
            "Training:  Epoch No:  21 \n",
            " Loss:  0.03384188272141937\n",
            "Validation:  Epoch No:  21 \n",
            " Loss:  0.06560190543714861\n",
            "epoch no 21\n",
            "Training:  Epoch No:  22 \n",
            " Loss:  0.04054795719430273\n",
            "Validation:  Epoch No:  22 \n",
            " Loss:  0.06243203429754032\n",
            "epoch no 22\n",
            "Training:  Epoch No:  23 \n",
            " Loss:  0.03784964978029947\n",
            "Validation:  Epoch No:  23 \n",
            " Loss:  0.060748144060034974\n",
            "epoch no 23\n",
            "Training:  Epoch No:  24 \n",
            " Loss:  0.03467459012045061\n",
            "Validation:  Epoch No:  24 \n",
            " Loss:  0.05932488919635193\n",
            "epoch no 24\n",
            "Training:  Epoch No:  25 \n",
            " Loss:  0.03405883918766055\n",
            "Validation:  Epoch No:  25 \n",
            " Loss:  0.056434649470051855\n",
            "epoch no 25\n",
            "Training:  Epoch No:  26 \n",
            " Loss:  0.034429338318329804\n",
            "Validation:  Epoch No:  26 \n",
            " Loss:  0.059644528062689176\n",
            "epoch no 26\n",
            "Training:  Epoch No:  27 \n",
            " Loss:  0.03378498113123489\n",
            "Validation:  Epoch No:  27 \n",
            " Loss:  0.05652246385412565\n",
            "epoch no 27\n",
            "Training:  Epoch No:  28 \n",
            " Loss:  0.02671646569592337\n",
            "Validation:  Epoch No:  28 \n",
            " Loss:  0.05660446170316874\n",
            "epoch no 28\n",
            "Training:  Epoch No:  29 \n",
            " Loss:  0.030005866939391727\n",
            "Validation:  Epoch No:  29 \n",
            " Loss:  0.05691680297130125\n",
            "epoch no 29\n",
            "Training:  Epoch No:  30 \n",
            " Loss:  0.0307747925065048\n",
            "Validation:  Epoch No:  30 \n",
            " Loss:  0.05470822020815433\n",
            "epoch no 30\n",
            "Training:  Epoch No:  31 \n",
            " Loss:  0.02805295930673306\n",
            "Validation:  Epoch No:  31 \n",
            " Loss:  0.05382935550593737\n",
            "epoch no 31\n",
            "Training:  Epoch No:  32 \n",
            " Loss:  0.030124572624927407\n",
            "Validation:  Epoch No:  32 \n",
            " Loss:  0.052392349169032855\n",
            "epoch no 32\n",
            "Training:  Epoch No:  33 \n",
            " Loss:  0.03211490638231482\n",
            "Validation:  Epoch No:  33 \n",
            " Loss:  0.05127365077207787\n",
            "epoch no 33\n",
            "Training:  Epoch No:  34 \n",
            " Loss:  0.02529576316507336\n",
            "Validation:  Epoch No:  34 \n",
            " Loss:  0.052387093028874405\n",
            "epoch no 34\n",
            "Training:  Epoch No:  35 \n",
            " Loss:  0.02807814408648324\n",
            "Validation:  Epoch No:  35 \n",
            " Loss:  0.052007568689213454\n",
            "epoch no 35\n",
            "Training:  Epoch No:  36 \n",
            " Loss:  0.02768651825474099\n",
            "Validation:  Epoch No:  36 \n",
            " Loss:  0.05064021098627353\n",
            "epoch no 36\n",
            "Training:  Epoch No:  37 \n",
            " Loss:  0.027307462542820914\n",
            "Validation:  Epoch No:  37 \n",
            " Loss:  0.05092743570172425\n",
            "epoch no 37\n",
            "Training:  Epoch No:  38 \n",
            " Loss:  0.024107411620442797\n",
            "Validation:  Epoch No:  38 \n",
            " Loss:  0.05073696045950976\n",
            "epoch no 38\n",
            "Training:  Epoch No:  39 \n",
            " Loss:  0.027003513145180142\n",
            "Validation:  Epoch No:  39 \n",
            " Loss:  0.0494388486135332\n",
            "epoch no 39\n",
            "Training:  Epoch No:  40 \n",
            " Loss:  0.020624790439274943\n",
            "Validation:  Epoch No:  40 \n",
            " Loss:  0.04960378046383835\n",
            "epoch no 40\n",
            "Training:  Epoch No:  41 \n",
            " Loss:  0.020773635444547946\n",
            "Validation:  Epoch No:  41 \n",
            " Loss:  0.05023442873114015\n",
            "epoch no 41\n",
            "Training:  Epoch No:  42 \n",
            " Loss:  0.024967381934962504\n",
            "Validation:  Epoch No:  42 \n",
            " Loss:  0.04893068439659061\n",
            "epoch no 42\n",
            "Training:  Epoch No:  43 \n",
            " Loss:  0.027658172280301745\n",
            "Validation:  Epoch No:  43 \n",
            " Loss:  0.04870678297672416\n",
            "epoch no 43\n",
            "Training:  Epoch No:  44 \n",
            " Loss:  0.025807836786590627\n",
            "Validation:  Epoch No:  44 \n",
            " Loss:  0.04754183290084688\n",
            "epoch no 44\n",
            "Training:  Epoch No:  45 \n",
            " Loss:  0.01875776797066671\n",
            "Validation:  Epoch No:  45 \n",
            " Loss:  0.048201964037479374\n",
            "epoch no 45\n",
            "Training:  Epoch No:  46 \n",
            " Loss:  0.02299612437555382\n",
            "Validation:  Epoch No:  46 \n",
            " Loss:  0.04691780796203923\n",
            "epoch no 46\n",
            "Training:  Epoch No:  47 \n",
            " Loss:  0.022201078839783762\n",
            "Validation:  Epoch No:  47 \n",
            " Loss:  0.04560698881443341\n",
            "epoch no 47\n",
            "Training:  Epoch No:  48 \n",
            " Loss:  0.024893476695276\n",
            "Validation:  Epoch No:  48 \n",
            " Loss:  0.04585940965114187\n",
            "epoch no 48\n",
            "Training:  Epoch No:  49 \n",
            " Loss:  0.02174135800147559\n",
            "Validation:  Epoch No:  49 \n",
            " Loss:  0.0440556855210607\n",
            "epoch no 49\n",
            "Training:  Epoch No:  50 \n",
            " Loss:  0.024936666668518317\n",
            "Validation:  Epoch No:  50 \n",
            " Loss:  0.04494241356141211\n",
            "epoch no 50\n",
            "Training:  Epoch No:  51 \n",
            " Loss:  0.01972641161406139\n",
            "Validation:  Epoch No:  51 \n",
            " Loss:  0.045499939377647465\n",
            "epoch no 51\n",
            "Training:  Epoch No:  52 \n",
            " Loss:  0.025590073672465775\n",
            "Validation:  Epoch No:  52 \n",
            " Loss:  0.046346059865513524\n",
            "epoch no 52\n",
            "Training:  Epoch No:  53 \n",
            " Loss:  0.01900033034751912\n",
            "Validation:  Epoch No:  53 \n",
            " Loss:  0.04546384775702117\n",
            "epoch no 53\n",
            "Training:  Epoch No:  54 \n",
            " Loss:  0.02204163763168594\n",
            "Validation:  Epoch No:  54 \n",
            " Loss:  0.04532582627766575\n",
            "epoch no 54\n",
            "Training:  Epoch No:  55 \n",
            " Loss:  0.02089176078207019\n",
            "Validation:  Epoch No:  55 \n",
            " Loss:  0.0432790730200509\n",
            "epoch no 55\n",
            "Training:  Epoch No:  56 \n",
            " Loss:  0.020204586614196787\n",
            "Validation:  Epoch No:  56 \n",
            " Loss:  0.045190856361554156\n",
            "epoch no 56\n",
            "Training:  Epoch No:  57 \n",
            " Loss:  0.023867171545527373\n",
            "Validation:  Epoch No:  57 \n",
            " Loss:  0.043747803607755174\n",
            "epoch no 57\n",
            "Training:  Epoch No:  58 \n",
            " Loss:  0.01795704136161788\n",
            "Validation:  Epoch No:  58 \n",
            " Loss:  0.044486971405041785\n",
            "epoch no 58\n",
            "Training:  Epoch No:  59 \n",
            " Loss:  0.016521283970269054\n",
            "Validation:  Epoch No:  59 \n",
            " Loss:  0.043330313428064385\n",
            "epoch no 59\n",
            "Training:  Epoch No:  60 \n",
            " Loss:  0.015761150615323447\n",
            "Validation:  Epoch No:  60 \n",
            " Loss:  0.043162665633873644\n",
            "epoch no 60\n",
            "Training:  Epoch No:  61 \n",
            " Loss:  0.018018692156259192\n",
            "Validation:  Epoch No:  61 \n",
            " Loss:  0.04348988374299044\n",
            "epoch no 61\n",
            "Training:  Epoch No:  62 \n",
            " Loss:  0.019623822171112214\n",
            "Validation:  Epoch No:  62 \n",
            " Loss:  0.044362897136287494\n",
            "epoch no 62\n",
            "Training:  Epoch No:  63 \n",
            " Loss:  0.020648891551180295\n",
            "Validation:  Epoch No:  63 \n",
            " Loss:  0.04427500843216555\n",
            "epoch no 63\n",
            "Training:  Epoch No:  64 \n",
            " Loss:  0.02006400168310604\n",
            "Validation:  Epoch No:  64 \n",
            " Loss:  0.04588930664828181\n",
            "epoch no 64\n",
            "Training:  Epoch No:  65 \n",
            " Loss:  0.0188966817195212\n",
            "Validation:  Epoch No:  65 \n",
            " Loss:  0.04551509252569815\n",
            "epoch no 65\n",
            "Training:  Epoch No:  66 \n",
            " Loss:  0.018694043931064296\n",
            "Validation:  Epoch No:  66 \n",
            " Loss:  0.0438489957734721\n",
            "epoch no 66\n",
            "Training:  Epoch No:  67 \n",
            " Loss:  0.01757036560320851\n",
            "Validation:  Epoch No:  67 \n",
            " Loss:  0.04463441171776717\n",
            "epoch no 67\n",
            "Training:  Epoch No:  68 \n",
            " Loss:  0.016062543925854572\n",
            "Validation:  Epoch No:  68 \n",
            " Loss:  0.044559106931434396\n",
            "epoch no 68\n",
            "Training:  Epoch No:  69 \n",
            " Loss:  0.01675120830648474\n",
            "Validation:  Epoch No:  69 \n",
            " Loss:  0.04367368740808549\n",
            "epoch no 69\n",
            "Training:  Epoch No:  70 \n",
            " Loss:  0.016601187066420442\n",
            "Validation:  Epoch No:  70 \n",
            " Loss:  0.042487969451632655\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(10)\n",
        "all_losses = {'train':[], 'test':[]}\n",
        "validation_accracy = []\n",
        "for epoch in range(num_epochs):\n",
        "    print('epoch no', epoch)\n",
        "\n",
        "    model_weights = copy.deepcopy(model.state_dict())\n",
        "    averaged_weights = {}\n",
        "    for layer in model_weights.keys():\n",
        "        averaged_weights[layer] = torch.zeros_like(model_weights[layer])\n",
        "\n",
        "    client_losses = []\n",
        "\n",
        "    for client in random.sample([i for i in range(num_users)], 10):\n",
        "\n",
        "        client_loss = 0\n",
        "        counter = 0\n",
        "\n",
        "        for local_epoch in range(10):\n",
        "            for batch_data, batch_labels in train_loaders[client]:\n",
        "                # Training\n",
        "                batch_data, batch_labels = batch_data.to(device), batch_labels.to(device)\n",
        "                \n",
        "                optimizer.zero_grad()\n",
        "                \n",
        "                batch_outputs = model(batch_data)        \n",
        "                loss = criterion(batch_outputs, batch_labels)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                client_loss += loss.item()\n",
        "                counter += 1\n",
        "            client_losses.append(client_loss / counter)    \n",
        "\n",
        "        for layer in model.state_dict().keys():\n",
        "            averaged_weights[layer] += 0.1 * copy.deepcopy(model.state_dict()[layer])\n",
        "\n",
        "        model.load_state_dict(model_weights)\n",
        "        \n",
        "    model.load_state_dict(averaged_weights)  \n",
        "\n",
        "    all_losses['train'].append(sum(client_losses) / len(client_losses))\n",
        "    print('Training: ', 'Epoch No: ', epoch+1,'\\n',\n",
        "            'Loss: ', all_losses['train'][-1])\n",
        "\n",
        "    # Validation\n",
        "    with torch.set_grad_enabled(False):\n",
        "        batch_losses = []\n",
        "        counter = 0\n",
        "        total = 0\n",
        "        for batch_data, batch_labels in test_loader:\n",
        "            total += len(batch_data)\n",
        "            batch_data, batch_labels = batch_data.to(device), batch_labels.to(device)\n",
        "            batch_outputs = model(batch_data)            \n",
        "            loss = criterion(batch_outputs, batch_labels)\n",
        "            batch_losses.append(loss.item())\n",
        "            \n",
        "            for i in range(len(batch_data)):\n",
        "                true = int(batch_labels[i])\n",
        "                pred = int(torch.argmax(batch_outputs[i,:]))\n",
        "                if pred == true:\n",
        "                    counter += 1\n",
        "\n",
        "        all_losses['test'].append(sum(batch_losses) / len(batch_losses))\n",
        "        print('Validation: ', 'Epoch No: ', epoch+1,'\\n',\n",
        "            'Loss: ', all_losses['test'][-1])\n",
        "        acc = (counter*100) / total   \n",
        "        validation_accracy.append(acc)    \n",
        "\n",
        "    if acc >= 99:\n",
        "        print('99 % accuracy reached!')\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "t0PE6TDVspAg",
        "outputId": "32f1bd44-eede-437f-c7df-7e9078121f80"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.plot(all_losses['train'], label='train')\n",
        "plt.plot(all_losses['test'], label='test')\n",
        "plt.legend()\n",
        "plt.xlabel('Rounds')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Loss VS Rounds');"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "8W5G1fv8spDV",
        "outputId": "854b1ba3-49d2-405f-d30c-c3c826fe7177"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.plot(validation_accracy)\n",
        "plt.xlabel('Rounds')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Test accuracy');"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V6-AVX6aspF6",
        "outputId": "1b7bdeb5-11b2-448d-f3a6-aa42bc1015a5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "98.75"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "max(validation_accracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kQyolwiA0GBK"
      },
      "source": [
        "# FedProx Algorithm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5x6cSZHKVq8r"
      },
      "source": [
        "## Data preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "7-N-BQwV0UqO"
      },
      "outputs": [],
      "source": [
        "num_users = 100  # number of devices\n",
        "num_labels = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "QDItFruXBAWn"
      },
      "outputs": [],
      "source": [
        "all_indices = [[] for i in range(10)]\n",
        "for i in range(len(mnist_trainset)):\n",
        "    label =  mnist_trainset.targets[i]\n",
        "    all_indices[label].append(i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Ph_4Y9NDgwxq"
      },
      "outputs": [],
      "source": [
        "partition_indices = {i:[] for i in range(100)}\n",
        "\n",
        "for label in range(10):\n",
        "    for user in range(num_users):  \n",
        "        if len(all_indices[label]) == 0:\n",
        "            break\n",
        "        elif len(partition_indices[user]) <= 300:    \n",
        "            partition_indices[user] += all_indices[label][0:300]\n",
        "            del all_indices[label][0:300]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "M6REoguWtvbx"
      },
      "outputs": [],
      "source": [
        "partition_indices[99] += all_indices[9][0:400]\n",
        "del all_indices[9][0:400]\n",
        "\n",
        "partition_indices[98] += all_indices[9][0:400]\n",
        "del all_indices[9][0:400]\n",
        "\n",
        "partition_indices[97] += all_indices[9][0:400]\n",
        "del all_indices[9][0:400]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "7RB-RiRi_nGf"
      },
      "outputs": [],
      "source": [
        "class mnist_non_iid_Dataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, indices):\n",
        "        self.indices = indices\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.indices)\n",
        "\n",
        "    def __getitem__(self,ind):\n",
        "        return  mnist_trainset[self.indices[ind]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "PXEEmrRI5nvP"
      },
      "outputs": [],
      "source": [
        "batch_size = 10\n",
        "train_loaders = []\n",
        "for i in range(num_users):\n",
        "    train_loaders.append(torch.utils.data.DataLoader(\n",
        "                 dataset=mnist_non_iid_Dataset(partition_indices[i]),\n",
        "                 batch_size=batch_size,\n",
        "                 shuffle=True\n",
        "                 ))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "HGYF3NGUm_AN"
      },
      "outputs": [],
      "source": [
        "test_loader = torch.utils.data.DataLoader(\n",
        "                dataset=mnist_testset,\n",
        "                batch_size=batch_size,\n",
        "                shuffle=False\n",
        "                )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MNm0T2clYSkq"
      },
      "source": [
        "## Define loss function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "JP3I-pzy5n-C"
      },
      "outputs": [],
      "source": [
        "def loss_function(global_weights, local_weights, net_output, batch_labels):\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    loss1  = criterion(net_output, batch_labels)\n",
        "    loss2 = 0\n",
        "    for layer in global_weights.keys():\n",
        "        loss2 += (LA.matrix_norm(global_weights[layer].flatten().unsqueeze(-1) - local_weights[layer].flatten().unsqueeze(-1))) **2\n",
        "\n",
        "    return loss1 + (mu/2) * (loss2)    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "72VVm-4tI1CR"
      },
      "outputs": [],
      "source": [
        "model = CNN().to(device)\n",
        "optimizer = optim.SGD(model.parameters(), lr = 0.1) \n",
        "criterion = nn.CrossEntropyLoss()\n",
        "num_epochs = 300\n",
        "mu = 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YHr2VwkiKQ-v",
        "outputId": "a880457f-ca27-40aa-fcf9-4de64f548edd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch no 0\n",
            "Training:  Epoch No:  1 \n",
            " Loss:  5.726013383017923\n",
            "Validation:  Epoch No:  1 \n",
            " Loss:  2.049858866095543\n",
            "validation accuracy:  23.64\n",
            "epoch no 1\n",
            "Training:  Epoch No:  2 \n",
            " Loss:  3.29981167709613\n",
            "Validation:  Epoch No:  2 \n",
            " Loss:  1.653504260957241\n",
            "validation accuracy:  50.91\n",
            "epoch no 2\n",
            "Training:  Epoch No:  3 \n",
            " Loss:  2.8541637406039717\n",
            "Validation:  Epoch No:  3 \n",
            " Loss:  1.501644079685211\n",
            "validation accuracy:  49.73\n",
            "epoch no 3\n",
            "Training:  Epoch No:  4 \n",
            " Loss:  0.9965762579271363\n",
            "Validation:  Epoch No:  4 \n",
            " Loss:  1.3539879039227962\n",
            "validation accuracy:  50.49\n",
            "epoch no 4\n",
            "Training:  Epoch No:  5 \n",
            " Loss:  2.6306469163230677\n",
            "Validation:  Epoch No:  5 \n",
            " Loss:  2.882657174766064\n",
            "validation accuracy:  42.85\n",
            "epoch no 5\n",
            "Training:  Epoch No:  6 \n",
            " Loss:  4.8553454745129905\n",
            "Validation:  Epoch No:  6 \n",
            " Loss:  0.759196814775467\n",
            "validation accuracy:  73.47\n",
            "epoch no 6\n",
            "Training:  Epoch No:  7 \n",
            " Loss:  1.20967754904098\n",
            "Validation:  Epoch No:  7 \n",
            " Loss:  0.5411264705248177\n",
            "validation accuracy:  81.03\n",
            "epoch no 7\n",
            "Training:  Epoch No:  8 \n",
            " Loss:  1.530569645752504\n",
            "Validation:  Epoch No:  8 \n",
            " Loss:  0.6498169747889042\n",
            "validation accuracy:  77.22\n",
            "epoch no 8\n",
            "Training:  Epoch No:  9 \n",
            " Loss:  2.4864121508689094\n",
            "Validation:  Epoch No:  9 \n",
            " Loss:  0.40790402983129026\n",
            "validation accuracy:  86.73\n",
            "epoch no 9\n",
            "Training:  Epoch No:  10 \n",
            " Loss:  0.9145040300372574\n",
            "Validation:  Epoch No:  10 \n",
            " Loss:  0.35660849772579967\n",
            "validation accuracy:  88.59\n",
            "epoch no 10\n",
            "Training:  Epoch No:  11 \n",
            " Loss:  0.9071266912900315\n",
            "Validation:  Epoch No:  11 \n",
            " Loss:  0.34276509466208516\n",
            "validation accuracy:  88.71\n",
            "epoch no 11\n",
            "Training:  Epoch No:  12 \n",
            " Loss:  0.8780551890599791\n",
            "Validation:  Epoch No:  12 \n",
            " Loss:  0.3575617983788252\n",
            "validation accuracy:  88.03\n",
            "epoch no 12\n",
            "Training:  Epoch No:  13 \n",
            " Loss:  0.9772491039529181\n",
            "Validation:  Epoch No:  13 \n",
            " Loss:  0.3042266664542258\n",
            "validation accuracy:  90.17\n",
            "epoch no 13\n",
            "Training:  Epoch No:  14 \n",
            " Loss:  0.7417856930409455\n",
            "Validation:  Epoch No:  14 \n",
            " Loss:  0.40990347560308876\n",
            "validation accuracy:  86.15\n",
            "epoch no 14\n",
            "Training:  Epoch No:  15 \n",
            " Loss:  1.202546048238568\n",
            "Validation:  Epoch No:  15 \n",
            " Loss:  0.3366192638669163\n",
            "validation accuracy:  86.92\n",
            "epoch no 15\n",
            "Training:  Epoch No:  16 \n",
            " Loss:  1.5018304850005855\n",
            "Validation:  Epoch No:  16 \n",
            " Loss:  0.7184804042354226\n",
            "validation accuracy:  77.93\n",
            "epoch no 16\n",
            "Training:  Epoch No:  17 \n",
            " Loss:  2.1354409208827554\n",
            "Validation:  Epoch No:  17 \n",
            " Loss:  0.461679374746047\n",
            "validation accuracy:  85.22\n",
            "epoch no 17\n",
            "Training:  Epoch No:  18 \n",
            " Loss:  0.9916728589787253\n",
            "Validation:  Epoch No:  18 \n",
            " Loss:  0.22786831160355359\n",
            "validation accuracy:  92.85\n",
            "epoch no 18\n",
            "Training:  Epoch No:  19 \n",
            " Loss:  0.8343107930128605\n",
            "Validation:  Epoch No:  19 \n",
            " Loss:  0.2938660869654268\n",
            "validation accuracy:  89.01\n",
            "epoch no 19\n",
            "Training:  Epoch No:  20 \n",
            " Loss:  1.6499133179980632\n",
            "Validation:  Epoch No:  20 \n",
            " Loss:  0.2474009696217254\n",
            "validation accuracy:  90.93\n",
            "epoch no 20\n",
            "Training:  Epoch No:  21 \n",
            " Loss:  1.0654604997187853\n",
            "Validation:  Epoch No:  21 \n",
            " Loss:  0.23882674278365448\n",
            "validation accuracy:  92.03\n",
            "epoch no 21\n",
            "Training:  Epoch No:  22 \n",
            " Loss:  0.9047020449163311\n",
            "Validation:  Epoch No:  22 \n",
            " Loss:  0.340480184270069\n",
            "validation accuracy:  88.06\n",
            "epoch no 22\n",
            "Training:  Epoch No:  23 \n",
            " Loss:  1.4242648565040694\n",
            "Validation:  Epoch No:  23 \n",
            " Loss:  0.1734534325709101\n",
            "validation accuracy:  94.36\n",
            "epoch no 23\n",
            "Training:  Epoch No:  24 \n",
            " Loss:  0.7368341904151954\n",
            "Validation:  Epoch No:  24 \n",
            " Loss:  0.24404516885231714\n",
            "validation accuracy:  91.75\n",
            "epoch no 24\n",
            "Training:  Epoch No:  25 \n",
            " Loss:  1.0306539819372496\n",
            "Validation:  Epoch No:  25 \n",
            " Loss:  0.2960766107458621\n",
            "validation accuracy:  89.28\n",
            "epoch no 25\n",
            "Training:  Epoch No:  26 \n",
            " Loss:  0.690546071844693\n",
            "Validation:  Epoch No:  26 \n",
            " Loss:  0.24865419576410205\n",
            "validation accuracy:  91.18\n",
            "epoch no 26\n",
            "Training:  Epoch No:  27 \n",
            " Loss:  1.39511751586156\n",
            "Validation:  Epoch No:  27 \n",
            " Loss:  0.24913571218919242\n",
            "validation accuracy:  90.74\n",
            "epoch no 27\n",
            "Training:  Epoch No:  28 \n",
            " Loss:  3.4618896605283864\n",
            "Validation:  Epoch No:  28 \n",
            " Loss:  0.5953499005376361\n",
            "validation accuracy:  81.97\n",
            "epoch no 28\n",
            "Training:  Epoch No:  29 \n",
            " Loss:  3.0309844735411873\n",
            "Validation:  Epoch No:  29 \n",
            " Loss:  0.24499104905989952\n",
            "validation accuracy:  92.09\n",
            "epoch no 29\n",
            "Training:  Epoch No:  30 \n",
            " Loss:  0.9741749853408087\n",
            "Validation:  Epoch No:  30 \n",
            " Loss:  0.1408913092177827\n",
            "validation accuracy:  95.41\n",
            "95 % accuracy reached!\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(10)\n",
        "all_losses = {'train':[], 'test':[]}\n",
        "validation_accracy = []\n",
        "for epoch in range(num_epochs):\n",
        "    print('epoch no', epoch)\n",
        "\n",
        "    model_weights = copy.deepcopy(model.state_dict())\n",
        "    averaged_weights = {}\n",
        "    for layer in model_weights.keys():\n",
        "        averaged_weights[layer] = torch.zeros_like(model_weights[layer])\n",
        "\n",
        "    client_losses = []\n",
        "\n",
        "    random_clients = random.sample([i for i in range(num_users)], 10)\n",
        "    total_samples = 0\n",
        "    for element in random_clients:\n",
        "        total_samples += len(train_loaders[element])\n",
        "\n",
        "    freq = {}\n",
        "    for element in random_clients:\n",
        "        freq[element] = len(train_loaders[element]) / total_samples\n",
        "\n",
        "    for client in random_clients:\n",
        "\n",
        "        client_loss = 0\n",
        "        counter = 0\n",
        "\n",
        "        for local_epoch in range(5):\n",
        "            for batch_data, batch_labels in train_loaders[client]:\n",
        "                # Training\n",
        "                batch_data, batch_labels = batch_data.to(device), batch_labels.to(device)\n",
        "                \n",
        "                optimizer.zero_grad()\n",
        "                \n",
        "                batch_outputs = model(batch_data)        \n",
        "                loss = loss_function(model_weights, model.state_dict(), batch_outputs, batch_labels)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                client_loss += loss.item()\n",
        "                counter += 1\n",
        "            client_losses.append(client_loss / counter)    \n",
        "        \n",
        "        for layer in model.state_dict().keys():\n",
        "            averaged_weights[layer] += freq[client] * copy.deepcopy(model.state_dict()[layer])\n",
        "\n",
        "        model.load_state_dict(model_weights)\n",
        "        \n",
        "    model.load_state_dict(averaged_weights)  \n",
        "\n",
        "    all_losses['train'].append(sum([a*b for a,b in zip(client_losses,list(freq.values()))]))\n",
        "    \n",
        "\n",
        "    print('Training: ', 'Epoch No: ', epoch+1,'\\n',\n",
        "            'Loss: ', all_losses['train'][-1])\n",
        "\n",
        "\n",
        "    # Validation\n",
        "    with torch.set_grad_enabled(False):\n",
        "        batch_losses = []\n",
        "        counter = 0\n",
        "        total = 0\n",
        "        for batch_data, batch_labels in test_loader:\n",
        "            total += len(batch_data)\n",
        "            batch_data, batch_labels = batch_data.to(device), batch_labels.to(device)\n",
        "            batch_outputs = model(batch_data)            \n",
        "            loss = criterion(batch_outputs, batch_labels)\n",
        "            batch_losses.append(loss.item())\n",
        "            \n",
        "            for i in range(len(batch_data)):\n",
        "                true = int(batch_labels[i])\n",
        "                pred = int(torch.argmax(batch_outputs[i,:]))\n",
        "                if pred == true:\n",
        "                    counter += 1\n",
        "\n",
        "        all_losses['test'].append(sum(batch_losses) / len(batch_losses))\n",
        "        print('Validation: ', 'Epoch No: ', epoch+1,'\\n',\n",
        "            'Loss: ', all_losses['test'][-1])\n",
        "        acc = (counter*100) / total   \n",
        "        validation_accracy.append(acc)    \n",
        "        print('validation accuracy: ', acc)\n",
        "\n",
        "    if acc >= 95:\n",
        "        print('95 % accuracy reached!')\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "PIrMVdElKlqA",
        "outputId": "e781ee05-1320-4984-a340-a390b21e7ba8"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.plot(all_losses['train'], label='train')\n",
        "plt.plot(all_losses['test'], label='test')\n",
        "plt.legend()\n",
        "plt.xlabel('Rounds')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Loss VS Rounds');"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "YezoZQIab4_F",
        "outputId": "3bb4e9cb-14d2-4b73-ce49-cb9f5a6daf6e"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.plot(validation_accracy)\n",
        "plt.xlabel('Rounds')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Test accuracy');"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rDNEzqSAIaTP",
        "outputId": "0639feea-8e11-4558-91ce-c42cdd4e3ac9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "95.39"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "max(validation_accracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pJB_OG5iTdnC"
      },
      "source": [
        "## learning rate = 0.01"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "Y4zn5YNbTgV7"
      },
      "outputs": [],
      "source": [
        "model = CNN().to(device)\n",
        "optimizer = optim.SGD(model.parameters(), lr = 0.01) \n",
        "criterion = nn.CrossEntropyLoss()\n",
        "num_epochs = 300\n",
        "mu = 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "d3Fni9ERTpY-",
        "outputId": "3e203766-1cd5-46e1-934f-ce918d5aa9b1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch no 0\n",
            "Training:  Epoch No:  1 \n",
            " Loss:  1.695860435234176\n",
            "Validation:  Epoch No:  1 \n",
            " Loss:  3.080661289334297\n",
            "validation accuracy:  19.27\n",
            "epoch no 1\n",
            "Training:  Epoch No:  2 \n",
            " Loss:  0.6596546377729176\n",
            "Validation:  Epoch No:  2 \n",
            " Loss:  2.795608528137207\n",
            "validation accuracy:  30.51\n",
            "epoch no 2\n",
            "Training:  Epoch No:  3 \n",
            " Loss:  0.6992351052195074\n",
            "Validation:  Epoch No:  3 \n",
            " Loss:  1.5260534834861756\n",
            "validation accuracy:  49.08\n",
            "epoch no 3\n",
            "Training:  Epoch No:  4 \n",
            " Loss:  0.6101997790354732\n",
            "Validation:  Epoch No:  4 \n",
            " Loss:  1.5045503205657005\n",
            "validation accuracy:  47.98\n",
            "epoch no 4\n",
            "Training:  Epoch No:  5 \n",
            " Loss:  0.25525319460358464\n",
            "Validation:  Epoch No:  5 \n",
            " Loss:  1.2156576247513293\n",
            "validation accuracy:  55.53\n",
            "epoch no 5\n",
            "Training:  Epoch No:  6 \n",
            " Loss:  0.5795076310308651\n",
            "Validation:  Epoch No:  6 \n",
            " Loss:  1.141354898005724\n",
            "validation accuracy:  56.09\n",
            "epoch no 6\n",
            "Training:  Epoch No:  7 \n",
            " Loss:  0.23920086135353066\n",
            "Validation:  Epoch No:  7 \n",
            " Loss:  1.314440350741148\n",
            "validation accuracy:  52.4\n",
            "epoch no 7\n",
            "Training:  Epoch No:  8 \n",
            " Loss:  0.18222362318770807\n",
            "Validation:  Epoch No:  8 \n",
            " Loss:  0.9256639916449785\n",
            "validation accuracy:  65.33\n",
            "epoch no 8\n",
            "Training:  Epoch No:  9 \n",
            " Loss:  0.25224739090185294\n",
            "Validation:  Epoch No:  9 \n",
            " Loss:  1.274728786468506\n",
            "validation accuracy:  50.35\n",
            "epoch no 9\n",
            "Training:  Epoch No:  10 \n",
            " Loss:  0.24604883161104385\n",
            "Validation:  Epoch No:  10 \n",
            " Loss:  0.7991485082507134\n",
            "validation accuracy:  74.91\n",
            "epoch no 10\n",
            "Training:  Epoch No:  11 \n",
            " Loss:  0.1538325887689781\n",
            "Validation:  Epoch No:  11 \n",
            " Loss:  0.7639755010902881\n",
            "validation accuracy:  74.01\n",
            "epoch no 11\n",
            "Training:  Epoch No:  12 \n",
            " Loss:  0.19314779439455076\n",
            "Validation:  Epoch No:  12 \n",
            " Loss:  0.9248428270518779\n",
            "validation accuracy:  64.19\n",
            "epoch no 12\n",
            "Training:  Epoch No:  13 \n",
            " Loss:  0.22140318305573117\n",
            "Validation:  Epoch No:  13 \n",
            " Loss:  0.7088055412247777\n",
            "validation accuracy:  74.88\n",
            "epoch no 13\n",
            "Training:  Epoch No:  14 \n",
            " Loss:  0.28953257520263564\n",
            "Validation:  Epoch No:  14 \n",
            " Loss:  1.1145058766901492\n",
            "validation accuracy:  61.09\n",
            "epoch no 14\n",
            "Training:  Epoch No:  15 \n",
            " Loss:  0.21475667786085978\n",
            "Validation:  Epoch No:  15 \n",
            " Loss:  0.7770663109123707\n",
            "validation accuracy:  71.63\n",
            "epoch no 15\n",
            "Training:  Epoch No:  16 \n",
            " Loss:  0.12660376105742974\n",
            "Validation:  Epoch No:  16 \n",
            " Loss:  0.876613096781075\n",
            "validation accuracy:  69.18\n",
            "epoch no 16\n",
            "Training:  Epoch No:  17 \n",
            " Loss:  0.0571068375095413\n",
            "Validation:  Epoch No:  17 \n",
            " Loss:  0.7629161154786125\n",
            "validation accuracy:  73.56\n",
            "epoch no 17\n",
            "Training:  Epoch No:  18 \n",
            " Loss:  0.23207573752088997\n",
            "Validation:  Epoch No:  18 \n",
            " Loss:  0.9980570063292981\n",
            "validation accuracy:  65.53\n",
            "epoch no 18\n",
            "Training:  Epoch No:  19 \n",
            " Loss:  0.15335492310211435\n",
            "Validation:  Epoch No:  19 \n",
            " Loss:  0.6591242601424455\n",
            "validation accuracy:  75.93\n",
            "epoch no 19\n",
            "Training:  Epoch No:  20 \n",
            " Loss:  0.04223365550096787\n",
            "Validation:  Epoch No:  20 \n",
            " Loss:  0.8965682137981057\n",
            "validation accuracy:  65.39\n",
            "epoch no 20\n",
            "Training:  Epoch No:  21 \n",
            " Loss:  0.0606478427487935\n",
            "Validation:  Epoch No:  21 \n",
            " Loss:  0.6580516965501011\n",
            "validation accuracy:  78.2\n",
            "epoch no 21\n",
            "Training:  Epoch No:  22 \n",
            " Loss:  0.06749426180421789\n",
            "Validation:  Epoch No:  22 \n",
            " Loss:  0.6817783449850976\n",
            "validation accuracy:  76.39\n",
            "epoch no 22\n",
            "Training:  Epoch No:  23 \n",
            " Loss:  0.07673632368452307\n",
            "Validation:  Epoch No:  23 \n",
            " Loss:  0.5734428889304399\n",
            "validation accuracy:  80.66\n",
            "epoch no 23\n",
            "Training:  Epoch No:  24 \n",
            " Loss:  0.06200620496157377\n",
            "Validation:  Epoch No:  24 \n",
            " Loss:  0.6311507617607712\n",
            "validation accuracy:  77.31\n",
            "epoch no 24\n",
            "Training:  Epoch No:  25 \n",
            " Loss:  0.09681992796052741\n",
            "Validation:  Epoch No:  25 \n",
            " Loss:  0.498101782720536\n",
            "validation accuracy:  83.37\n",
            "epoch no 25\n",
            "Training:  Epoch No:  26 \n",
            " Loss:  0.15292644914394865\n",
            "Validation:  Epoch No:  26 \n",
            " Loss:  0.471680428378284\n",
            "validation accuracy:  83.17\n",
            "epoch no 26\n",
            "Training:  Epoch No:  27 \n",
            " Loss:  0.13842123462911698\n",
            "Validation:  Epoch No:  27 \n",
            " Loss:  0.7546388331502676\n",
            "validation accuracy:  70.41\n",
            "epoch no 27\n",
            "Training:  Epoch No:  28 \n",
            " Loss:  0.1467414566262021\n",
            "Validation:  Epoch No:  28 \n",
            " Loss:  0.7841986115379259\n",
            "validation accuracy:  72.04\n",
            "epoch no 28\n",
            "Training:  Epoch No:  29 \n",
            " Loss:  0.08591512129489058\n",
            "Validation:  Epoch No:  29 \n",
            " Loss:  0.5588019641600549\n",
            "validation accuracy:  78.9\n",
            "epoch no 29\n",
            "Training:  Epoch No:  30 \n",
            " Loss:  0.16783254764389455\n",
            "Validation:  Epoch No:  30 \n",
            " Loss:  0.4146140689123422\n",
            "validation accuracy:  85.88\n",
            "epoch no 30\n",
            "Training:  Epoch No:  31 \n",
            " Loss:  0.09086205066150675\n",
            "Validation:  Epoch No:  31 \n",
            " Loss:  0.41825400735251606\n",
            "validation accuracy:  86.29\n",
            "epoch no 31\n",
            "Training:  Epoch No:  32 \n",
            " Loss:  0.12697442769121575\n",
            "Validation:  Epoch No:  32 \n",
            " Loss:  0.5896375090517104\n",
            "validation accuracy:  77.24\n",
            "epoch no 32\n",
            "Training:  Epoch No:  33 \n",
            " Loss:  0.09251904859890539\n",
            "Validation:  Epoch No:  33 \n",
            " Loss:  0.5427594783436507\n",
            "validation accuracy:  81.43\n",
            "epoch no 33\n",
            "Training:  Epoch No:  34 \n",
            " Loss:  0.13635540892121098\n",
            "Validation:  Epoch No:  34 \n",
            " Loss:  0.5403927729949355\n",
            "validation accuracy:  81.37\n",
            "epoch no 34\n",
            "Training:  Epoch No:  35 \n",
            " Loss:  0.1788545164516246\n",
            "Validation:  Epoch No:  35 \n",
            " Loss:  0.4398493872359395\n",
            "validation accuracy:  85.7\n",
            "epoch no 35\n",
            "Training:  Epoch No:  36 \n",
            " Loss:  0.06453434383694345\n",
            "Validation:  Epoch No:  36 \n",
            " Loss:  0.40797295555099844\n",
            "validation accuracy:  85.86\n",
            "epoch no 36\n",
            "Training:  Epoch No:  37 \n",
            " Loss:  0.12543693624498944\n",
            "Validation:  Epoch No:  37 \n",
            " Loss:  0.5140115216155536\n",
            "validation accuracy:  81.27\n",
            "epoch no 37\n",
            "Training:  Epoch No:  38 \n",
            " Loss:  0.06475073540801256\n",
            "Validation:  Epoch No:  38 \n",
            " Loss:  0.7779241839819587\n",
            "validation accuracy:  78.12\n",
            "epoch no 38\n",
            "Training:  Epoch No:  39 \n",
            " Loss:  0.09888773859478533\n",
            "Validation:  Epoch No:  39 \n",
            " Loss:  0.5008783255787567\n",
            "validation accuracy:  80.44\n",
            "epoch no 39\n",
            "Training:  Epoch No:  40 \n",
            " Loss:  0.09964012048666518\n",
            "Validation:  Epoch No:  40 \n",
            " Loss:  0.4477692250646651\n",
            "validation accuracy:  83.43\n",
            "epoch no 40\n",
            "Training:  Epoch No:  41 \n",
            " Loss:  0.20713248352269237\n",
            "Validation:  Epoch No:  41 \n",
            " Loss:  0.4130236252276227\n",
            "validation accuracy:  85.07\n",
            "epoch no 41\n",
            "Training:  Epoch No:  42 \n",
            " Loss:  0.07535874771707328\n",
            "Validation:  Epoch No:  42 \n",
            " Loss:  0.5461334675420075\n",
            "validation accuracy:  80.55\n",
            "epoch no 42\n",
            "Training:  Epoch No:  43 \n",
            " Loss:  0.1540922963952439\n",
            "Validation:  Epoch No:  43 \n",
            " Loss:  0.356234944276046\n",
            "validation accuracy:  87.99\n",
            "epoch no 43\n",
            "Training:  Epoch No:  44 \n",
            " Loss:  0.07961848432430998\n",
            "Validation:  Epoch No:  44 \n",
            " Loss:  0.36596902408869936\n",
            "validation accuracy:  86.66\n",
            "epoch no 44\n",
            "Training:  Epoch No:  45 \n",
            " Loss:  0.06824778477125984\n",
            "Validation:  Epoch No:  45 \n",
            " Loss:  0.31355165971349924\n",
            "validation accuracy:  89.29\n",
            "epoch no 45\n",
            "Training:  Epoch No:  46 \n",
            " Loss:  0.07353888533384322\n",
            "Validation:  Epoch No:  46 \n",
            " Loss:  0.41726557656936347\n",
            "validation accuracy:  85.86\n",
            "epoch no 46\n",
            "Training:  Epoch No:  47 \n",
            " Loss:  0.10574464279234172\n",
            "Validation:  Epoch No:  47 \n",
            " Loss:  0.29786235508229586\n",
            "validation accuracy:  90.22\n",
            "epoch no 47\n",
            "Training:  Epoch No:  48 \n",
            " Loss:  0.14266619756150373\n",
            "Validation:  Epoch No:  48 \n",
            " Loss:  0.36224816894810646\n",
            "validation accuracy:  87.1\n",
            "epoch no 48\n",
            "Training:  Epoch No:  49 \n",
            " Loss:  0.07366009883980992\n",
            "Validation:  Epoch No:  49 \n",
            " Loss:  0.3147749126823619\n",
            "validation accuracy:  89.01\n",
            "epoch no 49\n",
            "Training:  Epoch No:  50 \n",
            " Loss:  0.06707622576425346\n",
            "Validation:  Epoch No:  50 \n",
            " Loss:  0.33536403736146164\n",
            "validation accuracy:  87.78\n",
            "epoch no 50\n",
            "Training:  Epoch No:  51 \n",
            " Loss:  0.06357812036325534\n",
            "Validation:  Epoch No:  51 \n",
            " Loss:  0.5074684142568149\n",
            "validation accuracy:  81.65\n",
            "epoch no 51\n",
            "Training:  Epoch No:  52 \n",
            " Loss:  0.06902517911924505\n",
            "Validation:  Epoch No:  52 \n",
            " Loss:  0.6186811949791153\n",
            "validation accuracy:  80.98\n",
            "epoch no 52\n",
            "Training:  Epoch No:  53 \n",
            " Loss:  0.10256086584770431\n",
            "Validation:  Epoch No:  53 \n",
            " Loss:  0.6483002924690955\n",
            "validation accuracy:  81.8\n",
            "epoch no 53\n",
            "Training:  Epoch No:  54 \n",
            " Loss:  0.12748618308583567\n",
            "Validation:  Epoch No:  54 \n",
            " Loss:  0.3784891418791376\n",
            "validation accuracy:  86.46\n",
            "epoch no 54\n",
            "Training:  Epoch No:  55 \n",
            " Loss:  0.08032317430448409\n",
            "Validation:  Epoch No:  55 \n",
            " Loss:  0.3873066916926764\n",
            "validation accuracy:  86.13\n",
            "epoch no 55\n",
            "Training:  Epoch No:  56 \n",
            " Loss:  0.024726620258488808\n",
            "Validation:  Epoch No:  56 \n",
            " Loss:  0.5150239066081121\n",
            "validation accuracy:  80.99\n",
            "epoch no 56\n",
            "Training:  Epoch No:  57 \n",
            " Loss:  0.030198068446671285\n",
            "Validation:  Epoch No:  57 \n",
            " Loss:  0.40072117769462057\n",
            "validation accuracy:  86.01\n",
            "epoch no 57\n",
            "Training:  Epoch No:  58 \n",
            " Loss:  0.04240955916435148\n",
            "Validation:  Epoch No:  58 \n",
            " Loss:  0.37502240769565104\n",
            "validation accuracy:  86.6\n",
            "epoch no 58\n",
            "Training:  Epoch No:  59 \n",
            " Loss:  0.1698513805478276\n",
            "Validation:  Epoch No:  59 \n",
            " Loss:  0.3582458392661065\n",
            "validation accuracy:  87.31\n",
            "epoch no 59\n",
            "Training:  Epoch No:  60 \n",
            " Loss:  0.07984396421840369\n",
            "Validation:  Epoch No:  60 \n",
            " Loss:  0.4392490390064195\n",
            "validation accuracy:  84.67\n",
            "epoch no 60\n",
            "Training:  Epoch No:  61 \n",
            " Loss:  0.19459036874564514\n",
            "Validation:  Epoch No:  61 \n",
            " Loss:  0.5671175776915625\n",
            "validation accuracy:  80.35\n",
            "epoch no 61\n",
            "Training:  Epoch No:  62 \n",
            " Loss:  0.1553405195003013\n",
            "Validation:  Epoch No:  62 \n",
            " Loss:  0.46948007177142426\n",
            "validation accuracy:  83.61\n",
            "epoch no 62\n",
            "Training:  Epoch No:  63 \n",
            " Loss:  0.03147770756700811\n",
            "Validation:  Epoch No:  63 \n",
            " Loss:  0.4998970914101228\n",
            "validation accuracy:  82.28\n",
            "epoch no 63\n",
            "Training:  Epoch No:  64 \n",
            " Loss:  0.1301492112526515\n",
            "Validation:  Epoch No:  64 \n",
            " Loss:  0.330858500872273\n",
            "validation accuracy:  88.68\n",
            "epoch no 64\n",
            "Training:  Epoch No:  65 \n",
            " Loss:  0.09295799062517152\n",
            "Validation:  Epoch No:  65 \n",
            " Loss:  0.26617020383942874\n",
            "validation accuracy:  91.11\n",
            "epoch no 65\n",
            "Training:  Epoch No:  66 \n",
            " Loss:  0.10804044115433778\n",
            "Validation:  Epoch No:  66 \n",
            " Loss:  0.2803396021919325\n",
            "validation accuracy:  89.95\n",
            "epoch no 66\n",
            "Training:  Epoch No:  67 \n",
            " Loss:  0.13986775150025885\n",
            "Validation:  Epoch No:  67 \n",
            " Loss:  0.2589924005395733\n",
            "validation accuracy:  90.54\n",
            "epoch no 67\n",
            "Training:  Epoch No:  68 \n",
            " Loss:  0.06881315395855928\n",
            "Validation:  Epoch No:  68 \n",
            " Loss:  0.28233209209376947\n",
            "validation accuracy:  89.86\n",
            "epoch no 68\n",
            "Training:  Epoch No:  69 \n",
            " Loss:  0.056544585758715474\n",
            "Validation:  Epoch No:  69 \n",
            " Loss:  0.2547318159211427\n",
            "validation accuracy:  91.72\n",
            "epoch no 69\n",
            "Training:  Epoch No:  70 \n",
            " Loss:  0.055447103442825316\n",
            "Validation:  Epoch No:  70 \n",
            " Loss:  0.29499059889046475\n",
            "validation accuracy:  90.07\n",
            "epoch no 70\n",
            "Training:  Epoch No:  71 \n",
            " Loss:  0.03026203291483232\n",
            "Validation:  Epoch No:  71 \n",
            " Loss:  0.26841649745590984\n",
            "validation accuracy:  91.21\n",
            "epoch no 71\n",
            "Training:  Epoch No:  72 \n",
            " Loss:  0.05858869468785512\n",
            "Validation:  Epoch No:  72 \n",
            " Loss:  0.2995254411720671\n",
            "validation accuracy:  89.8\n",
            "epoch no 72\n",
            "Training:  Epoch No:  73 \n",
            " Loss:  0.08522584354891447\n",
            "Validation:  Epoch No:  73 \n",
            " Loss:  0.27748345855530354\n",
            "validation accuracy:  89.8\n",
            "epoch no 73\n",
            "Training:  Epoch No:  74 \n",
            " Loss:  0.05083089947025292\n",
            "Validation:  Epoch No:  74 \n",
            " Loss:  0.4125818548901007\n",
            "validation accuracy:  84.23\n",
            "epoch no 74\n",
            "Training:  Epoch No:  75 \n",
            " Loss:  0.06933105257146731\n",
            "Validation:  Epoch No:  75 \n",
            " Loss:  0.35261152374208904\n",
            "validation accuracy:  87.82\n",
            "epoch no 75\n",
            "Training:  Epoch No:  76 \n",
            " Loss:  0.13081191221626465\n",
            "Validation:  Epoch No:  76 \n",
            " Loss:  0.39452305088471623\n",
            "validation accuracy:  86.58\n",
            "epoch no 76\n",
            "Training:  Epoch No:  77 \n",
            " Loss:  0.1863041230576028\n",
            "Validation:  Epoch No:  77 \n",
            " Loss:  0.2712282000135165\n",
            "validation accuracy:  90.42\n",
            "epoch no 77\n",
            "Training:  Epoch No:  78 \n",
            " Loss:  0.07253120471096029\n",
            "Validation:  Epoch No:  78 \n",
            " Loss:  0.2646855810061097\n",
            "validation accuracy:  90.95\n",
            "epoch no 78\n",
            "Training:  Epoch No:  79 \n",
            " Loss:  0.07295578672170364\n",
            "Validation:  Epoch No:  79 \n",
            " Loss:  0.4409255758449435\n",
            "validation accuracy:  85.01\n",
            "epoch no 79\n",
            "Training:  Epoch No:  80 \n",
            " Loss:  0.062023779179435225\n",
            "Validation:  Epoch No:  80 \n",
            " Loss:  0.22790804958553054\n",
            "validation accuracy:  92.28\n",
            "epoch no 80\n",
            "Training:  Epoch No:  81 \n",
            " Loss:  0.13447087800910162\n",
            "Validation:  Epoch No:  81 \n",
            " Loss:  0.23823014850798063\n",
            "validation accuracy:  91.83\n",
            "epoch no 81\n",
            "Training:  Epoch No:  82 \n",
            " Loss:  0.044987615802273215\n",
            "Validation:  Epoch No:  82 \n",
            " Loss:  0.25104211805667725\n",
            "validation accuracy:  91.76\n",
            "epoch no 82\n",
            "Training:  Epoch No:  83 \n",
            " Loss:  0.08074465331892108\n",
            "Validation:  Epoch No:  83 \n",
            " Loss:  0.23684765195823274\n",
            "validation accuracy:  92.03\n",
            "epoch no 83\n",
            "Training:  Epoch No:  84 \n",
            " Loss:  0.08250624393531765\n",
            "Validation:  Epoch No:  84 \n",
            " Loss:  0.23506135124363936\n",
            "validation accuracy:  92.32\n",
            "epoch no 84\n",
            "Training:  Epoch No:  85 \n",
            " Loss:  0.068435686805976\n",
            "Validation:  Epoch No:  85 \n",
            " Loss:  0.22679488097527065\n",
            "validation accuracy:  92.08\n",
            "epoch no 85\n",
            "Training:  Epoch No:  86 \n",
            " Loss:  0.08837214818715841\n",
            "Validation:  Epoch No:  86 \n",
            " Loss:  0.1960120345563628\n",
            "validation accuracy:  93.78\n",
            "epoch no 86\n",
            "Training:  Epoch No:  87 \n",
            " Loss:  0.06565516727506554\n",
            "Validation:  Epoch No:  87 \n",
            " Loss:  0.18320283063407988\n",
            "validation accuracy:  93.93\n",
            "epoch no 87\n",
            "Training:  Epoch No:  88 \n",
            " Loss:  0.08222114339100922\n",
            "Validation:  Epoch No:  88 \n",
            " Loss:  0.2105066642698366\n",
            "validation accuracy:  93.18\n",
            "epoch no 88\n",
            "Training:  Epoch No:  89 \n",
            " Loss:  0.09971559248854037\n",
            "Validation:  Epoch No:  89 \n",
            " Loss:  0.30477896301448343\n",
            "validation accuracy:  89.24\n",
            "epoch no 89\n",
            "Training:  Epoch No:  90 \n",
            " Loss:  0.067635977161298\n",
            "Validation:  Epoch No:  90 \n",
            " Loss:  0.3591814336754032\n",
            "validation accuracy:  86.52\n",
            "epoch no 90\n",
            "Training:  Epoch No:  91 \n",
            " Loss:  0.09041681131455148\n",
            "Validation:  Epoch No:  91 \n",
            " Loss:  0.3467471194213722\n",
            "validation accuracy:  87.8\n",
            "epoch no 91\n",
            "Training:  Epoch No:  92 \n",
            " Loss:  0.04966286982951715\n",
            "Validation:  Epoch No:  92 \n",
            " Loss:  0.3040028974271845\n",
            "validation accuracy:  89.89\n",
            "epoch no 92\n",
            "Training:  Epoch No:  93 \n",
            " Loss:  0.04162193673693397\n",
            "Validation:  Epoch No:  93 \n",
            " Loss:  0.24608584345853887\n",
            "validation accuracy:  91.91\n",
            "epoch no 93\n",
            "Training:  Epoch No:  94 \n",
            " Loss:  0.07080559457851468\n",
            "Validation:  Epoch No:  94 \n",
            " Loss:  0.18795127553632482\n",
            "validation accuracy:  94.04\n",
            "epoch no 94\n",
            "Training:  Epoch No:  95 \n",
            " Loss:  0.11376054923345108\n",
            "Validation:  Epoch No:  95 \n",
            " Loss:  0.18829287081141957\n",
            "validation accuracy:  93.86\n",
            "epoch no 95\n",
            "Training:  Epoch No:  96 \n",
            " Loss:  0.058428073942894114\n",
            "Validation:  Epoch No:  96 \n",
            " Loss:  0.19061673522251657\n",
            "validation accuracy:  93.71\n",
            "epoch no 96\n",
            "Training:  Epoch No:  97 \n",
            " Loss:  0.03882302471090904\n",
            "Validation:  Epoch No:  97 \n",
            " Loss:  0.2803783597848378\n",
            "validation accuracy:  89.92\n",
            "epoch no 97\n",
            "Training:  Epoch No:  98 \n",
            " Loss:  0.05780345071253731\n",
            "Validation:  Epoch No:  98 \n",
            " Loss:  0.16794979188404977\n",
            "validation accuracy:  94.83\n",
            "epoch no 98\n",
            "Training:  Epoch No:  99 \n",
            " Loss:  0.07296035563994924\n",
            "Validation:  Epoch No:  99 \n",
            " Loss:  0.34004521517129616\n",
            "validation accuracy:  87.98\n",
            "epoch no 99\n",
            "Training:  Epoch No:  100 \n",
            " Loss:  0.06734232374071145\n",
            "Validation:  Epoch No:  100 \n",
            " Loss:  0.26797719627153127\n",
            "validation accuracy:  90.25\n",
            "epoch no 100\n",
            "Training:  Epoch No:  101 \n",
            " Loss:  0.06643970307446384\n",
            "Validation:  Epoch No:  101 \n",
            " Loss:  0.237729275112506\n",
            "validation accuracy:  91.27\n",
            "epoch no 101\n",
            "Training:  Epoch No:  102 \n",
            " Loss:  0.07076735653847677\n",
            "Validation:  Epoch No:  102 \n",
            " Loss:  0.3290762749807909\n",
            "validation accuracy:  87.74\n",
            "epoch no 102\n",
            "Training:  Epoch No:  103 \n",
            " Loss:  0.12739332107145976\n",
            "Validation:  Epoch No:  103 \n",
            " Loss:  0.323501667357632\n",
            "validation accuracy:  88.73\n",
            "epoch no 103\n",
            "Training:  Epoch No:  104 \n",
            " Loss:  0.17918750249844917\n",
            "Validation:  Epoch No:  104 \n",
            " Loss:  0.2999699863134883\n",
            "validation accuracy:  89.39\n",
            "epoch no 104\n",
            "Training:  Epoch No:  105 \n",
            " Loss:  0.0830381509937197\n",
            "Validation:  Epoch No:  105 \n",
            " Loss:  0.1612327744739596\n",
            "validation accuracy:  94.84\n",
            "epoch no 105\n",
            "Training:  Epoch No:  106 \n",
            " Loss:  0.10556279795071008\n",
            "Validation:  Epoch No:  106 \n",
            " Loss:  0.22227079198916908\n",
            "validation accuracy:  92.22\n",
            "epoch no 106\n",
            "Training:  Epoch No:  107 \n",
            " Loss:  0.0531860365935265\n",
            "Validation:  Epoch No:  107 \n",
            " Loss:  0.42571723567158914\n",
            "validation accuracy:  84.1\n",
            "epoch no 107\n",
            "Training:  Epoch No:  108 \n",
            " Loss:  0.1250171829867819\n",
            "Validation:  Epoch No:  108 \n",
            " Loss:  0.24113739598274697\n",
            "validation accuracy:  91.3\n",
            "epoch no 108\n",
            "Training:  Epoch No:  109 \n",
            " Loss:  0.0647765469872198\n",
            "Validation:  Epoch No:  109 \n",
            " Loss:  0.17114687735610642\n",
            "validation accuracy:  94.59\n",
            "epoch no 109\n",
            "Training:  Epoch No:  110 \n",
            " Loss:  0.04103529866717901\n",
            "Validation:  Epoch No:  110 \n",
            " Loss:  0.3330417133355513\n",
            "validation accuracy:  88.65\n",
            "epoch no 110\n",
            "Training:  Epoch No:  111 \n",
            " Loss:  0.11021565625609776\n",
            "Validation:  Epoch No:  111 \n",
            " Loss:  0.1864746692518238\n",
            "validation accuracy:  93.86\n",
            "epoch no 111\n",
            "Training:  Epoch No:  112 \n",
            " Loss:  0.04776635553069517\n",
            "Validation:  Epoch No:  112 \n",
            " Loss:  0.2453509579943493\n",
            "validation accuracy:  91.82\n",
            "epoch no 112\n",
            "Training:  Epoch No:  113 \n",
            " Loss:  0.0869212754529694\n",
            "Validation:  Epoch No:  113 \n",
            " Loss:  0.23420733616128564\n",
            "validation accuracy:  92.17\n",
            "epoch no 113\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-b299756ce7a5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m                 \u001b[0mbatch_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m                 \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m                 \u001b[0mclient_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    154\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "torch.manual_seed(10)\n",
        "all_losses = {'train':[], 'test':[]}\n",
        "validation_accracy = []\n",
        "for epoch in range(num_epochs):\n",
        "    print('epoch no', epoch)\n",
        "\n",
        "    model_weights = copy.deepcopy(model.state_dict())\n",
        "    averaged_weights = {}\n",
        "    for layer in model_weights.keys():\n",
        "        averaged_weights[layer] = torch.zeros_like(model_weights[layer])\n",
        "\n",
        "    client_losses = []\n",
        "\n",
        "    random_clients = random.sample([i for i in range(num_users)], 10)\n",
        "    total_samples = 0\n",
        "    for element in random_clients:\n",
        "        total_samples += len(train_loaders[element])\n",
        "\n",
        "    freq = {}\n",
        "    for element in random_clients:\n",
        "        freq[element] = len(train_loaders[element]) / total_samples\n",
        "\n",
        "    for client in random_clients:\n",
        "\n",
        "        client_loss = 0\n",
        "        counter = 0\n",
        "\n",
        "        for local_epoch in range(5):\n",
        "            for batch_data, batch_labels in train_loaders[client]:\n",
        "                # Training\n",
        "                batch_data, batch_labels = batch_data.to(device), batch_labels.to(device)\n",
        "                \n",
        "                optimizer.zero_grad()\n",
        "                \n",
        "                batch_outputs = model(batch_data)        \n",
        "                loss = loss_function(model_weights, model.state_dict(), batch_outputs, batch_labels)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                client_loss += loss.item()\n",
        "                counter += 1\n",
        "            client_losses.append(client_loss / counter)    \n",
        "        \n",
        "        for layer in model.state_dict().keys():\n",
        "            averaged_weights[layer] += freq[client] * copy.deepcopy(model.state_dict()[layer])\n",
        "\n",
        "        model.load_state_dict(model_weights)\n",
        "        \n",
        "    model.load_state_dict(averaged_weights)  \n",
        "\n",
        "    all_losses['train'].append(sum([a*b for a,b in zip(client_losses,list(freq.values()))]))\n",
        "    \n",
        "\n",
        "    print('Training: ', 'Epoch No: ', epoch+1,'\\n',\n",
        "            'Loss: ', all_losses['train'][-1])\n",
        "\n",
        "\n",
        "    # Validation\n",
        "    with torch.set_grad_enabled(False):\n",
        "        batch_losses = []\n",
        "        counter = 0\n",
        "        total = 0\n",
        "        for batch_data, batch_labels in test_loader:\n",
        "            total += len(batch_data)\n",
        "            batch_data, batch_labels = batch_data.to(device), batch_labels.to(device)\n",
        "            batch_outputs = model(batch_data)            \n",
        "            loss = criterion(batch_outputs, batch_labels)\n",
        "            batch_losses.append(loss.item())\n",
        "            \n",
        "            for i in range(len(batch_data)):\n",
        "                true = int(batch_labels[i])\n",
        "                pred = int(torch.argmax(batch_outputs[i,:]))\n",
        "                if pred == true:\n",
        "                    counter += 1\n",
        "\n",
        "        all_losses['test'].append(sum(batch_losses) / len(batch_losses))\n",
        "        print('Validation: ', 'Epoch No: ', epoch+1,'\\n',\n",
        "            'Loss: ', all_losses['test'][-1])\n",
        "        acc = (counter*100) / total   \n",
        "        validation_accracy.append(acc)    \n",
        "        print('validation accuracy: ', acc)\n",
        "\n",
        "    if acc >= 95:\n",
        "        print('95 % accuracy reached!')\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "LGzakD4fTpgj",
        "outputId": "0ec3661c-b7bd-4244-96cf-30056cf3c9ae"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.plot(all_losses['train'], label='train')\n",
        "plt.plot(all_losses['test'], label='test')\n",
        "plt.legend()\n",
        "plt.xlabel('Rounds')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Loss VS Rounds');"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "Q_I_bVzKTpi-",
        "outputId": "35d3a770-eb13-4458-8a64-6c642018b7a3"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.plot(validation_accracy)\n",
        "plt.xlabel('Rounds')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Test accuracy');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R6e8bQJYc4z_"
      },
      "source": [
        "## Clients with different computational power"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "_pUH1wQLc9fj"
      },
      "outputs": [],
      "source": [
        "clients_power = []\n",
        "for i in range(num_users):\n",
        "    clients_power.append(random.randrange(2,11))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9aq70sgLijA5"
      },
      "source": [
        "## learning rate = 0.01"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "Nbu_JBPddA8U"
      },
      "outputs": [],
      "source": [
        "model = CNN().to(device)\n",
        "optimizer = optim.SGD(model.parameters(), lr = 0.01) \n",
        "criterion = nn.CrossEntropyLoss()\n",
        "num_epochs = 70\n",
        "mu = 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "uuWCNoyHdA-d",
        "outputId": "d9191e5f-cdf3-4660-aeb9-7483a9e12e5b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch no 0\n",
            "Training:  Epoch No:  1 \n",
            " Loss:  1.7789729139621242\n",
            "Validation:  Epoch No:  1 \n",
            " Loss:  3.1468099937438967\n",
            "validation accuracy:  19.2\n",
            "epoch no 1\n",
            "Training:  Epoch No:  2 \n",
            " Loss:  0.9772735019468126\n",
            "Validation:  Epoch No:  2 \n",
            " Loss:  1.8916505951881408\n",
            "validation accuracy:  36.78\n",
            "epoch no 2\n",
            "Training:  Epoch No:  3 \n",
            " Loss:  0.36906357491326414\n",
            "Validation:  Epoch No:  3 \n",
            " Loss:  1.6620984825491905\n",
            "validation accuracy:  44.48\n",
            "epoch no 3\n",
            "Training:  Epoch No:  4 \n",
            " Loss:  0.284387506259867\n",
            "Validation:  Epoch No:  4 \n",
            " Loss:  1.9522831510305405\n",
            "validation accuracy:  26.52\n",
            "epoch no 4\n",
            "Training:  Epoch No:  5 \n",
            " Loss:  0.36340216712861656\n",
            "Validation:  Epoch No:  5 \n",
            " Loss:  1.4478344997763635\n",
            "validation accuracy:  42.08\n",
            "epoch no 5\n",
            "Training:  Epoch No:  6 \n",
            " Loss:  0.22801957610723667\n",
            "Validation:  Epoch No:  6 \n",
            " Loss:  1.5706695396304131\n",
            "validation accuracy:  44.83\n",
            "epoch no 6\n",
            "Training:  Epoch No:  7 \n",
            " Loss:  0.2243858355380526\n",
            "Validation:  Epoch No:  7 \n",
            " Loss:  1.649694757193327\n",
            "validation accuracy:  45.6\n",
            "epoch no 7\n",
            "Training:  Epoch No:  8 \n",
            " Loss:  0.19721539942142152\n",
            "Validation:  Epoch No:  8 \n",
            " Loss:  0.9337957817316055\n",
            "validation accuracy:  64.2\n",
            "epoch no 8\n",
            "Training:  Epoch No:  9 \n",
            " Loss:  0.24021421722565145\n",
            "Validation:  Epoch No:  9 \n",
            " Loss:  1.4828861483484506\n",
            "validation accuracy:  50.62\n",
            "epoch no 9\n",
            "Training:  Epoch No:  10 \n",
            " Loss:  0.17290844786114345\n",
            "Validation:  Epoch No:  10 \n",
            " Loss:  1.0903292993307114\n",
            "validation accuracy:  62.23\n",
            "epoch no 10\n",
            "Training:  Epoch No:  11 \n",
            " Loss:  0.13717051023594476\n",
            "Validation:  Epoch No:  11 \n",
            " Loss:  0.8435416120290756\n",
            "validation accuracy:  70.49\n",
            "epoch no 11\n",
            "Training:  Epoch No:  12 \n",
            " Loss:  0.206120152964107\n",
            "Validation:  Epoch No:  12 \n",
            " Loss:  1.0012842211127282\n",
            "validation accuracy:  62.37\n",
            "epoch no 12\n",
            "Training:  Epoch No:  13 \n",
            " Loss:  0.10096519691684641\n",
            "Validation:  Epoch No:  13 \n",
            " Loss:  1.3150955325067044\n",
            "validation accuracy:  55.3\n",
            "epoch no 13\n",
            "Training:  Epoch No:  14 \n",
            " Loss:  0.22483246723677047\n",
            "Validation:  Epoch No:  14 \n",
            " Loss:  1.2293092410564423\n",
            "validation accuracy:  60.32\n",
            "epoch no 14\n",
            "Training:  Epoch No:  15 \n",
            " Loss:  0.18269286796246964\n",
            "Validation:  Epoch No:  15 \n",
            " Loss:  0.7001420620158315\n",
            "validation accuracy:  74.06\n",
            "epoch no 15\n",
            "Training:  Epoch No:  16 \n",
            " Loss:  0.1091037170816693\n",
            "Validation:  Epoch No:  16 \n",
            " Loss:  0.7640913254320622\n",
            "validation accuracy:  73.25\n",
            "epoch no 16\n",
            "Training:  Epoch No:  17 \n",
            " Loss:  0.16731639865811707\n",
            "Validation:  Epoch No:  17 \n",
            " Loss:  0.6730996969565749\n",
            "validation accuracy:  75.97\n",
            "epoch no 17\n",
            "Training:  Epoch No:  18 \n",
            " Loss:  0.23294956307803383\n",
            "Validation:  Epoch No:  18 \n",
            " Loss:  0.5348052450940013\n",
            "validation accuracy:  81.63\n",
            "epoch no 18\n",
            "Training:  Epoch No:  19 \n",
            " Loss:  0.1385484163176426\n",
            "Validation:  Epoch No:  19 \n",
            " Loss:  0.7082599867694079\n",
            "validation accuracy:  74.69\n",
            "epoch no 19\n",
            "Training:  Epoch No:  20 \n",
            " Loss:  0.10806694575640892\n",
            "Validation:  Epoch No:  20 \n",
            " Loss:  1.4633619644232094\n",
            "validation accuracy:  58.43\n",
            "epoch no 20\n",
            "Training:  Epoch No:  21 \n",
            " Loss:  0.3266853934294313\n",
            "Validation:  Epoch No:  21 \n",
            " Loss:  0.724721873845905\n",
            "validation accuracy:  72.98\n",
            "epoch no 21\n",
            "Training:  Epoch No:  22 \n",
            " Loss:  0.24369771153796171\n",
            "Validation:  Epoch No:  22 \n",
            " Loss:  0.7459938464611768\n",
            "validation accuracy:  72.03\n",
            "epoch no 22\n",
            "Training:  Epoch No:  23 \n",
            " Loss:  0.18773973893289322\n",
            "Validation:  Epoch No:  23 \n",
            " Loss:  0.5096459324099123\n",
            "validation accuracy:  82.21\n",
            "epoch no 23\n",
            "Training:  Epoch No:  24 \n",
            " Loss:  0.11986263418169543\n",
            "Validation:  Epoch No:  24 \n",
            " Loss:  0.9820349963456392\n",
            "validation accuracy:  67.69\n",
            "epoch no 24\n",
            "Training:  Epoch No:  25 \n",
            " Loss:  0.14267442367791475\n",
            "Validation:  Epoch No:  25 \n",
            " Loss:  0.5107865179888904\n",
            "validation accuracy:  81.62\n",
            "epoch no 25\n",
            "Training:  Epoch No:  26 \n",
            " Loss:  0.12160901560545721\n",
            "Validation:  Epoch No:  26 \n",
            " Loss:  0.6194638274163008\n",
            "validation accuracy:  76.0\n",
            "epoch no 26\n",
            "Training:  Epoch No:  27 \n",
            " Loss:  0.20276010628368663\n",
            "Validation:  Epoch No:  27 \n",
            " Loss:  0.4900695089045912\n",
            "validation accuracy:  82.32\n",
            "epoch no 27\n",
            "Training:  Epoch No:  28 \n",
            " Loss:  0.12718132223992307\n",
            "Validation:  Epoch No:  28 \n",
            " Loss:  0.6851295389384031\n",
            "validation accuracy:  76.17\n",
            "epoch no 28\n",
            "Training:  Epoch No:  29 \n",
            " Loss:  0.09555883916892892\n",
            "Validation:  Epoch No:  29 \n",
            " Loss:  0.5631343307644129\n",
            "validation accuracy:  79.01\n",
            "epoch no 29\n",
            "Training:  Epoch No:  30 \n",
            " Loss:  0.08638564836372076\n",
            "Validation:  Epoch No:  30 \n",
            " Loss:  0.5854119673445821\n",
            "validation accuracy:  78.55\n",
            "epoch no 30\n",
            "Training:  Epoch No:  31 \n",
            " Loss:  0.04841511529270676\n",
            "Validation:  Epoch No:  31 \n",
            " Loss:  0.6477384134046733\n",
            "validation accuracy:  78.6\n",
            "epoch no 31\n",
            "Training:  Epoch No:  32 \n",
            " Loss:  0.14800405232387062\n",
            "Validation:  Epoch No:  32 \n",
            " Loss:  0.4054869286920875\n",
            "validation accuracy:  85.96\n",
            "epoch no 32\n",
            "Training:  Epoch No:  33 \n",
            " Loss:  0.10561147732482626\n",
            "Validation:  Epoch No:  33 \n",
            " Loss:  0.39463248548842966\n",
            "validation accuracy:  86.84\n",
            "epoch no 33\n",
            "Training:  Epoch No:  34 \n",
            " Loss:  0.15280238866028698\n",
            "Validation:  Epoch No:  34 \n",
            " Loss:  0.4371752889351919\n",
            "validation accuracy:  84.99\n",
            "epoch no 34\n",
            "Training:  Epoch No:  35 \n",
            " Loss:  0.2169140010649072\n",
            "Validation:  Epoch No:  35 \n",
            " Loss:  0.678183447919786\n",
            "validation accuracy:  74.09\n",
            "epoch no 35\n",
            "Training:  Epoch No:  36 \n",
            " Loss:  0.2613472046292009\n",
            "Validation:  Epoch No:  36 \n",
            " Loss:  0.3666259312406182\n",
            "validation accuracy:  88.02\n",
            "epoch no 36\n",
            "Training:  Epoch No:  37 \n",
            " Loss:  0.11835743691986164\n",
            "Validation:  Epoch No:  37 \n",
            " Loss:  0.6561911296136678\n",
            "validation accuracy:  77.98\n",
            "epoch no 37\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-870675d94335>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m                 \u001b[0;31m# Model computations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m                 \u001b[0mbatch_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m                 \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mstate_dict\u001b[0;34m(self, destination, prefix, keep_vars)\u001b[0m\n\u001b[1;32m   1302\u001b[0m         \"\"\"\n\u001b[1;32m   1303\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdestination\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m             \u001b[0mdestination\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOrderedDict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1305\u001b[0m             \u001b[0mdestination\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_metadata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOrderedDict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1306\u001b[0m         \u001b[0mdestination\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_metadata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mprefix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlocal_metadata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mversion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_version\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "torch.manual_seed(10)\n",
        "all_losses = {'train':[], 'test':[]}\n",
        "validation_accracy = []\n",
        "for epoch in range(num_epochs):\n",
        "    print('epoch no', epoch)\n",
        "\n",
        "    model_weights = copy.deepcopy(model.state_dict())\n",
        "    averaged_weights = {}\n",
        "    for layer in model_weights.keys():\n",
        "        averaged_weights[layer] = torch.zeros_like(model_weights[layer])\n",
        "\n",
        "    client_losses = []\n",
        "\n",
        "    random_clients = random.sample([i for i in range(num_users)], 10)\n",
        "    total_samples = 0\n",
        "    for element in random_clients:\n",
        "        total_samples += len(train_loaders[element])\n",
        "\n",
        "    freq = {}\n",
        "    for element in random_clients:\n",
        "        freq[element] = len(train_loaders[element]) / total_samples\n",
        "\n",
        "    for client in random_clients:\n",
        "\n",
        "        client_loss = 0\n",
        "        counter = 0\n",
        "\n",
        "        for local_epoch in range(clients_power[client]):\n",
        "            for batch_data, batch_labels in train_loaders[client]:\n",
        "                # Training\n",
        "                batch_data, batch_labels = batch_data.to(device), batch_labels.to(device)\n",
        "                \n",
        "                optimizer.zero_grad()\n",
        "                \n",
        "                batch_outputs = model(batch_data)        \n",
        "                loss = loss_function(model_weights, model.state_dict(), batch_outputs, batch_labels)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                client_loss += loss.item()\n",
        "                counter += 1\n",
        "            client_losses.append(client_loss / counter)    \n",
        "        \n",
        "        for layer in model.state_dict().keys():\n",
        "            averaged_weights[layer] += freq[client] * copy.deepcopy(model.state_dict()[layer])\n",
        "\n",
        "        model.load_state_dict(model_weights)\n",
        "        \n",
        "    model.load_state_dict(averaged_weights)  \n",
        "\n",
        "    all_losses['train'].append(sum([a*b for a,b in zip(client_losses,list(freq.values()))]))\n",
        "    \n",
        "\n",
        "    print('Training: ', 'Epoch No: ', epoch+1,'\\n',\n",
        "            'Loss: ', all_losses['train'][-1])\n",
        "\n",
        "\n",
        "    # Validation\n",
        "    with torch.set_grad_enabled(False):\n",
        "        batch_losses = []\n",
        "        counter = 0\n",
        "        total = 0\n",
        "        for batch_data, batch_labels in test_loader:\n",
        "            total += len(batch_data)\n",
        "            batch_data, batch_labels = batch_data.to(device), batch_labels.to(device)\n",
        "            batch_outputs = model(batch_data)            \n",
        "            loss = criterion(batch_outputs, batch_labels)\n",
        "            batch_losses.append(loss.item())\n",
        "            \n",
        "            for i in range(len(batch_data)):\n",
        "                true = int(batch_labels[i])\n",
        "                pred = int(torch.argmax(batch_outputs[i,:]))\n",
        "                if pred == true:\n",
        "                    counter += 1\n",
        "\n",
        "        all_losses['test'].append(sum(batch_losses) / len(batch_losses))\n",
        "        print('Validation: ', 'Epoch No: ', epoch+1,'\\n',\n",
        "            'Loss: ', all_losses['test'][-1])\n",
        "        acc = (counter*100) / total   \n",
        "        validation_accracy.append(acc)    \n",
        "        print('validation accuracy: ', acc)\n",
        "\n",
        "    if acc >= 95:\n",
        "        print('95 % accuracy reached!')\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "afL4C_QjdBAh",
        "outputId": "b75fe907-166b-41a4-815b-051f500ceaca"
      },
      "outputs": [
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOzdd3yV5fn48c+VQQKZhGxCWAl7ylRRQFDBvfdotaKtrbW1/tTWulq7tdav1kHds466caCCCCjI3iOMQBhJIJBBdnL//rjPgRBOkpPkrHCu9+uVV5JznvOcK0d5rue+r3uIMQallFLBK8TfASillPIvTQRKKRXkNBEopVSQ00SglFJBThOBUkoFOU0ESikV5DQRKBUEROQBEXnV33GowKSJQAUkEdkuIlN9/J53i8g8F48niki1iAwRkU4i8oiI5IlImSPOx5o5pxGRQ45jd4nIoyIS6t2/RKnW0USg1BGvAieJSO9Gj18BrDbGrAHuAUYDY4EYYBKwrIXzDjfGRAMTgcuBGzwZtFLtpYlAdSgiEiEij4nIbsfXYyIS4XguUUQ+FpGDIlIkIt+KSIjjubscd+SlIrJRRKY0PrcxJg/4Gri20VPXAS87fh4DvGeM2W2s7caYl3GDMSYHWACMaPD33CQiOY54PxSRdMfjvRytibAGx84VkZ84fv6RiMwXkX+IyAER2SYi0xsc21tEvnH8vbOBxAbPRYrIqyKy3/FZ/SAiKe78Der4pIlAdTS/A8ZjL6bDsXfm9zqeuwPIA5KAFOC3gBGR/sDPgTHGmBjgTGB7E+d/iQaJwPHaEcDrjoe+B34tIj8TkaEiIu4GLiIDgFOAHMfvpwF/Bi4D0oBc4E13zweMAzZiL/J/A55rEM/rwFLHc38Arm/wuuuBOKAH0A24Bahoxfuq44wmAtXRXA08ZIwpMMYUAg9y5MJdg72g9jTG1BhjvjV2Ma06IAIYJCLhjrv4LU2c/z0gRUROcvx+HfCp473AXrj/6ohjCbBLRK4/9jRHWSYih4D1wFzg3w3+lueNMcuMMVXYbqcTRaSXW58E5BpjZhpj6rAJLM0Reya25fJ7Y0yVMWYe8FGD19VgE0CWMabOGLPUGFPi5nuq45AmAtXRpGPvnJ1yHY8B/B17t/2FiGwVkbvhcJfM7cADQIGIvOnsgmnMGFMOvA1c57i7vpoj3UI4LpxPGmNOBuKBh4HnRWRgMzGfAERj6wPjgChXf4sxpgzYD3Rv6UNw2Nsobhzvkw4cMMYcanBsw8/sFeBz4E1H99rfRCTczfdUxyFNBKqj2Q30bPB7puMxjDGlxpg7jDF9gPOwXThTHM+9boyZ4Hitwd7VN+UlbHfN6diC8EeuDjLGVBhjngQOAIOaC9pRT3gL+A64z9XfIiJR2Dv1XYDzIt6lwWlSm3uPBvYAXR3nc8psEEuNMeZBY8wg4CTgHGzLRwUpTQQqkIU7CpvOrzDgDeBeEUkSkUTsRfVVABE5R0SyHHfyxdguoXoR6S8ipzmKypXY/vD6Zt73W+Ag8CzwpjGm2vmEiNwuIpNEpLOIhDm6hWKA5W7+TX8BbhKRVMff8mMRGeGI7U/AIkfXVSE2IVwjIqEicgPQ1503MMbkYrutHnQMd50AnNvgb5jsqG+EAiXYrqLmPg91nNNEoALZLOxF2/n1APBH7EVuFbAaO3Tzj47js4EvgTLsnfe/jTFzsPWBvwD7sN0pydj+eJccdYWXsXfrjUcElQOPOM6zD7gVuNgYs9WdP8gYsxqYB9xpjPkS+D3wLvYuvi92qKrTTcCd2O6iwcBCd97D4SpsN1QRcH+jvyMVeAebBNYD32C7i1SQEt2YRimlgpu2CJRSKshpIlBKqSCniUAppYKcJgKllApyYS0fElgSExNNr169/B2GUkp1KEuXLt1njEly9VyHSwS9evViyZIl/g5DKaU6FBHJbeo57RpSSqkgp4lAKaWCnCYCpZQKch2uRqCUUm1RU1NDXl4elZWV/g7FqyIjI8nIyCA83P0FZTURKKWCQl5eHjExMfTq1YtW7CfUoRhj2L9/P3l5efTu3XjH1aZp15BSKihUVlbSrVu34zYJAIgI3bp1a3WrRxOBUipoHM9JwKktf2PwJIL8dfDlg1Be5O9IlFIqoARPIijaCvMfhYNNzqlQSimvOXjwIP/+979bPrCRs846i4MHD3ohoiOCJxHEptnvJXv8G4dSKig1lQhqa2ubfd2sWbOIj4/3VlhAMI0ainHsVV66279xKKWC0t13382WLVsYMWIE4eHhREZG0rVrVzZs2MCmTZu44IIL2LlzJ5WVlfzyl79kxowZwJFldcrKypg+fToTJkxg4cKFdO/enQ8++IDOnTu3O7bgSQTRySCh2iJQSvHgR2tZt7vEo+cclB7L/ecObvL5v/zlL6xZs4YVK1Ywd+5czj77bNasWXN4mOfzzz9PQkICFRUVjBkzhosvvphu3boddY7NmzfzxhtvMHPmTC677DLeffddrrnmmnbHHjyJICQUolOgVBOBUsr/xo4de9RY/8cff5z33nsPgJ07d7J58+ZjEkHv3r0ZMWIEAKNGjWL79u0eiSV4EgHYOkGJdg0pFeyau3P3laioqMM/z507ly+//JLvvvuOLl26MGnSJJdzASIiIg7/HBoaSkVFhUdi8VqxWEQiRWSxiKwUkbUi8qCLYyJE5L8ikiMii0Skl7fiASAmTVsESim/iImJobS01OVzxcXFdO3alS5durBhwwa+//57n8bmzRZBFXCaMaZMRMKB+SLyqTGm4V94I3DAGJMlIlcAfwUu91pEsemw7VuvnV4ppZrSrVs3Tj75ZIYMGULnzp1JSUk5/Ny0adN4+umnGThwIP3792f8+PE+jc1ricAYY4Ayx6/hji/T6LDzgQccP78DPCEi4nit58WkQVUxVB+CTlEtH6+UUh70+uuvu3w8IiKCTz/91OVzzjpAYmIia9asOfz4b37zG4/F5dV5BCISKiIrgAJgtjFmUaNDugM7AYwxtUAx0A1viXUMIdWRQ0opdZhXE4Exps4YMwLIAMaKyJC2nEdEZojIEhFZUlhY2PaAYhyTynQugVJKHeaTmcXGmIPAHGBao6d2AT0ARCQMiAP2u3j9s8aY0caY0UlJLvdeds/hFoEmAqWUcvLmqKEkEYl3/NwZOB3Y0OiwD4HrHT9fAnzttfoAHGkRaCJQSqnDvDlqKA14SURCsQnnLWPMxyLyELDEGPMh8BzwiojkAEXAFV6MByKiISJWh5AqpVQD3hw1tAoY6eLx+xr8XAlc6q0YXIrRSWVKKdVQ8Kw+6hSrk8qUUr7X1mWoAR577DHKy8s9HNERwZcIYtJ1+KhSyucCOREE11pDYFsEZflQX2cXolNKKR9ouAz16aefTnJyMm+99RZVVVVceOGFPPjggxw6dIjLLruMvLw86urq+P3vf09+fj67d+9m8uTJJCYmMmfOHI/HFnyJICYNTB2UFRzZrEYpFVw+vRv2rvbsOVOHwvS/NPl0w2Wov/jiC9555x0WL16MMYbzzjuPefPmUVhYSHp6Op988glg1yCKi4vj0UcfZc6cOSQmJno2Zofg6xqK1Q1qlFL+9cUXX/DFF18wcuRITjjhBDZs2MDmzZsZOnQos2fP5q677uLbb78lLi7OJ/EEZ4sAbJ2gu39DUUr5STN37r5gjOGee+7h5ptvPua5ZcuWMWvWLO69916mTJnCfffd5+IMnhXELQItGCulfKfhMtRnnnkmzz//PGVldl3OXbt2UVBQwO7du+nSpQvXXHMNd955J8uWLTvmtd4QfC2CqCTHlpXaNaSU8p2Gy1BPnz6dq666ihNPPBGA6OhoXn31VXJycrjzzjsJCQkhPDycp556CoAZM2Ywbdo00tPTvVIsFm+u6OANo0ePNkuWLGnfSR4dBL1PhQuf9kxQSqmAt379egYOHOjvMHzC1d8qIkuNMaNdHR98XUOgs4uVUqqB4EwEOrtYKaUOC9JE0F1nFysVhDpaV3hbtOVvDM5EEJMG1aVQ5b0qvFIqsERGRrJ///7jOhkYY9i/fz+RkZGtel3wjRqCo7esTIrxbyxKKZ/IyMggLy+Pdu1y2AFERkaSkZHRqtcEZyJouGVlUj//xqKU8onw8HB69+7t7zACUnB2Dekm9kopdVhwJgLdxF4ppQ4LzkTQqQtExmmLQCmlCNZEAHaDGp1LoJRSQZwIYnV2sVJKQTAnAm0RKKUUEMyJwLllZV2tvyNRSim/Ct5EEJMGph4OFfg7EqWU8iuvJQIR6SEic0RknYisFZFfujhmkogUi8gKx5f3t+Jx0rkESikFeHdmcS1whzFmmYjEAEtFZLYxZl2j4741xpzjxThcO2ouwSifv71SSgUKr7UIjDF7jDHLHD+XAusJpF2CtUWglFKAj2oEItILGAkscvH0iSKyUkQ+FZHBvogHgC6JEBKus4uVUkHP64vOiUg08C5wuzGmpNHTy4CexpgyETkLeB/IdnGOGcAMgMzMTM8EFhICManaIlBKBT2vtghEJBybBF4zxvyv8fPGmBJjTJnj51lAuIgkujjuWWPMaGPM6KSkJM8FGJOmLQKlVNDz5qghAZ4D1htjHm3imFTHcYjIWEc8+70V0zFi07RFoJQKet7sGjoZuBZYLSIrHI/9FsgEMMY8DVwC/FREaoEK4Arjy+2DYtIh5yufvZ1SSgUiryUCY8x8QFo45gngCW/F0KLYNKgug8oSiIz1WxhKKeVPwTuzGGyLAHTNIaVUUAvuRBDrmFSmq5AqpYJYcCeCw7OLtUWglApewZ0IDs8u3uXfOJRSyo+COxGEd4bIeB1CqpQKasGdCMC2CrRrSCkVxDQRxOiWlUqp4KaJIDZNWwRKqaCmiSAmHcoKoK7G35EopZRfaCKITQOM3b9YKaWCkCaCGN2gRikV3DQRxDbcslIppYKPJgJtESilgpwmgi7ddMtKpVRQ00QQEuKYS6AtAqVUcNJEADqXQCkV1DQRgM4uVkoFNU0EcGS9IR/ukqmUUoFCEwHYFkFNOVQW+zsSpZTyOU0EcGRfAq0TKKWCkCYCOLJTmdYJlFJBSBMBNJhdrC0CpVTw0UQAOrtYKRXUvJYIRKSHiMwRkXUislZEfuniGBGRx0UkR0RWicgJ3oqnWeGR0DlBZxcrpYJSmBfPXQvcYYxZJiIxwFIRmW2MWdfgmOlAtuNrHPCU47vvxaZri0ApFZS81iIwxuwxxixz/FwKrAe6NzrsfOBlY30PxItImrdialZMmrYIlFJBySc1AhHpBYwEFjV6qjuws8HveRybLBCRGSKyRESWFBYWeifIWF1vSCkVnLyeCEQkGngXuN0YU9KWcxhjnjXGjDbGjE5KSvJsgE4x6XCoULesVEoFHa8mAhEJxyaB14wx/3NxyC6gR4PfMxyP+Z5zy8rSve07z74cqCrzSEhKKeUL3hw1JMBzwHpjzKNNHPYhcJ1j9NB4oNgY45/+mRgPzC4u3QtPnQRz/uSZmJRSyge8OWroZOBaYLWIrHA89lsgE8AY8zQwCzgLyAHKgR97MZ7mxXpgdvGip6GuCrZ85ZmYlFLKB7yWCIwx8wFp4RgD3OqtGFqlvS2CqlL44XkIi4TCDbZ1EJPqufiUUspLdGaxU5cECI1oe4tg2ctQVQxnPmx/3/qN52JTSikv0kTgJAKJ2bD5C6irbd1r62rg+6cg8yQYdQN07grbNBEopToGTQQNTbrbdussfaF1r1v7PhTvhJNvs3sg9zrFtgh0oxulVAegiaChAedA71Ph6z9CeZF7rzEGFv4LEvtB9pn2sT4ToSQPirZ6L1allPIQTQQNicC0v0BVCcz9s3uv2ToX9q6Gk35hWwMAfSY7npvjlTCVUsqTNBE0ljIYRt8APzwHBetbPn7h4xCdAsMuP/JYQh+IzdCCsVKqQ9BE4Mrk30FEDHx2T/P9/HtXw5avYdzNEBZx5HER2z20/Vuor/d+vEop1Q5BkwiKy2uYv3kf1bVuXJi7JMDk39qunY2fNn3cwv+D8Cjbgmis90SoOAB7V7U9aKWU8oGgSQRzNxVwzXOL2LrPzXWARt8Aif3h899CbdWxzxfnwZp3YdT1drhoY30m2u9b57Y5ZqWU8oWgSQTZyTEAbM53MxGEhsO0P8GBbXaOQGPfP2W7jcb/1PXrY1IhaYDOJ1BKBbygSQR9kqIIEdhc0IqVQbOmQr/pMO8fUJp/5PHKYlj6Egy5COIzm35974mQ+53rFoVSSgWIoEkEkeGh9OwWxeb80ta98MyHobYSvn7oyGNLXoDqUjtktDl9JkFtBeT90NpwlVLKZ9xKBCISJSIhjp/7ich5jr0GOpSs5OjWtQgAuvW13T/LX4Ndy6C22q4y2nsipA1v/rW9TgYJ0TqBUiqgudsimAdEikh34Avs8tIveisob+mXEs32fYfcGznU0Kl3QlSiHU66+i27QunJt7X8usg4SD9B5xMopQKau4lAjDHlwEXAv40xlwKDvReWd2Qnx1Bbb9i+/1DrXhgZC1Pug53fw6d3Q8oQ6DvFvdf2mQi7lkJlm3bpVEopr3M7EYjIicDVwCeOx0K9E5L3ZCVHA60YOdTQiKttV5CzNiDNbrVwRJ9JYOogd2Hr31MppXzA3URwO3AP8J4xZq2I9AE63EI6WcnRiMDmglYWjAFCQuG8J+z8giEXu/+6jLF2sxpP1QkKN8IrF8Grl3jmfEqpoOfWDmXGmG+AbwAcReN9xhg3OskDS2R4KJkJXdrWIgBIGwbn/LN1rwmPhMzx7Z9PUFUK3/zVzl+orz3yWERM+86rlAp67o4ael1EYkUkClgDrBORO70bmndkJ0e3rUXQHr0nQsE6KCto/WuNgdXvwBNj7JIWw6+E85+0z+Wv9WycSqmg5G7X0CBjTAlwAfAp0Bs7cqjDyU6JYdu+Q9TU+XAxuD6T7Pdt81r3uvx18OI58O6NdoXTn3wF5z9x5Hx7V3suRqVU0HI3EYQ75g1cAHxojKkBOuT2W9nJ0dTUGXJbO3KoPdKG26Gk7u5PUFlsh6o+PQEK1truqJu+hozR9vnY7nZ9I00ESikPcKtGADwDbAdWAvNEpCfQIcdDNlxzKCvZR/3rIaGO7Svn2a6e5kYc7VkFr18GpXth1I/ssNUuCUcfI2KHsOav8WrYSqng4FaLwBjzuDGmuzHmLGPlApO9HJtXHBk51MaCcVv1mQTFO+widk3JXQgvnm1nI9/0NZz72LFJwCl1mO06qq/zRrRKqSDibrE4TkQeFZEljq9HgKgWXvO8iBSIiMvbVhGZJCLFIrLC8XVfG+Jvtc6dQsno2tk/iQCanmW86XN45UJbC7jhc+h+QvPnSx1i1zHav8WTUSqlgpC7NYLngVLgMsdXCfBCC695EZjWwjHfGmNGOL4eauFYj8lOjmn94nPt1S0LYtJdzydY9Ta8eRUk9YcbPoP4Hi2fL2WI/Z6vdQKlVPu4mwj6GmPuN8ZsdXw9CPRp7gXGmHlAUbsj9ILslGi2Fh6i1pcjh5zbV26bd/T2lYtnwv9ugh7j4fqP7ZpG7kgaACHhWjBWSrWbu4mgQkQmOH8RkZOBCg+8/4kislJEPhWRJtcuEpEZzm6pwsLCdr9pdnIM1XX17Cgqb/e5WqXPJKgoskVeY2DuX2HWb6D/dLjmHbumkbvCOtkWxN42FoyLd2m3klIKcH/U0C3AyyIS5/j9AHB9O997GdDTGFMmImcB7wPZrg40xjwLPAswevTodg9bzXasObQpv4w+SdHtPZ37eju3r5wDK16zy1kPv9IuXRHq7n+KBlKHtn3pivd/agvXv1zl/rpJqn3q6wFjR5EpFUDcHTW00hgzHBgGDDPGjAROa88bG2NKjDFljp9nYecquNkv0j7OxedyfD3DODYNEvvBnD/ZJDD+Z3D+v9uWBMDWCUr3wKF9rXtdbRXsXAQHd9iVUZVvfHQbvHapv6NQ6hit2qHMcfF2zh/4dXveWERSReytqIiMdcSyvz3ndFdURBjd4/0wcgig72l2x7PJ98KZf4KQdmwSl+ooGLe2TrB7hY0BYM3/2v7+qnW2z7dDhHXIrwow7dmqstn+BBF5A/gO6C8ieSJyo4jcIiK3OA65BFgjIiuBx4ErjDE+m62cnRLNprYuPtcek38HN3wBE+9sf5dMylD7vbUTy3IX2O89xsG6948uXivvqD4EB7bbIb9FzcwlUcoP2tgnAbSwxIQx5soWnn8CeKId798u/VJiWLhlP3X1htAQH/aRR8ZC5jjPnCuqmx2S2toWQe5CSOwPY35iRyzlLbYrpCrvKdjA4X8y+WsgMcuv4SjVULMtAhEpFZESF1+lQLqPYvSKrORoqmvr2enrkUOeljqkdSOH6utsfaDnSXa0UmgErH3Pe/Epq6DBSrG6aqwKMM0mAmNMjDEm1sVXjDGmPa0JvzsycsjHBWNPSx0K+zbaArA78tdAVQn0PNnuZZB9Oqx9X/utvS1/HYR3sRMLNRGoANOeGkGHlp3iWHzOHwVjT0oZYjeqKdzg3vHOLTN7nmi/D7kIyvbCju+9E5+yCtbaSYCpQ3WxQBVwgjYRREeEkR4XSU5HTwSpw+x3d7uHchdAfCbEZdjfs8+EsM7aPeRt+esgZRCkDIaDuXZ3OaUCRNAmAoCslJiO3zWU0Nt2ObhTMDbGtgh6nnzksYho6HcGrPtAu4e8pawAyvdB8uAja0QVrPdvTEo1ENSJoF9yNDkFZdTVd8g9dqyQUEge5F53w75NUL7fFoobGnwRHCo4MqzUl5Y8DyvesEnqeOWsCThbBKDdQyqgBHUiyE6Jpqq2nl0HPLFskh+lDrUtgpYupofrAycf/Xj2GbZV4evJZQdy4ZM74P1b4OXzjt/x9c67/+TBENcDImK1YKwCSlAnAucOZR2+eyh1CFQehOK85o/LXQhRyZDQaOHYTl2g3zRY/yHU1XovzsaWPA+I3YVt13L494nw3ZPHXxdVwVqISoLoJMfucoM1EaiAEtSJIDvFDiHt8COHnAXj5robjLFdPz1Pcj2jechFttto+7feibGxmgpY9jIMOBtOuQNuXWSX6f78t/Dc6ba4erzIX2e775ycieB47g5THUpQJ4LYyHBSYyPZ7OvF5zwteRAgzReMD+6Akl3Hdgs5ZU2FTtGw1kfdQ2v+Z5fkHjvD/h7XHa58Ey5+zi7F8MypMOfPUFvtm3i8pb7eDu1NabDKespgO5ejeKf/4lKqgaBOBGBbBZv9seaQJ0VE29FDzSWCw/WBk1w/H94Z+p8F6z+CuhrPx9iQMbD4GUgaCL0mHHlcBIZeArf+AIMvhG/+YhNC3hLvxuNNB7ZBTXmjFoFzdzntHlKBQRNBcgw5BWXUd+SRQ9DyRKUdCyEy7ugLUmODL4SKA03vq+wpeT/AnpUw9ibX3VRR3eDimXDV2/bO+cWzW7/UdqAocHRxpTT43JMH2u86ckgFCE0EKdFU1NSx62AHHzmUMhSKtjY9USl3IWSe2Pyy11lT7IgWb08uW/ysfZ9hlzd/XL8z4NKX7JLZ2+d7NyZvyV8HiJ1V7BQRA/E9tUWgAoYmgmRnwbiD1wlSnUtSuyiylubD/pymu4WcwiJs8XbDR97rmy/Nt2sbjbjadmm1JH2ErV34qojtaQVroWsv6BR19OMpQzQRqIChicAxhLTD1wkOb1Kz6tjndjQxf8CVwRdCZbHdTtMblr0E9TV2CWx3hIbbJbI7cosgxcV23CmDbXKu6eAtUXVcCPpEENclnOSYiI4/hDS2O0TGu+53zv3OThhLG97yefpMtrUEb3QP1dXYuQN9p7RuPf5eE+zIm7JCz8fkTTUVULTFdV0mZTCYevcXC1TKi4I+EYBj5FBHTwQijhnGrhLBQugx1t5dtySsEww4FzZ8AjWVno1xw8d2j+VxN7fudb1Osd9zO1iroHCjvdinuEoEOnJIBQ5NBDhGDuWX4sOdMr0jdai9sDScmVtxwLYSMluoDzQ0+EI7WmfL156Nb/FM21+eNbV1r0sb7qgTdLBE4BwxlOyiayiht1319XiaOKc6LE0E2BbBoeo6dhd7+A7Y11KGOPbE3XrksR2LANNyobihPhOhc1fPTi7bu8bObB7zE7tQXmt01DpB/lq7A1zjJT3AsVjgQB1CqgKCJgIaFoyPk5FDDSeW5S6AkHDIGO3+eULDYeC5sPFTzxUzFz9r74BHXN2213fEOkHBOkjqD6FNbOaXMtgmgo7eElUdniYCGgwh7egjh5L6Q0jY0Ylgx3fQfZSdOdwaQy6G6jJY9d/2x1VxAFa9BcMuhS4JbTtHR6wTFKx3PWLIKWWIXd+prMB3MSnlgiYCoGtUJxKjIzr+XIKwCDtxydndUH0Idi9vXbeQU++JkDEG5v4VqsvbF9fy12yX1Zib2n6OtBEdq05QXmQL483N5Na9CVSA0ETgkJ18HIwcAnuX6WwR5P1g9zNuSyIQgakPQuluuy5QW9XXww8z7azmtGFtP09omD1HR0kErpaWaOxwItCRQ8q/vJYIROR5ESkQEZe3O2I9LiI5IrJKRE7wVizuyE6JJie/7DgYOTTE3oke2m+HjUqIHTraFr1Otnsaz/+nvcNti5wv7WqiY9vRGjgcj7NO0AG6UvKbGTHk1CUBYtI1ESi/82aL4EVgWjPPTweyHV8zgKe8GEuLslNiKK2qZW9JBx85dHipidU2EaQOtRPE2mrq/VBZYpNBWyx+BqJT7dyE9nLWCTpCq6BgrZ3gF5Pa/HEpgzQRKL/zWiIwxswDmruNPB942VjfA/EikuateFpy3BSMUxyJYNdS2zXkzrISzZ5vMAy/AhY90/IOaI0VbrQtgtE/thPV2qsjzSdwLi3hanXVhlIG21aOt5f+VqoZ/qwRdAca7syR53jsGCIyQ0SWiMiSwkLvDB90JoIOv21lVDfb3bD8VbtqZ1vqA41N/i1gYO6f3X9NZQm8db1tjYz6cftjgI5TJzDGjhhqrlDslDLErr20b7P343LasxKKd/nu/VTA6xDFYmPMs8aY0caY0UlJSV55j27REXSL6tTxEwHYOoFzUlnmie0/X3ymHfGz4nUocGNtnLpaeOcG2EA8NaMAACAASURBVLcJLnsZYlLaH4NTrwmwb2Ng1wkO7oDq0uYLxU6+LhjX1cJL58GsO33zfqpD8Gci2AX0aPB7huMxvxmaEcfKncX+DMEznOvYJPaHqETPnPOUO2y3zNd/aPnYL+6FnNlw9j+gzyTPvL9TR6gTNLe0RGPdsu2EP18NId21BCoPwta5HX8bUOUx/kwEHwLXOUYPjQeKjTF7/BgPI3rEs6mglLKqWn+G0X7OgrEnuoWcorrBybfZheN2LGr6uCXPw6KnYNxPYfQNnnt/p7Th0CkmsBOB8+7euRNZc8I62YmAvmoR5Hxlv9ccgp3f++Y9VcDz5vDRN4DvgP4ikiciN4rILSJyi+OQWcBWIAeYCfzMW7G4a2RmV4yBVTsP+juU9skYbWcYZ5/u2fOO/xlEp8CXD7heFmHrXPjkN5B1Opz5sGff2yk0DHoGeJ2gYB3EZUJkrHvHpwz2YSL40g4oCAm3PyuFd0cNXWmMSTPGhBtjMowxzxljnjbGPO143hhjbjXG9DXGDDXG+H2H8hEZ8QAs7+iJID4T7thkdxvzpE5RMPEuu9HNps+Pfm7fZnjrOkjsB5c83/qF5Voj0OsE+evcqw84pQy2E/faOlfDXYf22Znmg86zi/g5Wwcq6HWIYrGvxHUJp29SFMt3HPB3KO0X1c075z3hOkjoC189eGS56/IieP1y2wq56k3374TbqtcE+z0Qt6+srYb9m90bMeTkLBgXeHlJ6i1zAGP3ps6aausSJX7tjVUBQhNBIyN6dGXFzoMdf4axt4SGw5Tf24vWqv/a8e9vXQfFO+GK1+1+A96WGsB1gv2b7bIezS0215ivNqnJ+RK6dIO0kUf2hNiirQKlieAYIzPj2VdWTd4B3Uu2SYMugPSRMOdP8NHt9s78vP+z3Q2+EMh1gsNLS7SiRRCdYi/Q3hw5VF9vL/p9T4OQEJuoolO1TqAATQTHGJlp6wTLjofuIW9xLkhXvBNWvAoTfm1nH/tSrwl2nkJpvm/ftyUFa20hNjHb/deIeL9gnL8aDhXa/aKd75k11XYX1XXwUXKq3TQRNNI/JYbO4aEs39HBC8be1mcijLzG1gxO+73v399ZJwi0/Qny19mCuTv7QzeUMsTORm64zagnOe/8+5525LGsKXZOwe5l3nlP1WFoImgkLDSEoRlxrOjoI4d84fwnbZdQiB/+NwrUOkHBOvfmDzSWPAhqyu1Krd6Q8xWkDjt6lnefSXZ1Wu0eCnqaCFwYmRnPut0lVNV66e5MtV8g1gkqi213WWuGjjp5c5OaymLYuehIgdipSwJ0H62JQGkicGVkj65U19WzdneJv0NRzel1SmDVCQrW2+/uLC3RWNIAe3fujTrBtnl2JFPjRAD2sV3L7P4VKmhpInDBWTDWOkGAc7dOsPMHWPOu94uizot4W1oEnbrY+RneSAQ5X9luNFcbFGVNBQxsneP591UdhiYCF1JiI+ke3/n4mFh2PEsdBhGxsM3FxDJjYOs38OI58NxUuxrqC9Nh/xbvxVOwzsYT16PlY11JGdx015AxcHCnvagf3OH+OY2xr+kz0XUBO30EdE7Q7qEgF+bvAALViB7xWjAOdK72JzAGNn8B8/5uN+aJToUzHrarsH76/+DpCXDGH2D0jS1vGtNa+Y5CcVvPmzIE1r0Pu1fYWkPhRtv1VbjRLuFRc8gel9AXfva9e5v97NsMxTvglF+5fj4k1I4kyvnKzjXwR+Ff+Z0mgiaMzIznk9V7KCitJDkm0t/hqKb0mgCbP4eS3bBzMXz7D9i72i76dvajMOJqCHf89+t9Knzwc/jkDtgwC85/AmLTPROHMXYOweCL2n4OZ8H42YlHHovtboejnnCt/V5XA5/dZVd5HX+L6/M0dHjY6JSmj8maCmvesXMN0oa3PX7VYWkiaIKzTrBix0HOGNzCvrPKf5x1gqdOhooi6JYF5/8bhl12bFdIbDpc8669iH5xL/x7PJz1CAy9pP2tg/y1dnROa5aWaCxrKpzxR+iSCEn97IU/IuboY4yBTZ/Z3eKGXWZH/jQn50t7nq49mz7GObcg50tNBEFK24FNGJweR3iodPyVSI93qcPs3X9Mml319NbFMPLqpid0icCYG+GW+Xbjnv/9BN7+UftGzVQfgnd/YpeJGHhu288T1glO+gWMuBK6jzo2CYCN/8yHoaoE5v2j+fPVVEDuAtejhRqKSbGfoy9XIzUG1n8cOCO+gpwmgiZEhocyMC2WFTpyKLCFhsEvV8JPF8CQi91f/rpbX7jhM5hyP2z4xLYOdrRhoxZj4ONf2w3oL34OYnzQekwZbGd1L362+eJ37gK7b3VWM91CTllT7VyDSh/t0Lf6bfjv1fCfqbAvxzfv6S91tTBzCnz5oL8jaZImgmaM7BHPyryD1NXrSqQBLSSkbV07IaFwyq9hxhx79/3qJZC3tHXnWPYSrHoTJt0DfSe3Poa2mnwvhHaCL+9v+picryAsEnqe3PL5sqbauQbb5nkuxqaU7IZZv7E76dWUw/Nn2gK5Nxhji+D+tOwlu0Xoomfal2g/ut3etHiBJoJmjMzsSnl13fGxob1qWupQ+NHHtr/91Ytgr5uze/eshFn/z/axn+rjzeBjUmDCr2D9R5C70PUxOV/aJBDeueXz9Rhr5xp4exipMfDhL+y+DZe+BDd8buN78RzXw4DbqrocFj0Ljw2DF6b5LxlUldp6TkIfO+pr+WttO0/eUlj6gteWINFE0AydWBZEYtPh+g/tLmwvnw+Fm5o/vuKg3YchKhEumumfYZcn3mpHFX3+22MvdAd32KGnLdUHnELD7VyDnK9cb0Pa2P4t9mLbWstessnm9Ids91xilk0Gcd3h1Ytt3aA9Kg7AN3+Hx4bAp3fausvORXZUlD8seNyu+nrRTMgYCz/MbFtSWvgviIizizx6gSaCZmQmdCEhqhMrdurEsqDQtRdc94HtZnr5PCja6vo4Y+CDW6E4Dy590SYDf+jUBabcZ7efXP320c85C7/uJgLnscU7bQJpijHw3b/hidH2Trs122se2A6f/84O4x3zkyOPx3WHH39qW2ZvXQvLX3X/nE4le+xIsH8OgTl/tMX2H38Kt/5gC+Ff/QFqKlt/3vYo2QPfPQGDL7T7iI+72f4/1drNgPZvgXUfwpgbXA8g8ABNBM0QEUb0iNcWQTBJzLbJoLYSXjrfXuwb++4J2PAxnP4H18s2+NLQyyBthN06tOEdes6XdjRVa/ZFcBaVm+oeqq2y8zA+v8d2ORVsgJfOs3sht6S+Ht6/FRC7am3jFlSXBPu595lkk+yCf7V8TmNsofnD2+Bfw+C7J6HfNDsi7Oq3oedJ9n3O+IOdVPfDzJbP6Ulz/2TnfUxx1HEGnmc3IVr0TOvO892TtsU2zo15I22kiaAFI3vEs7mgjOKKGn+HonwlZTBc+55dq/+l844e4pj7Hcy+3/6jHv9T/8XoFBICZ/4JSnbB90/ax+pq7PIaWVNaV0SPz7RDal0lgtJ824+/4lWYeDdc9yFc9V/Yn2Mfb2kY6OJn7JpQ0/5s38eViGi48r92Ut7s++yXs5vq0D5bQ1g8047SeuEs+FsfeGIUrHzTjqL6xVK45DnbsmiozyTb2pn399a1YNqjYL1t2Yy9CRJ628fCOsHoGyBntvtLnRzaBytes3NGvDgiTRNBC0ZmdgVgVZ62CoJK+kh7V1m6B165wF5AygrhnR/byVnnP+H5JSraqtfJMOAcmP+YvSDvXAzVpe4NG20saypsX3B062L3cpg52a6DdOlLMPkem4D6Traf0cEd8OLZtivElX2b4csHIPtMe8FuTlgnuPg/dgmQBf+yEwX/1hf+3hdeOseONlr9jh3hNPBcmPZXuH01nPNPW5BtytQHobIE5j/a6o+kTWbfb4vvjQcRjPqx3cFusZutk8Uzbev0pNs8H2MDOrO4BcN6xCFiZxifkp3k73CUL2WOhyvfhNcutckgMs4WI69+2/4cSE5/CJ4cB3MethPbQsJsX3xrZU2xLYvcBZB9ur3ofnArRCXZom7asKOP732Kna392iXw4llw/UcQl3Hk+bpaeP+ndmTQeY+7lzxDQuHsRyC+B2z6HLqfYNdwShpgv8ektT4Jpw6BEVfZbpkxNzU/07q9ts2zy55MffDYmd8xKTD4AnuXf9q9thXUlOpyO1ek33RI6u+9ePFyi0BEponIRhHJEZG7XTz/IxEpFJEVjq+fuDqPP8VGhpOVFK0zjINVn4lw+at2Qblt8+Csfxzb9RAIuvW13RDLX7FdJT3GtS1Z9TwZwjrbhfu+egjevdG2jm6ac2wSOPyaE+Ha9203xgtnwYHcI88tfNwu/nf2I63r2hCxw2Nv+My2vk681Sap2PS2t8Qm/9bu+TDn4ba93h319bZoHdej6T79sTPszPCVbzR/rhWv2WVTTvZuawC8mAhEJBR4EpgODAKuFBFXC7X/1xgzwvH1H2/F0x4jM+NZvuMAxp1hder40+8M2x8+/W928bdAdeqddhns0t1t6xYCu0Bfrwm2S+LbR+xwxes+hOgWWsM9xthib+VBmwyKttr1l+b8yY6aGXJx2+LxpLgMW9dZ9V87B8Qb1rxrz33a748sdthYxhhb4F88s+mhuvV1dlBC99F2hV0v82aLYCyQY4zZaoypBt4Ezvfi+3nNyMyuHCivIXd/G8ZNq+ND1hQ7/C+QdUmwM5zBjp5pq0Hn2zvn6X+Hcx93b7lrsF04139kZwu/cBa8cyN07moX9gsUE35l91/44vfuzZdojZpK24pKHQZDL236OBH7/9K+jbB1rutj1n9oh9uefJtPalHeTATdgZ0Nfs9zPNbYxSKySkTeERGXO3qIyAwRWSIiSwoLC70Ra7MOTyzT+QQq0I27GW5b3r5VUEdeA3dth3EzWn8RShtuZ2nX1UDhejj3XxDVre2xeFpkHEy8C7Z90/rx/C1Z/KwdpnrGH1qeYDj4IlvLWfzssc8ZYyeiJfSxgwB8wN+jhj4CehljhgGzgZdcHWSMedYYM9oYMzopyfcF2+zkGKI6heoCdCrwiTQ/esbdc0TGtv31KYPhJ7PhsldgwFnti8UbRt9gJw/Ovt92wXhCeZHdCyPrdDtctSXhkTDqR7Dx06NrKmAL9buXwYk/d38RxXbyZiLYBTS8w89wPHaYMWa/MabK8et/gFFejKfNQkOEYRnxWjBWyl0JfWDQef6OwrWwTnaSV/4aWy/whG8fsesKnf6Q+68ZfaPtgvuhUWl0weN2T4oRV3kmNjd4MxH8AGSLSG8R6QRcAXzY8AARSWvw63nAei/G0y4jM+NZt7uEyhoP3UEopfxn8IWQfgJ8/Ue7b0NblRfBspdtF8+IqyHF1XiYJsR1h4Hn2Nc7520UbLBDT8fOcG+xQA/xWiIwxtQCPwc+x17g3zLGrBWRh0TEeatwm4isFZGVwG3Aj7wVT3uNzOxKbb1hzS4frdeulPIeEduXX7ILFj3duteW7LYjfl46F/6eZVdTje8Jk3/X+jjG3mxHWjnXilr4f3b47hjfjqT36oQyY8wsYFajx+5r8PM9wD3ejMFTRvRwbF258yCje7WwPaBSKvD1mmAna337qL2zj0q0XTKHv3ez3ztF2RE86z+yX3mL7esT+9tRSAPPtUXytozu6XkSpAyxLYrs021X1egf+7zArjOL3ZQUE0FG1866AJ1Sx5Mz/mh3Slv0DNRVuT4mLNIu8wD2gn/avXatKU/M9hWx3UAf3Qbv3gSmzk6e8zFNBK0wMrMrS7b7aNEqpZT3JWbBrYvskM3qMjs7uny/4/u+I99j0uxQTm8sTTH0UrvAXu58W7vo2svz79ECTQStML5PAh+t3M3NryzhofOHkBLbxMzBJuwrq+Jvn21g495Snr52FGlxvisGKaWaIWLX+o+IObJaqK906mJncC983OuLyzVFOtqyCaNHjzZLlizxy3vX1tXz3PxtPDp7E51CQ7hr+gCuGptJSEjzfYO1dfW8+n0uj8zeRGVNHeGhIaTERvLmjPGtTiZKqeNQdTnsWWFrBl4iIkuNMaNdPefvCWUdSlhoCDdP7Mvnt5/K0Iw47n1/DZc/+x05BWVNvmbxtiLO+b/5PPDROkb0iOfTX57KKzeOo6Ckkqtmfk9haRP9kirolFbW8PhXm8ndf8jfoShf69TFq0mgJdoiaCNjDO8szeOPn6ynorqOn5+WxS0T+9IpzObWgpJK/vzpBt5bvov0uEh+f84gpg1JRRwjCxZvK+L65xeT0bUzb84YT7foCH/+OSoA3PfBGl7+LpeIsBBum5LNTaf0Ofz/k1Lt1VyLQBNBOxWWVvHQx+v4aOVu+qVE88cLhrIq7yCPfbmZ6tp6Zpzah59N7kuXTseWYxZu2ccNL/5Ar25RvHHTeLpGubm4lzrurNlVzHlPzOeCEd2prK1j1uq9ZCdH86eLhjJGhysrD9BE4ANfb8jn3vfWsLvYDjOb1D+J+88dTO/EqGZf9+3mQm58aQnZydG8/pPxxHUJ90W4KoDU1xsuemoheQfK+eqOScR1Duer9fnc98Fadh2s4Mqxmdw9bYD+v6HaRROBj5RV1fLct9sYnB7LlIHJh7uBWjJnYwE3v7yUgWkxvPKTccRG6j/4YPLG4h3c87/VPHrZcC464cjuXoeqannsy008v2A7XbuE8/tzBnHe8HS3/79SHYMxhvV7StlRVM6Zg1O89t9XE0EH8OW6fG55dSlDM+J45cZxREd4d2Rvfb1BBL2o+FnRoWpOe2Qu/VJi+O+M8S7/e6zZVcxv31vNqrxiTslO5IHzBtM3qZktDlXAM8awYW8ps1bv4ZNVe9i6zw4QuO20LH59hne2pdRE0EF8tmYPt76+nBMy4/nFadl0CguxX6EhRDh/dvweExne6kJidW09C7bs47PVe5m9Pp/QEOFnk/py5dhMIsN9s9xtW9TVG0JbGKLrTTV19Xy4YjdlVbVcOTbTowXcu99dxdtL85h12yn0T41p8ri6esMr323n759v5FB1HZP6J3H9ib2Y2C+pxeHLKjAYY9iUX8Ynq3bz8eo9bC08RIjASX0TOWtoGst2HOCdpXncd84gbpjg+bkMmgg6kI9X7ea2N5ZT38J/lrAQYUBaDEO7xzM8I46hGXH0S4khPPToi1RlTR3fbCrkszV7+XJ9PqWVtURHhHHagGTySypZtK2I1NhIbp3cl8vG9CAiLHASQl294YEP1/LG4h2ckNmVM4ekcubgFDK6dvHJ+9fU1fO/ZXk8OWcLO4rs6pBDu8fxrytG0McDd+RLcw9w8VMLuemU3vzubPdWrSworeT1RTt4bdEOCkur6NmtC9eO78mlo3poDSFAFVfU8Or3uby3fBc5BWWECIzv042zh6Vx5uBUEh0jBmvr6vn568v5bO1eHrl0OBePymjhzK2jiaCDyTtQzt7iSqpr66mqq6e6tsGX4/c9xZWs2VXMqryDlFTWAhARFsKg9FiGZ8TTJymKRduKmLOhgPLqOuI6h3P6oBSmD0nl5KzEwy2AhVv28egXm1iSe4D0uEh+flo2l4zKaPddb3FFDTPnbeWrDQXce/ZATs5KbNXra+rq+fVbK/lo5W6mD0ll275DbNhbCsCQ7rGcOSiVaUNSyUqO9nj3VnVtPe8uy+PJOTnkHahgWEYct52WTU1dPXf/bzXVtfXcf+4gLh/To83vXVtXz3lPLKDoUDVf3jGx1V2B1bX1fL52Ly8t3M6S3AN0Dg/lgpHdue7EngxMa8emMh5gjOGDFbtZv6eE60/qRXp8cM6g319WxXPzt/HKd7mUVtUyrncC5wxPZ9rgVJJiXA8Xr6qt44YXf+D7rUU8fc0oTh+U4rF4NBEcx4wx5O4vZ2XeQVbnFbMqr5g1u4spr64jMboTZwxOZfqQVMb36XZMa6HhOebn7OORLzaxYudBMrp25rbTsrnwhO5NvqYph6pqeXHhdp75ZgsllbUkRkdQdKiKu6cP4KZT+rh14ayoruNnry1lzsZC7p4+gFsm9gVg+75DfL52L5+v3csyx+J/fRKjOHNIKleOySSzW/taClW1dby9JI+n5m5h18EKhveI5/Yp2Uzqn3Q47j3FFdzx1koWbtnPtMGp/PmioW0a9vvigm088NE6nrzqBM4eltbyC5qxZlcxr3yXy/srdlFVW8+43gncNiWbk/p283kNaHN+Kfe+v4ZF2+yaXJHhIcw4tS+3TOzjcgj18Si/pJJn523l9UU7qKyt46yhadw6KYtB6e4l6LKqWq6e+T3r95by0o/HcmJfz6xEqokgyNTVG3YfrCA9vnOr+taNMczdWMijszexelcxmQldmD4klQnZiYzpldBsHaGypo7XFu3gqbk57CurZurAZH59en8yu3XhzrdX8umavZwzLI2/XTKs2QtCSWUNP3lxCT/kFvHwBUO5alymy+PySyr5Yl0+n6/Zy/db9yMC153Yi1+clkV8l9ZdmCuq63h76U6enruF3cWVjMyM55dTspnYL8nlhbS+3jDz263844uNJER14tHLRrSqxVNQWsmUf3zDiMx4Xr5hrMcu1gfLq3lryU5eWLCdPcWVjOudwB1n9Gdsb+/PQ6ioruPxrzczc95WoiLCuGvaAE7JTuRvn2/ko5W7SYmN4K5pA7hgRPfjtqaxs6icZ+Zt4a0f8qgzhvNHpPOzSVlkJbe+G/HAoWoufeY79hZX8sZN4xmaEdfu+DQRqFYxxvDl+gKen7+NJblF1NQZOoWFMLZXAhOyE5mQlcigtFhCQoSaunreWZrH419tZk9xJSf17cZvzuzPCZldjzrfU99s4R+fbyQ7OYZnrh1FLxfzK/aXVXH9C4vZsKeUf14+gnOHp7sVb35JJf+cvYm3luwkJjKcX5yWxbUn9myx3rGnuIKXv8vl9UU7KK6oYXTPrvxyajYTshLdujiv2VXMbW8uZ9u+Q8w4pQ93nNHfrS61X/13BZ+s2sNnt5/ikVpDY5U1dby5eAdPzt1CYWkVp2Qn8uvT+zGywX8TT5q9Lp8HPrRzHi4+IYN7zhpwuN8bYMn2Iv7w8TpW5hUzPCOO+84dxKiegTNJrqC0ksSoiDYlKGMMy3ce5PVFO3h/+S5E4JJRPfjpxL7tbqHuKa7gkqe+o6KmjrdvObHdI8U0Eag2O1RVy+JtRXy7eR/zcwrZlG/XVeoW1YkT+3Zj9a5icveXMzIznjvP6M9JzdwZz9tUyC/eWI4xhn9dOZLJ/ZMPP7f7YAXXPreIvAMVPH3NKCYPSG7yPE3ZsLeEP8/awDebCumR0Jm7pg3g7KFpx1zUV+w8yHPztzFr9R6MMZw5OJUbJvRmdM+urb47r6iu4w+frOP1RTsYnB7L1eN6MqpnV7KTo11eWL7fup8rnv2en0/O4jdnemeYYMPYXv0+l6e+2WKHqQ5I5ten92NI97hjjsspKGPD3hI25ZeyMb/MFqITupCVHE1WcjR9k6Lpmxx1VGsu70A5D3y4ji/X59MvJZo/nD+EcX1cd2PU1xveX7GLv362gfySKs4dns7d0wfQ3Y/1g4rqOu77YA1vL80jKSaCMwalMK2FblSnrYVlvL9iNx+s2EXu/nIiwkK4cmwmN0/s49FVhbftO8SlTy+kU2gI7/z0pHbVWzQRKI/JL6lk/uZ9LMjZx4It+0iKieBXU/tx2gD3JtDtLCpnxitL2bC3hF9P7cetk7PILSrnmv8soqSihud+NKbdXRnzNhXyp1nr2bC3lJGZ8dx79kCGZ8Tz2dq9PD9/G8t2HCQmIozLx/Tg+pN60SOh/aOQvli7l/s+WMveEjuzPDYyjBN6dmV0z66M6pnAiB7xhIUKZ/3rWypq6pj9q4l07uSbEVrOus2z87ZSXFHDtMGpZKdEs3FvKZvyS8ktKsd5GegUFkJ2cjRJMRHs2F9OblE5dQ2GsHWP70yfpChSYyP5eNUeAG6fms0NE3q7VU8qr67l6W+28sw3WwA4JTuJcb0TGNM7gcHpsa2uSbXV5vxSbn19GZsLyrhufE/2lVUzZ6MdWBEbGcbUQSlMG5zKqf2SDneJFpRW8tHKPXywYher8ooRgZP6duP8Ed2ZNiTVaxNB1+wq5spnvyc5NoK3bzmJhDYuRaOJQAWUiuo67vnfKt5fsZtJ/ZNYs6uYegMv3zD2mLvVtqqrN7y7NI9/fLGRgtIqEqI6UXSomp7duvDjk3pxyegeHp+0Z4xh+/5ylmwvYtmOAyzZfoDNjpVpw0KE7l07k7u/nP9cN5qpHhwN4q6Syhqe+3Ybz8/fRnlNHb0To+ifEkO/lBj6p0bTLyWGzIQuhDW4GFfV1pG7v5wtBWXkFJSRU1jGlsIycveVc1JWN+47d3Cb7up3Hazg33NyWJCzj+377dDczuGhnNAznjG9EhjbK4GRmV29kizfXZrHve+vISoilMcuH8mEbNuKrayp49vN+w4PtS6uqKFzeCiT+idRVlXLgpx91BsYnB7LhSO7c+7wdJ8tI794WxHXPreIS0Zl8PCFQ9t0Dk0EKuAYY3hhwXYenrWe5JgIXrlxXJuKai0pr65l5rxtrNtTzMUnZDBlYIpPJ6cdLK9m2Y4DLM21iSHbsTChP1XW1AEEzCTCgpJKfth+gB+2F7F4WxHr95ZgjE2efZKiiO/SifjO4cR3CSe+SyfiOocT5/g9oUsnhmbEEePG3XhFdR33f7iGt5bkMa53Ao9fObLJC3lNXT2Lthbx2do9zF6XT0RYKOcNT+eCkelkJTc98c+bluYWMTAtts2jrzQRqICVU1BK1y6ddBludVhJZQ1Lcw/ww7YiNheUUVxRQ0lFDQfLazhYUU1lTf1Rx4eFCGN6JTCpfxKT+ifTL+XYuSU5BWXc+toyNhWU8ovJWdw2Jfuolk8w0ESglDpuVNbUUVxRQ3FFDfkllSzI2c/cjQWHJxymx0UysX8yk/oncMwLNgAAB6pJREFUcXJWIrPX7eV3762hc3go/7x8BKf2S/LzX+AfmgiUUse9PcUVfLOxkDkbC1iQs5+yqlrCQoTaesPY3gn8XzNdQcGguUTg1al+IjIN+BcQCvzHGPOXRs9HAC8Do4D9wOXGmO3ejEkpdXxKi+vMFWMzuWJsJtW19SzNPcA3mwpJjO7Ej07qFXRdQa3htUQgIqHAk8DpQB7wg4h8aIxZ1+CwG4EDxpgsEbkC+CtwubdiUkoFh05hIZzYt5vHlmc43nkzRY4FcowxW40x1cCbwPmNjjkfeMnx8zvAFNEF8pVSyqe8mQi6Azsb/J7neMzlMcaYWqAYOCaFi8gMEVkiIksKCwu9FK5SSgWnDtFpZox51hgz2hgzOikpOCv+SinlLd5MBLuAHg1+z3A85vIYEQkD4rBFY6WUUj7izUTwA5AtIr1FpBNwBfBho2M+BK53/HwJ8LXpaONZlVKqg/PaqCFjTK2I/Bz4HDt89HljzFoReQhYYoz5EHgOeEVEcoAibLJQSinlQ16dR2CMmQXMavTYfQ1+rgQu9WYMSimlmtchisVKKaW8p8MtMSEihUBuG1+eCOzzYDje1FFi1Tg9r6PEqnF6lrfj7GmMcTnsssMlgvYQkSVNrbURaDpKrBqn53WUWDVOz/JnnNo1pJRSQU4TgVJKBblgSwTP+juAVugosWqcntdRYtU4PctvcQZVjUAppdSxgq1FoJRSqhFNBEopFeSCJhGIyDQR2SgiOSJyt7/jaYqIbBeR1SKyQkQCak9OEXleRApEZE2DxxJEZLaIbHZ87+rPGB0xuYrzARHZ5fhcV4jIWf6M0RFTDxGZIyLrRGStiPzS8XhAfabNxBmIn2mkiCwWkZWOWB90PN5bRBY5/v3/17H+WSDG+aKIbGvwmY7wSTzBUCNw7Ja2iQa7pQFXNtotLSCIyHZgtDEm4CbAiMipQBnwsjFmiOOxvwFFxpi/OBJsV2PMXQEY5wNAmTHmH/6MrSERSQPSjDHLRCQGWApcAPyIAPpMm4nzMgLvMxUgyhhTJiLhwHzgl8Cvgf8ZY94UkaeBlcaYpwIwzluAj40x7/gynmBpEbizW5pqgTFmHnZxwIYa7jL3EvYC4VdNxBlwjDF7jDHLHD+XAuuxmzUF1GfaTJwBx1hljl/DHV8GOA27CyIExmfaVJx+ESyJwJ3d0gKFAb4QkaUiMsPfwbghxRizx/HzXiDFn8G04OcissrRdeT3LqyGRKQXMBJYRAB/po3ihAD8TEUkVERWAAXAbGALcNCxCyIEyL//xnEaY5yf6cOOz/SfIhLhi1iCJRF0JBOMMScA04FbHd0cHYJjL4lA7Wt8CugLjAD2AI/4N5wjRCQaeBe43RhT0vC5QPpMXcQZkJ+pMabOGDMCuxnWWGCAn0NyqXGcIjIEuAcb7xggAfBJl2CwJAJ3dksLCMaYXY7vBcB72P+RA1m+ow/Z2Zdc4Od4XDLG5Dv+4dUDMwmQz9XRP/wu8Jox5n+OhwPuM3UVZ6B+pk7GmIPAHOBEIN6xCyIE2L//BnFOc3TDGWNMFfACPvpMgyURuLNbmt+JSJSjGIeIRAFnAGuaf5XfNdxl7nrgAz/G0iTnhdXhQgLgc3UUDJ8D1htjHm3wVEB9pk3FGaCfaZKIxDt+7owdILIee6G9xHFYIHymruLc0OAGQLB1DJ98pkExagjAMbTtMY7slvawn0M6hoj0wbYCwG4a9HogxSkibwCTsMvl5gP3A+8DbwGZ2OXBLzPG+LVQ20Sck7BdGAbYDtzcoB/eL0RkAvAtsBqodzz8W2z/e8B8ps3EeSWB95kOwxaDQ7E3um8ZYx5y/Nt6E9vdshy4xnHXHWhxfg0kAQKsAG5pUFT2XjzBkgiUUkq5FixdQ0oppZqgiUAppYKcJgKllApymgiUUirIaSJQSqkgp4lAKUBE6hyrPa4RkY+cY7y9+H4/EpEnvPkeSrlLE4FSVoUxZoRjtdKi/9/e/bpYFYRhHP8+TS0KazFtEIugLigYVAwGm8llk8gGQRAsBousWNU/wKg2DZoEQRZWQXZXEFmDwSA2wwY1+CMoj2HmsGcV3SNcr2GeTzrcGYYz4fCeuZf7vMC5/31DEeOSQhDxq0VqKJmkKUlLNQTsfhesJmlB0oF6vb3Gh3dv+vckPVTpJ3C1W1TSrKTXkp4Bh3qfT9eTyIqkJ2PcZwSQQhCxTu1dcYy1CJLbwEXbeyn/rL08YJkpYAbYA8yoNHbZAVyhFIDDwO7e/DnguO19wImRbCTiL6QQRBSbayRwF/v8SNJWYJvtx3XOLWBIGuy87Y+2vwKvgEngILBge7X2xLjTm/8UuCnpDCVyIGKsUggiii81EniSkvOy0W8E31h7fjb9NNbPsPlOyY36LdtngUuUhNznkiaG3nTEKKQQRPTY/gycBy4An4D3ko7U4VNAdzp4C+yv1yfZ2DJwVNJEjXSe7gYk7bS9bHsOWGV9ZHrEP/fHN5WIFtl+IeklJV3zNHBD0hbgDTBbp10H7tYucg8GrPmu9k1eBD5QkiU71yTtopxE5oGVUe0lYoikj0ZENC5fDUVENC6FICKicSkEERGNSyGIiGhcCkFERONSCCIiGpdCEBHRuB+90N6u9R/0GQAAAABJRU5ErkJggg==",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.plot(all_losses['train'], label='train')\n",
        "plt.plot(all_losses['test'], label='test')\n",
        "plt.legend()\n",
        "plt.xlabel('Rounds')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Loss VS Rounds');"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "-BDVnGU1dBC5",
        "outputId": "30061e80-f3b8-4f5d-80bc-d1e3915165db"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.plot(validation_accracy)\n",
        "plt.xlabel('Rounds')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Test accuracy');"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Federated Learning",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.2 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.2"
    },
    "vscode": {
      "interpreter": {
        "hash": "11e20b166bd109e52469344a4841af8eb5ce8da21ff5795682d67af5ea448bd9"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
